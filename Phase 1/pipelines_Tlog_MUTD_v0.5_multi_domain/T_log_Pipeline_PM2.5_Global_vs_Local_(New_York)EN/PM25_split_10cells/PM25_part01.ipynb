{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cd4fc3",
   "metadata": {
    "id": "DI_v9LmzBLsM"
   },
   "source": [
    "Block 1 ‚Äî Preparation (imports, seeds, folders, logger)\n",
    "Here is a single, executable cell that:\n",
    "\n",
    "imports the necessary libraries,\n",
    "\n",
    "sets the seeds,\n",
    "\n",
    "creates the data/, results/, and logs/ folders,\n",
    "\n",
    "delete /content/sample_data if it exists,\n",
    "\n",
    "configures a logger with console and file output,\n",
    "\n",
    "initializes a log file, logs/logs.csv, and appends it to logs/summary.md,\n",
    "\n",
    "uses timezone-aware timestamps (without the deprecated utcnow()),\n",
    "\n",
    "avoids escaping warnings with raw strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f76b3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation/Mise √† jour des d√©pendances via requirements.txt...\n",
      "\n",
      "‚úÖ Toutes les d√©pendances ont √©t√© install√©es ou mises √† jour avec succ√®s.\n",
      "Veuillez RED√âMARRER le noyau (kernel) du notebook si c'est la premi√®re ex√©cution.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ‚öôÔ∏è Installation des d√©pendances du projet\n",
    "# Cette cellule garantit que toutes les librairies n√©cessaires sont install√©es.\n",
    "# ============================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_requirements(file_path=\"requirements.txt\"):\n",
    "    \"\"\"Installe les paquets list√©s dans requirements.txt.\"\"\"\n",
    "    print(f\"Installation/Mise √† jour des d√©pendances via {file_path}...\")\n",
    "    try:\n",
    "        # Ex√©cute la commande pip\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", file_path])\n",
    "        print(\"\\n‚úÖ Toutes les d√©pendances ont √©t√© install√©es ou mises √† jour avec succ√®s.\")\n",
    "        print(\"Veuillez RED√âMARRER le noyau (kernel) du notebook si c'est la premi√®re ex√©cution.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n‚ùå ERREUR lors de l'installation des d√©pendances : {e}\")\n",
    "\n",
    "# Ex√©cuter l'installation\n",
    "install_requirements()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4842d72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1761272459391,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "hQ1dLGasA3xc",
    "outputId": "3cf250a0-bb9f-4eb3-c9db-a6ba4935ebec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 02:14:33,438 [INFO] Bloc 1 pr√™t: imports, seeds, dossiers et logger configur√©s.\n",
      "2025-11-11 02:14:33,440 [INFO] Plot de v√©rification sauvegard√©: results\\env_check_plot.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloc 1 OK ‚Äî Dossiers et logger pr√™ts.\n",
      "Seeds fix√©s: 42\n",
      "Logs: logs\\logs.csv\n",
      "Summary: logs\\summary.md\n"
     ]
    }
   ],
   "source": [
    "# Bloc 1 ‚Äî Pr√©paration\n",
    "# Imports, seeds, dossiers, logger, journaux init\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# Seeds et conventions\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# =========================\n",
    "# Dossiers\n",
    "# =========================\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Supprimer sample_data si pr√©sent (environnements Colab)\n",
    "sample_data_path = '/content/sample_data'\n",
    "if os.path.isdir(sample_data_path):\n",
    "    try:\n",
    "        shutil.rmtree(sample_data_path)\n",
    "    except Exception as e:\n",
    "        # Silencieux mais on loguera plus bas\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# Timestamps et helpers\n",
    "# =========================\n",
    "def utc_timestamp():\n",
    "    # Timezone-aware ISO 8601\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "LOG_CSV_PATH = os.path.join('logs', 'logs.csv')\n",
    "SUMMARY_MD_PATH = os.path.join('logs', 'summary.md')\n",
    "\n",
    "# Init du fichier logs.csv si vide\n",
    "if not os.path.isfile(LOG_CSV_PATH):\n",
    "    with open(LOG_CSV_PATH, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['timestamp', 'level', 'message'])\n",
    "\n",
    "# =========================\n",
    "# Logger\n",
    "# =========================\n",
    "logger = logging.getLogger('TlogV01')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_fmt = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "console_handler.setFormatter(console_fmt)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Fichier handler (logs/app.log)\n",
    "file_handler = logging.FileHandler(os.path.join('logs', 'app.log'))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_fmt = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "file_handler.setFormatter(file_fmt)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# =========================\n",
    "# Fonctions de journalisation\n",
    "# =========================\n",
    "def log_event(level: str, message: str):\n",
    "    \"\"\"\n",
    "    √âcrit dans logs.csv et via logger standard.\n",
    "    level: 'INFO' | 'WARNING' | 'ERROR'\n",
    "    \"\"\"\n",
    "    ts = utc_timestamp()\n",
    "    # Logger console/fichier\n",
    "    if level.upper() == 'INFO':\n",
    "        logger.info(message)\n",
    "    elif level.upper() == 'WARNING':\n",
    "        logger.warning(message)\n",
    "    elif level.upper() == 'ERROR':\n",
    "        logger.error(message)\n",
    "    else:\n",
    "        logger.info(message)\n",
    "\n",
    "    # Ajout dans logs.csv\n",
    "    with open(LOG_CSV_PATH, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([ts, level.upper(), message])\n",
    "\n",
    "def append_summary_md(text: str):\n",
    "    \"\"\"\n",
    "    Append dans summary.md. Utiliser des cha√Ænes brutes pour\n",
    "    inclure LaTeX sans warnings d‚Äô√©chappement.\n",
    "    \"\"\"\n",
    "    with open(SUMMARY_MD_PATH, 'a', encoding='utf-8') as f:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "# =========================\n",
    "# Banner de session\n",
    "# =========================\n",
    "session_header = r\"\"\"# Session Log T_log V0.1\n",
    "\n",
    "- Session started: {ts}\n",
    "- Conventions: bias=0 by default, seeds fixed (42), outputs in results/\n",
    "\"\"\".format(ts=utc_timestamp())\n",
    "\n",
    "# √âcrire header si le fichier est nouveau\n",
    "if not os.path.isfile(SUMMARY_MD_PATH) or os.path.getsize(SUMMARY_MD_PATH) == 0:\n",
    "    append_summary_md(session_header)\n",
    "\n",
    "# =========================\n",
    "# V√©rification environnement\n",
    "# =========================\n",
    "plt.figure(figsize=(4, 3))\n",
    "x = np.linspace(0, 2*np.pi, 200)\n",
    "plt.plot(x, np.sin(x), color='steelblue', lw=2)\n",
    "plt.title('Env check plot')\n",
    "plt.tight_layout()\n",
    "env_plot_path = os.path.join('results', 'env_check_plot.png')\n",
    "plt.savefig(env_plot_path, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# =========================\n",
    "# Logs init\n",
    "# =========================\n",
    "log_event('INFO', 'Bloc 1 pr√™t: imports, seeds, dossiers et logger configur√©s.')\n",
    "log_event('INFO', f'Plot de v√©rification sauvegard√©: {env_plot_path}')\n",
    "\n",
    "append_summary_md(r\"\"\"---\n",
    "## Bloc 1 ‚Äî Pr√©paration\n",
    "- Imports, seeds, dossiers et logger configur√©s.\n",
    "- Env check plot: results/env_check_plot.png\n",
    "\"\"\")\n",
    "\n",
    "# Affichage de confirmation minimal\n",
    "print(\"Bloc 1 OK ‚Äî Dossiers et logger pr√™ts.\")\n",
    "print(f\"Seeds fix√©s: {SEED}\")\n",
    "print(f\"Logs: {LOG_CSV_PATH}\")\n",
    "print(f\"Summary: {SUMMARY_MD_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42f428",
   "metadata": {
    "id": "Jyzr3ZsWB_KT"
   },
   "source": [
    "Quick summary: We'll move on to Block 2 ‚Äî Data Acquisition. The goal is to unzip your Urban Air Quality & Climate Dataset (1958-2025).zip ZIP file into the data/ folder, verify its contents (CSV files, etc.), and log the operation.\n",
    "\n",
    "Block 2 ‚Äî Data Acquisition (Air Quality)\n",
    "Here is the corresponding Python cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2356fab2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1761272662747,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "bhDQ4BIbCAJs",
    "outputId": "e9c63be6-728f-4743-9e5a-50a121245f03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D√©but du t√©l√©chargement de : krishd123/urban-air-quality-and-climate-dataset-1958-2025\n",
      "Dataset URL: https://www.kaggle.com/datasets/krishd123/urban-air-quality-and-climate-dataset-1958-2025\n",
      "T√©l√©chargement termin√©. Fichier cible : data\\urban_climate.csv\n",
      "\n",
      "==================================================\n",
      "ANALYSE DU FICHIER urban_climate.csv\n",
      " - Nombre d'enregistrements (n) : 11040\n",
      " - Nombre de colonnes : 12\n",
      " - Colonnes : ['city', 'country', 'latitude', 'longitude', 'year', 'month', 'temperature_celsius', 'humidity_percent', 'precipitation_mm', 'wind_speed_ms', 'urban_heat_island_intensity', 'data_source']\n",
      " - Aper√ßu sauvegard√© : results\\urban_climate_preview.csv\n",
      "\n",
      "Comptes de valeurs manquantes (seulement les colonnes non nulles) :\n",
      "Series([], dtype: int64)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 0. INSTALLATION DE KAGGLE ---\n",
    "# Cette ligne assure que la librairie Kaggle est install√©e\n",
    "!pip install kaggle --quiet\n",
    "\n",
    "# --- D√©pendance Kaggle ---\n",
    "try:\n",
    "    # Tenter d'importer la librairie Kaggle\n",
    "    import kaggle.api as kaggle_api\n",
    "except ImportError:\n",
    "    print(\"√âchec de l'importation de 'kaggle' m√™me apr√®s installation. Veuillez v√©rifier votre environnement.\")\n",
    "    raise\n",
    "# ------------------------\n",
    "\n",
    "# --- 1. CONFIGURATION ET FONCTIONS DE LOGGING ---\n",
    "\n",
    "# Identifiants du Dataset Kaggle\n",
    "KAGGLE_DATASET_ID = \"krishd123/urban-air-quality-and-climate-dataset-1958-2025\"\n",
    "TARGET_FILE_NAME = \"urban_climate.csv\"\n",
    "\n",
    "# Chemins de travail\n",
    "DATA_DIR = 'data'\n",
    "LOGS_DIR = 'logs'\n",
    "RESULTS_DIR = 'results'\n",
    "\n",
    "# Fichier de donn√©es apr√®s t√©l√©chargement/extraction\n",
    "LOCAL_COPY = os.path.join(DATA_DIR, TARGET_FILE_NAME)\n",
    "# Fichiers de log et de r√©sultats\n",
    "RESULT_PREVIEW = os.path.join(RESULTS_DIR, 'urban_climate_preview.csv')\n",
    "LOGS_CSV = os.path.join(LOGS_DIR, 'logs.csv')\n",
    "SUMMARY_MD = os.path.join(LOGS_DIR, 'summary.md')\n",
    "\n",
    "# Cr√©ation des dossiers\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def append_log(level, message):\n",
    "    \"\"\"Ajoute une entr√©e au fichier de log CSV et Markdown.\"\"\"\n",
    "    ts = datetime.utcnow().isoformat() + 'Z'\n",
    "    entry = pd.DataFrame([{'timestamp': ts, 'level': level, 'message': message}])\n",
    "    \n",
    "    # √âcriture du log\n",
    "    try:\n",
    "        if os.path.exists(LOGS_CSV):\n",
    "            df_logs = pd.read_csv(LOGS_CSV)\n",
    "            df_logs = pd.concat([df_logs, entry], ignore_index=True)\n",
    "        else:\n",
    "            df_logs = entry\n",
    "            \n",
    "        df_logs.to_csv(LOGS_CSV, index=False)\n",
    "        with open(SUMMARY_MD, 'a', encoding='utf-8') as f:\n",
    "            f.write(f'\\n- {ts} **{level}**: {message}\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"[ALERTE] √âchec de l'√©criture du log: {e}\")\n",
    "\n",
    "# Alias pour utiliser 'log_event' si d√©sir√©, tout en utilisant la fonction `append_log`\n",
    "log_event = append_log\n",
    "\n",
    "\n",
    "def find_and_auth_kaggle():\n",
    "    \"\"\"Tente de trouver les cl√©s d'API et authentifie l'API Kaggle.\"\"\"\n",
    "    log_event('INFO', 'Tentative d\\'authentification Kaggle...')\n",
    "    \n",
    "    # 1. V√©rifier les variables d'environnement (m√©thode Colab/Notebook)\n",
    "    if os.getenv('KAGGLE_USERNAME') and os.getenv('KAGGLE_KEY'):\n",
    "        log_event('INFO', 'Authentification via variables d\\'environnement (KAGGLE_USERNAME/KEY).')\n",
    "    \n",
    "    # 2. Chercher le fichier kaggle.json\n",
    "    else:\n",
    "        locations = [\n",
    "            os.path.join(os.path.expanduser('~'), '.kaggle', 'kaggle.json'), # Emplacement standard\n",
    "            os.path.join(os.getcwd(), 'kaggle.json')                       # R√©pertoire actuel\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for loc in locations:\n",
    "            if os.path.exists(loc):\n",
    "                try:\n",
    "                    with open(loc, 'r') as f:\n",
    "                        config = json.load(f)\n",
    "                        username = config.get('username')\n",
    "                        key = config.get('key')\n",
    "                        if username and key:\n",
    "                            os.environ['KAGGLE_USERNAME'] = username\n",
    "                            os.environ['KAGGLE_KEY'] = key\n",
    "                            log_event('INFO', f'Cl√©s lues et d√©finies via {loc}.')\n",
    "                            found = True\n",
    "                            break\n",
    "                except (json.JSONDecodeError, Exception):\n",
    "                    # Fichier trouv√© mais invalide, on continue la recherche\n",
    "                    continue\n",
    "        \n",
    "        if not found:\n",
    "            log_event('ERROR', \"Fichier kaggle.json introuvable. Veuillez le placer dans ~/.kaggle/ ou le r√©pertoire courant.\")\n",
    "            return False\n",
    "\n",
    "    # 3. Authentifier l'API\n",
    "    try:\n",
    "        kaggle_api.authenticate()\n",
    "        log_event('SUCCESS', 'Authentification Kaggle r√©ussie.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_event('ERROR', f'√âchec de l\\'authentification de l\\'API: {e}')\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- 2. AUTHENTIFICATION ET T√âL√âCHARGEMENT ---\n",
    "try:\n",
    "    if not find_and_auth_kaggle():\n",
    "        # Lever une exception si l'authentification √©choue\n",
    "        raise RuntimeError(\"Processus annul√©. √âchec de la configuration Kaggle. Assurez-vous d'avoir configur√© votre API Key.\")\n",
    "    \n",
    "    print(f\"\\nD√©but du t√©l√©chargement de : {KAGGLE_DATASET_ID}\")\n",
    "    log_event('INFO', f\"T√©l√©chargement et d√©compression du dataset : {KAGGLE_DATASET_ID}\")\n",
    "    \n",
    "    # T√©l√©charger et d√©compresser directement le dataset dans le dossier 'data/'\n",
    "    kaggle_api.dataset_download_files(\n",
    "        KAGGLE_DATASET_ID, \n",
    "        path=DATA_DIR, \n",
    "        unzip=True,\n",
    "        # 'force=True' pour re-t√©l√©charger si le fichier existe d√©j√† (reproductibilit√©)\n",
    "        force=True, \n",
    "        quiet=True # Rendre l'API Kaggle moins verbeuse\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(LOCAL_COPY):\n",
    "        raise FileNotFoundError(f\"Le fichier {TARGET_FILE_NAME} est introuvable apr√®s l'extraction du dataset Kaggle. Le dataset pourrait avoir une structure diff√©rente.\")\n",
    "        \n",
    "    log_event('SUCCESS', f\"T√©l√©chargement et pr√©paration du fichier : {LOCAL_COPY}\")\n",
    "    print(f\"T√©l√©chargement termin√©. Fichier cible : {LOCAL_COPY}\")\n",
    "\n",
    "\n",
    "    # --- 3. LECTURE ROBUSTE ET ANALYSE DU FICHIER ---\n",
    "    \n",
    "    read_errors = []\n",
    "    df = None\n",
    "    log_event('INFO', f\"Tentative de lecture du CSV : {LOCAL_COPY}\")\n",
    "    \n",
    "    # Tentative 1: Standard (utf-8, comma)\n",
    "    try:\n",
    "        df = pd.read_csv(LOCAL_COPY)\n",
    "    except Exception as e1:\n",
    "        read_errors.append(f\"Standard: {e1}\")\n",
    "        \n",
    "        # Tentative 2: utf-8, semicolon\n",
    "        try:\n",
    "            df = pd.read_csv(LOCAL_COPY, encoding='utf-8', sep=';')\n",
    "        except Exception as e2:\n",
    "            read_errors.append(f\"UTF-8/Semicolon: {e2}\")\n",
    "            \n",
    "            # Tentative 3: latin1, standard sep\n",
    "            try:\n",
    "                df = pd.read_csv(LOCAL_COPY, encoding='latin1')\n",
    "            except Exception as e3:\n",
    "                read_errors.append(f\"Latin1: {e3}\")\n",
    "                \n",
    "                # √âchec total de lecture\n",
    "                raise RuntimeError(f\"Impossible de lire le CSV ({LOCAL_COPY}). √âchecs: {'; '.join(read_errors)}\")\n",
    "    \n",
    "    # Si la lecture est r√©ussie:\n",
    "    n_rows = df.shape[0]\n",
    "    n_cols = df.shape[1]\n",
    "    cols = list(df.columns)\n",
    "    missing_counts = df.isna().sum()\n",
    "\n",
    "    # Sauvegarder un aper√ßu\n",
    "    df.head(200).to_csv(RESULT_PREVIEW, index=False)\n",
    "\n",
    "    # Log et affichage du succ√®s\n",
    "    log_event('SUCCESS', f'Chargement r√©ussi: {LOCAL_COPY}; rows={n_rows}; cols={n_cols}')\n",
    "    log_event('INFO', f'Colonnes d√©tect√©es: {cols}')\n",
    "    log_event('INFO', f'Missing per column (seulement > 0): {dict(missing_counts[missing_counts > 0])}') \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ANALYSE DU FICHIER {TARGET_FILE_NAME}\")\n",
    "    print(f\" - Nombre d'enregistrements (n) : {n_rows}\")\n",
    "    print(f\" - Nombre de colonnes : {n_cols}\")\n",
    "    print(\" - Colonnes :\", cols)\n",
    "    print(f\" - Aper√ßu sauvegard√© : {RESULT_PREVIEW}\")\n",
    "    print(\"\\nComptes de valeurs manquantes (seulement les colonnes non nulles) :\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    log_event('CRITICAL', f'√âchec du bloc de code: {e}')\n",
    "    print(\"\\n\" + \"#\"*50)\n",
    "    print(\"√âCHEC CRITIQUE: Le code n'a pas pu s'ex√©cuter.\")\n",
    "    print(f\"Erreur: {e}\")\n",
    "    print(f\"V√©rifiez que vous avez configur√© votre cl√© d'API Kaggle (fichier kaggle.json).\")\n",
    "    print(\"#\"*50)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58447101",
   "metadata": {
    "id": "MaXQBAZYCo5c"
   },
   "source": [
    "Perfect üëå, your **Block 2 ‚Äî Data Acquisition** is validated: the ZIP file has been successfully unzipped and we now have 5 usable components:\n",
    "\n",
    "- `metadata.json` ‚Üí complete documentation\n",
    "- `co2_emissions.csv` ‚Üí direct CO‚ÇÇ measurements (Mauna Loa, 1958‚Äì2025)\n",
    "- `air_quality_global.csv` ‚Üí PM2.5 and NO‚ÇÇ for 20 cities (1999‚Äì2025)\n",
    "- `urban_climate.csv` ‚Üí urban climate variables (1980‚Äì2025)\n",
    "- `ice_core_co2.csv` ‚Üí paleoclimate reconstructions (2000 years BP)\n",
    "\n",
    "---\n",
    "\n",
    "### Next strategic choice\n",
    "To apply your **T_log model V0.1**, we need to define:\n",
    "- **n** = system size (number of observations or nodes)\n",
    "- **d** = effective dimension (spectral, spatial, or chosen proxy)\n",
    "\n",
    "üëâ Two possible approaches:\n",
    "1. **Air Quality (PM2.5, NO‚ÇÇ)**:\n",
    "- n = number of measurement points (per city or global)\n",
    "- d = effective temporal/spatial dimension (e.g., d=1 for time series, d‚âà2‚Äì3 if combining several cities as a graph)\n",
    "\n",
    "2. **Direct CO‚ÇÇ (Mauna Loa)**:\n",
    "- n = number of months measured (‚âà800+)\n",
    "- d = 1 (one-dimensional time series)\n",
    "\n",
    "3. **Ice Core CO‚ÇÇ**:\n",
    "- n = ~2000 years of data\n",
    "- d = 1 (long time series)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e42981",
   "metadata": {
    "id": "UMyE6TXZDOaM"
   },
   "source": [
    "**Quick Summary:** We'll tackle **Block 3 ‚Äî Calculating T_log** on the `air_quality_global.csv` file. We'll load the PM2.5 data, choose a city (or the global set), define n as the number of valid observations, set d=1 (one-dimensional time series), then calculate and classify T_{log}.\n",
    "\n",
    "---\n",
    "\n",
    "### Block 3 ‚Äî Calculating T_log (Air Quality Global, PM2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45712763",
   "metadata": {
    "id": "bhsx34KtDl4P"
   },
   "source": [
    "Before starting the calculation of\n",
    "ùëá\n",
    "log\n",
    "‚Å°\n",
    ", it's more rigorous to check the state of the air_quality_global.csv file: structure, columns, missing values, duplicates, etc. This will constitute our Block 3a ‚Äî Data Inspection and Validation.\n",
    "\n",
    "Block 3a ‚Äî Inspection of the air_quality_global.csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8353d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1761273084455,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "lvVxlstsDnH2",
    "outputId": "b99b2a18-1efe-4833-fc0d-80fc046b1f5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_7700\\2067266531.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Aper√ßu du dataset ===\n",
      "       city country  latitude  longitude  year  month  pm25_ugm3  no2_ugm3  \\\n",
      "0  New York     USA   40.7128    -74.006  1999      1      18.11     35.98   \n",
      "1  New York     USA   40.7128    -74.006  1999      2      27.79     17.71   \n",
      "2  New York     USA   40.7128    -74.006  1999      3      12.05     40.99   \n",
      "3  New York     USA   40.7128    -74.006  1999      4      35.25     17.18   \n",
      "4  New York     USA   40.7128    -74.006  1999      5      38.39     25.07   \n",
      "5  New York     USA   40.7128    -74.006  1999      6      14.89     28.95   \n",
      "6  New York     USA   40.7128    -74.006  1999      7      19.66     27.85   \n",
      "7  New York     USA   40.7128    -74.006  1999      8      10.00     26.14   \n",
      "8  New York     USA   40.7128    -74.006  1999      9      15.04     38.56   \n",
      "9  New York     USA   40.7128    -74.006  1999     10      15.32     29.50   \n",
      "\n",
      "  data_quality           measurement_method data_source  \n",
      "0     Moderate  Reference/Equivalent Method     EPA_AQS  \n",
      "1         Good  Reference/Equivalent Method     EPA_AQS  \n",
      "2     Moderate  Reference/Equivalent Method     EPA_AQS  \n",
      "3         Poor  Reference/Equivalent Method     EPA_AQS  \n",
      "4         Good  Reference/Equivalent Method     EPA_AQS  \n",
      "5         Good  Reference/Equivalent Method     EPA_AQS  \n",
      "6     Moderate  Reference/Equivalent Method     EPA_AQS  \n",
      "7         Good  Reference/Equivalent Method     EPA_AQS  \n",
      "8         Good  Reference/Equivalent Method     EPA_AQS  \n",
      "9         Good  Reference/Equivalent Method     EPA_AQS  \n",
      "\n",
      "Colonnes disponibles : ['city', 'country', 'latitude', 'longitude', 'year', 'month', 'pm25_ugm3', 'no2_ugm3', 'data_quality', 'measurement_method', 'data_source']\n",
      "Nombre total de lignes : 6480\n",
      "\n",
      "=== Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6480 entries, 0 to 6479\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   city                6480 non-null   object \n",
      " 1   country             6480 non-null   object \n",
      " 2   latitude            6480 non-null   float64\n",
      " 3   longitude           6480 non-null   float64\n",
      " 4   year                6480 non-null   int64  \n",
      " 5   month               6480 non-null   int64  \n",
      " 6   pm25_ugm3           6480 non-null   float64\n",
      " 7   no2_ugm3            6480 non-null   float64\n",
      " 8   data_quality        6480 non-null   object \n",
      " 9   measurement_method  6480 non-null   object \n",
      " 10  data_source         6480 non-null   object \n",
      "dtypes: float64(4), int64(2), object(5)\n",
      "memory usage: 557.0+ KB\n",
      "None\n",
      "\n",
      "=== Valeurs manquantes par colonne ===\n",
      "city                  0\n",
      "country               0\n",
      "latitude              0\n",
      "longitude             0\n",
      "year                  0\n",
      "month                 0\n",
      "pm25_ugm3             0\n",
      "no2_ugm3              0\n",
      "data_quality          0\n",
      "measurement_method    0\n",
      "data_source           0\n",
      "dtype: int64\n",
      "\n",
      "Nombre de doublons d√©tect√©s : 0\n",
      "\n",
      "=== Statistiques descriptives ===\n",
      "                     count unique                       top  freq       mean  \\\n",
      "city                  6480     20                  New York   324        NaN   \n",
      "country               6480     10                       USA  3240        NaN   \n",
      "latitude            6480.0    NaN                       NaN   NaN   31.53551   \n",
      "longitude           6480.0    NaN                       NaN   NaN -35.877325   \n",
      "year                6480.0    NaN                       NaN   NaN     2012.0   \n",
      "month               6480.0    NaN                       NaN   NaN        6.5   \n",
      "pm25_ugm3           6480.0    NaN                       NaN   NaN   40.96821   \n",
      "no2_ugm3            6480.0    NaN                       NaN   NaN  39.617276   \n",
      "data_quality          6480      3                      Good  4917        NaN   \n",
      "measurement_method    6480      2  Federal Reference Method  5040        NaN   \n",
      "data_source           6480      2                   EPA_AQS  3240        NaN   \n",
      "\n",
      "                          std       min      25%      50%       75%       max  \n",
      "city                      NaN       NaN      NaN      NaN       NaN       NaN  \n",
      "country                   NaN       NaN      NaN      NaN       NaN       NaN  \n",
      "latitude            16.603137  -23.5505  29.2441  33.7503  40.14265     52.52  \n",
      "longitude           81.511132 -121.8863 -98.6535 -74.5856   5.88565  139.6503  \n",
      "year                 7.789482    1999.0   2005.0   2012.0    2019.0    2025.0  \n",
      "month                3.452319       1.0     3.75      6.5      9.25      12.0  \n",
      "pm25_ugm3           36.303963       5.1  19.3375   29.225     46.08    274.18  \n",
      "no2_ugm3            16.711882     10.25    27.08   36.845   48.9225    110.27  \n",
      "data_quality              NaN       NaN      NaN      NaN       NaN       NaN  \n",
      "measurement_method        NaN       NaN      NaN      NaN       NaN       NaN  \n",
      "data_source               NaN       NaN      NaN      NaN       NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "# Bloc 3a ‚Äî Inspection et validation du fichier air_quality_global.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "aq_path = \"data/air_quality_global.csv\"\n",
    "\n",
    "# Charger un √©chantillon pour inspection\n",
    "df_aq = pd.read_csv(aq_path)\n",
    "\n",
    "# Aper√ßu g√©n√©ral\n",
    "print(\"=== Aper√ßu du dataset ===\")\n",
    "print(df_aq.head(10))   # premi√®res lignes\n",
    "print(\"\\nColonnes disponibles :\", df_aq.columns.tolist())\n",
    "print(\"Nombre total de lignes :\", len(df_aq))\n",
    "\n",
    "# V√©rification des types et valeurs manquantes\n",
    "print(\"\\n=== Info ===\")\n",
    "print(df_aq.info())\n",
    "\n",
    "print(\"\\n=== Valeurs manquantes par colonne ===\")\n",
    "print(df_aq.isna().sum())\n",
    "\n",
    "# V√©rification des doublons\n",
    "nb_duplicates = df_aq.duplicated().sum()\n",
    "print(f\"\\nNombre de doublons d√©tect√©s : {nb_duplicates}\")\n",
    "\n",
    "# Aper√ßu statistique des colonnes num√©riques\n",
    "print(\"\\n=== Statistiques descriptives ===\")\n",
    "print(df_aq.describe(include='all').transpose().head(20))\n",
    "\n",
    "# Log\n",
    "log_event(\"INFO\", f\"Inspection du fichier {aq_path} effectu√©e : {len(df_aq)} lignes, {nb_duplicates} doublons\")\n",
    "append_summary_md(f\"- {utc_timestamp()} [INFO] Inspection du fichier {aq_path} : {len(df_aq)} lignes, {nb_duplicates} doublons\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f927a",
   "metadata": {
    "id": "N3nVABN-EfsA"
   },
   "source": [
    "Quick summary: We will calculate\n",
    "ùëá\n",
    "log\n",
    "‚Å°\n",
    "in global mode on the entire PM2.5 dataset (n = 6480, d = 1), then prepare a variant for each city (e.g., New York, n = 324).\n",
    "\n",
    "Block 3b ‚Äî Calculation of T_log (global PM2.5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOla8TlubWbwBcM47+jK5BC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
