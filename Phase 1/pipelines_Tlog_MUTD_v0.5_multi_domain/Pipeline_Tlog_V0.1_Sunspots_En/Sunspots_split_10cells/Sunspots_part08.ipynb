{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24520d4d",
   "metadata": {},
   "source": [
    "Résumé des tests annotés\n",
    "\n",
    "Échantillons appariés : 150 paires pour chaque valeur de lambda_max (0.1, 0.2, 0.4).\n",
    "\n",
    "Médianes : Levina‑Bickel m_hat médiane = 7.940; spectral d_s médian = 0.212 (λ=0.1), 0.299 (λ=0.2), 0.518 (λ=0.4).\n",
    "\n",
    "Différence moyenne (Levina − spectral) ≈ 7.73 (λ=0.1), 7.64 (λ=0.2), 7.43 (λ=0.4); écart-type des différences ≈ 0.053–0.055.\n",
    "\n",
    "Tests statistiques : Wilcoxon signed‑rank p ≈ 2.30e‑26 pour chaque λ_max (fortement significatif). Paired t p essentially 0 (ordre de 1e‑319 … 0) — la différence est extrêmement statistiquement significative.\n",
    "\n",
    "Corrélations : Spearman rho ≈ 0.11 (p≈0.18) pour λ=0.1, −0.036 (p≈0.66) pour 0.2, ≈0.003 (p≈0.97) pour 0.4 — aucune corrélation robuste entre m_hat et d_s aux trois réglages.\n",
    "\n",
    "Taille d’effet (Cohen d, apparié) : très grande (≈ 135–147) due à la très faible variance des différences relatives à la très grande moyenne de la différence — mathématiquement correcte mais peu informative ici (différences ≫ écart‑type).\n",
    "\n",
    "Interprétation concise et implication\n",
    "Les deux estimateurs ne donnent pas des valeurs comparables en échelle absolue : Levina‑Bickel retourne une dimension locale ≈ 8 tandis que la pente spectrale, pour la plage small‑λ choisie, renvoie d_s ≪ 1 (λ_small) ou ≈ 0.5 (λ=0.4).\n",
    "\n",
    "Les tests montrent une différence statistiquement robuste entre méthodes, mais la corrélation faible/absente indique qu’elles ne varient pas ensemble de façon cohérente sur les mêmes sous‑échantillons.\n",
    "\n",
    "Conclusion pratique : tu peux affirmer que « Levina‑Bickel et la pente spectrale (avec ces λ_max) fournissent des estimations incompatibles en valeur absolue » — il faut préciser la définition de « dimension » que tu rapportes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fbde5",
   "metadata": {},
   "source": [
    "Cell Python — Diagnostics de linéarité et résidus pour les fits log‑log (sélection de sous‑échantillons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de6a4c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved diag 1: n_points_fit=17, fit_ok=True\n",
      "Saved diag 2: n_points_fit=17, fit_ok=True\n",
      "Saved diag 3: n_points_fit=16, fit_ok=True\n",
      "Saved diag 4: n_points_fit=17, fit_ok=True\n",
      "Saved diag 5: n_points_fit=17, fit_ok=True\n",
      "Saved diag 6: n_points_fit=17, fit_ok=True\n",
      "Saved diag 7: n_points_fit=17, fit_ok=True\n",
      "Saved diag 8: n_points_fit=18, fit_ok=True\n",
      "Saved diag 9: n_points_fit=16, fit_ok=True\n",
      "Saved diag 10: n_points_fit=16, fit_ok=True\n",
      "Diagnostics saved to results/spectral_diagnostics_linearity\n"
     ]
    }
   ],
   "source": [
    "# Cell: Linear regression diagnostics on log N(lambda) vs log lambda for selected bootstrap subsamples\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters (modifiable)\n",
    "csv_path = 'data/sunspots_raw/Sunspots.csv'\n",
    "value_col_candidates = ['Number', 'Total Sunspot', 'Total Sunspot Number', 'Monthly Mean']\n",
    "embedding_dim = 10\n",
    "tau = 1\n",
    "k_neighbors = 10\n",
    "n_eig = 400                 # compute more eigenvalues to inspect small-to-mid range\n",
    "subsample_frac = 0.6\n",
    "rng_seed = 42\n",
    "n_diagnostics = 10          # number of bootstrap samples to inspect (choose 10 representative)\n",
    "lambda_max_diag = 0.2       # primary lambda_max used for the diagnostics (adjustable)\n",
    "min_points_for_fit = 6\n",
    "out_dir = 'results/spectral_diagnostics_linearity'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Utilities\n",
    "def takens_embed(x, dim, tau):\n",
    "    m = len(x) - (dim - 1) * tau\n",
    "    if m <= 0:\n",
    "        return None\n",
    "    embed = np.empty((m, dim))\n",
    "    for i in range(dim):\n",
    "        embed[:, i] = x[i * tau : i * tau + m]\n",
    "    return embed\n",
    "\n",
    "def build_laplacian_eigs(X_points, k_neighbors, n_eig):\n",
    "    n_nodes_local = X_points.shape[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k_neighbors + 1, n_nodes_local), algorithm='auto').fit(X_points)\n",
    "    distances, indices = nbrs.kneighbors(X_points)\n",
    "    adj = sparse.lil_matrix((n_nodes_local, n_nodes_local), dtype=np.float32)\n",
    "    for i in range(n_nodes_local):\n",
    "        for j in indices[i, 1:]:\n",
    "            adj[i, j] = 1.0\n",
    "            adj[j, i] = 1.0\n",
    "    adj = adj.tocsr()\n",
    "    deg = np.array(adj.sum(axis=1)).flatten()\n",
    "    deg[deg == 0] = 1.0\n",
    "    D_inv_sqrt = sparse.diags(1.0 / np.sqrt(deg))\n",
    "    I = sparse.identity(n_nodes_local, format='csr')\n",
    "    L_norm = I - D_inv_sqrt @ adj @ D_inv_sqrt\n",
    "    n_eig_local = min(n_eig, n_nodes_local - 1)\n",
    "    try:\n",
    "        eigvals, _ = eigsh(L_norm, k=n_eig_local, which='SM', tol=1e-6, maxiter=5000)\n",
    "    except Exception:\n",
    "        try:\n",
    "            from scipy.linalg import eigh\n",
    "            Ld = L_norm.toarray()\n",
    "            eigvals_all = eigh(Ld, eigvals_only=True)\n",
    "            eigvals = np.sort(eigvals_all)[:n_eig_local]\n",
    "        except Exception as e:\n",
    "            print(\"Eigen decomposition failed:\", e)\n",
    "            return None\n",
    "    return np.sort(eigvals)\n",
    "\n",
    "def spectral_counting(eigvals):\n",
    "    eps = 1e-12\n",
    "    lams = eigvals[eigvals > eps]\n",
    "    lam_vals = np.unique(lams)\n",
    "    N_vals = np.array([np.searchsorted(lams, lam, side='right') for lam in lam_vals])\n",
    "    return lam_vals, N_vals\n",
    "\n",
    "def linreg_diagnostics(lam_fit, N_fit):\n",
    "    # linear regression on log-log\n",
    "    x = np.log(lam_fit)\n",
    "    y = np.log(N_fit)\n",
    "    n = len(x)\n",
    "    slope, intercept, r_value, p_value, stderr = stats.linregress(x, y)\n",
    "    y_pred = intercept + slope * x\n",
    "    resid = y - y_pred\n",
    "    mse = np.sum(resid**2) / max(n - 2, 1)\n",
    "    ss_tot = np.sum((y - np.mean(y))**2)\n",
    "    r2 = 1.0 - np.sum(resid**2) / ss_tot if ss_tot > 0 else np.nan\n",
    "    # leverage for simple linear regression\n",
    "    xbar = np.mean(x)\n",
    "    Sxx = np.sum((x - xbar)**2)\n",
    "    h = np.repeat(1.0/n, n)\n",
    "    if Sxx > 0:\n",
    "        h = 1.0/n + ((x - xbar)**2) / Sxx\n",
    "    # standardized residuals\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        std_resid = resid / np.sqrt(mse * (1 - h))\n",
    "    # Cook's distance approximation\n",
    "    p = 2  # intercept + slope\n",
    "    cooks = (resid**2) / (p * mse) * (h / (1 - h)**2)\n",
    "    return dict(\n",
    "        slope=slope, intercept=intercept, r_value=r_value, p_value=p_value, stderr=stderr,\n",
    "        r2=r2, mse=mse, x=x, y=y, y_pred=y_pred, resid=resid, std_resid=std_resid, h=h, cooks=cooks\n",
    "    )\n",
    "\n",
    "# Load series and embedding\n",
    "df0 = pd.read_csv(csv_path)\n",
    "col = next((c for c in value_col_candidates if c in df0.columns), None)\n",
    "if col is None:\n",
    "    numeric_cols = df0.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        raise RuntimeError(\"No numeric column found in CSV.\")\n",
    "    col = numeric_cols[-1]\n",
    "series = pd.to_numeric(df0[col], errors='coerce').dropna().values\n",
    "X_full = takens_embed(series, embedding_dim, tau)\n",
    "if X_full is None:\n",
    "    raise RuntimeError(\"Embedding too short for given embedding_dim/tau.\")\n",
    "n_nodes = X_full.shape[0]\n",
    "\n",
    "# Select diagnostics indices deterministically\n",
    "rng = np.random.default_rng(rng_seed)\n",
    "indices_list = [rng.choice(np.arange(n_nodes), size=max(120, int(np.floor(subsample_frac * n_nodes))), replace=False)\n",
    "                for _ in range(n_diagnostics)]\n",
    "\n",
    "summary_rows = []\n",
    "for i, idx in enumerate(indices_list, start=1):\n",
    "    X_sub = X_full[idx, :]\n",
    "    eigvals = build_laplacian_eigs(X_sub, k_neighbors, n_eig)\n",
    "    if eigvals is None:\n",
    "        print(f\"diag {i}: eig failed; skipping\")\n",
    "        continue\n",
    "    lam_vals, N_vals = spectral_counting(eigvals)\n",
    "    mask = lam_vals <= lambda_max_diag\n",
    "    lam_fit = lam_vals[mask]\n",
    "    N_fit = N_vals[mask]\n",
    "    n_points = len(lam_fit)\n",
    "    result = None\n",
    "    if n_points >= min_points_for_fit:\n",
    "        result = linreg_diagnostics(lam_fit, N_fit)\n",
    "    # Save raw counting and eigvals\n",
    "    pd.DataFrame({'lambda': lam_vals, 'N_lambda': N_vals}).to_csv(f\"{out_dir}/diag_{i:02d}_counting.csv\", index=False)\n",
    "    pd.DataFrame({'eig_index': np.arange(1, len(eigvals)+1), 'eigval': eigvals}).to_csv(f\"{out_dir}/diag_{i:02d}_eigvals.csv\", index=False)\n",
    "    # Plot log-log with fit and diagnostic panels\n",
    "    plt.figure(figsize=(10,4))\n",
    "    ax1 = plt.subplot2grid((1,3), (0,0), colspan=2)\n",
    "    ax2 = plt.subplot2grid((1,3), (0,2))\n",
    "    # left: log-log + fit\n",
    "    ax1.loglog(lam_vals, N_vals, 'o', markersize=4, alpha=0.6, label='N(lambda)')\n",
    "    if result is not None:\n",
    "        # plot fitted line over lam_fit\n",
    "        lam_line = np.linspace(lam_fit.min(), lam_fit.max(), 200)\n",
    "        ax1.loglog(lam_line, np.exp(result['intercept']) * lam_line**(result['slope']), '-', color='C1', lw=1.5,\n",
    "                   label=f'fit (<= {lambda_max_diag}) slope={result[\"slope\"]:.3f} (stderr={result[\"stderr\"]:.3f}) R2={result[\"r2\"]:.3f}')\n",
    "    ax1.set_xlabel('lambda (eigenvalue)')\n",
    "    ax1.set_ylabel('N(lambda)')\n",
    "    ax1.set_title(f'diag {i}: log-log N(lambda) (n_points_fit={n_points})')\n",
    "    ax1.legend(fontsize=8)\n",
    "    ax1.grid(alpha=0.3, which='both')\n",
    "    # right: residuals vs fitted + Cook's high points\n",
    "    if result is not None:\n",
    "        ax2.plot(result['y_pred'], result['resid'], 'o', ms=5, alpha=0.7)\n",
    "        ax2.axhline(0, color='gray', lw=1)\n",
    "        ax2.set_xlabel('fitted log N')\n",
    "        ax2.set_ylabel('residuals')\n",
    "        ax2.set_title('residuals vs fitted')\n",
    "        # highlight potential influential points (Cook > 4/n)\n",
    "        cooks = result['cooks']\n",
    "        thresh = 4.0 / len(cooks) if len(cooks)>0 else 0\n",
    "        infl_idx = np.where(cooks > thresh)[0]\n",
    "        for ii in infl_idx:\n",
    "            ax2.annotate(f\"{ii+1}\", (result['y_pred'][ii], result['resid'][ii]), fontsize=7, color='red')\n",
    "        # inset: qqplot of standardized residuals\n",
    "        plt.sca(ax2)\n",
    "        ax_in = ax2.inset_axes([0.05, -0.65, 0.9, 0.6])\n",
    "        stats.probplot(result['std_resid'], dist=\"norm\", plot=ax_in)\n",
    "        ax_in.set_title('QQ std resid', fontsize=7)\n",
    "    else:\n",
    "        ax2.text(0.1, 0.5, f\"Insufficient fit points\\n(n_points={n_points})\", transform=ax2.transAxes)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_dir}/diag_{i:02d}_diagnostic.png\", dpi=150)\n",
    "    plt.close()\n",
    "    # collect summary\n",
    "    summary_rows.append({\n",
    "        'diag': i,\n",
    "        'n_nodes_sub': int(X_sub.shape[0]),\n",
    "        'n_eig_computed': int(len(eigvals)),\n",
    "        'n_lambda_total': int(len(lam_vals)),\n",
    "        'n_points_fit': int(n_points),\n",
    "        'fit_ok': bool(result is not None),\n",
    "        'slope': float(result['slope']) if result is not None else np.nan,\n",
    "        'stderr_slope': float(result['stderr']) if result is not None else np.nan,\n",
    "        'r2': float(result['r2']) if result is not None else np.nan,\n",
    "        'mse': float(result['mse']) if result is not None else np.nan,\n",
    "        'max_cook': float(np.nanmax(result['cooks'])) if result is not None else np.nan,\n",
    "        'n_influential': int(np.sum(result['cooks'] > (4.0 / max(1, len(result['cooks']))))) if result is not None else 0\n",
    "    })\n",
    "    print(f\"Saved diag {i}: n_points_fit={n_points}, fit_ok={result is not None}\")\n",
    "\n",
    "# Save diagnostics summary\n",
    "pd.DataFrame(summary_rows).to_csv(f\"{out_dir}/linearity_diagnostics_summary.csv\", index=False)\n",
    "print(\"Diagnostics saved to\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35cbcb3",
   "metadata": {},
   "source": [
    "Observations clés (d’après linearity_diagnostics_summary.csv)\n",
    "Fits retenus : 10 diagnostics, n_points_fit = 16–18 → plage λ ≤ 0.2 donne suffisamment de points pour la régression log‑log.\n",
    "\n",
    "Pentes (slope) sur λ_max = 0.2 : ≈ 0.140 − 0.154 → d_s = 2*slope ≈ 0.28 − 0.31 pour ces sous‑échantillons.\n",
    "\n",
    "Erreur standard des pentes : ~0.027–0.029 → estimation de la pente relativement précise.\n",
    "\n",
    "R² des fits : ~0.64–0.66 → la relation log N vs log λ est modérément bien expliquée par une droite linéaire sur la plage choisie.\n",
    "\n",
    "Diagnostics d’influence : max_cook ≫ 1 (≈100+) et n_influential = 1 pour chaque diag — il existe au moins un point très influent par fit (Cook élevé, dû à la formule approximative pour petites n et résidus).\n",
    "\n",
    "MSE et résidus : MSE ~0.23–0.25 (en log‑espace) — résidus non négligeables mais pas catastrophiques.\n",
    "\n",
    "Interprétation rapide\n",
    "Les régressions log‑log sur λ ≤ 0.2 sont raisonnablement linéaires pour ces sous‑échantillons (R² ~0.65) ; les pentes sont stables entre diagnostics.\n",
    "\n",
    "Le fait qu’il y ait 1 point « influent » par fit suggère que la pente est parfois tirée par quelques petites valeurs propres/points de comptage — ce qui peut expliquer la faible échelle absolue de d_s comparée à m_hat (Levina).\n",
    "\n",
    "En clair : la méthode spectrale, sur la plage λ choisie, produit une estimation cohérente et précise (faible stderr) mais d’échelle bien différente de Levina — la discordance n’est pas due à un fit très mauvais, mais à ce que chaque méthode mesure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a99f1f",
   "metadata": {},
   "source": [
    "Cell Python — Influence diagnostics, refit sans influents et régressions robustes (Theil‑Sen + RANSAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0740bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01: n_points=17, n_infl=1, slope_orig=0.1542, slope_noinfl=0.5558200020115645\n",
      "02: n_points=17, n_infl=1, slope_orig=0.1459, slope_noinfl=0.5501239674590426\n",
      "03: n_points=16, n_infl=1, slope_orig=0.1478, slope_noinfl=0.5445538208615759\n",
      "04: n_points=17, n_infl=1, slope_orig=0.1510, slope_noinfl=0.560048104583994\n",
      "05: n_points=17, n_infl=1, slope_orig=0.1518, slope_noinfl=0.5635590512413825\n",
      "06: n_points=17, n_infl=1, slope_orig=0.1544, slope_noinfl=0.5603363702308413\n",
      "07: n_points=17, n_infl=1, slope_orig=0.1496, slope_noinfl=0.5629016353983204\n",
      "08: n_points=18, n_infl=1, slope_orig=0.1541, slope_noinfl=0.5625108274735132\n",
      "09: n_points=16, n_infl=1, slope_orig=0.1461, slope_noinfl=0.5495966987448233\n",
      "10: n_points=16, n_infl=1, slope_orig=0.1403, slope_noinfl=0.5428415309250458\n",
      "Saved influence/refit outputs to results/spectral_influence_refit\n"
     ]
    }
   ],
   "source": [
    "# Cell: Identify influential points (Cook threshold), refit without them, compare with Theil-Sen and RANSAC\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import TheilSenRegressor, RANSACRegressor, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paramètres (adapter si besoin)\n",
    "in_dir = 'results/spectral_diagnostics_linearity'   # dossier où sont les diagnostics précédents\n",
    "out_dir = 'results/spectral_influence_refit'\n",
    "lambda_max_diag = 0.2   # même plage que diagnostics précédents\n",
    "min_points_for_fit = 6\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Charger les CSV produits précédemment (diag_XX_counting.csv et diag_XX_eigvals.csv attendus)\n",
    "diag_files = sorted([f for f in os.listdir(in_dir) if f.endswith('_counting.csv')])\n",
    "if not diag_files:\n",
    "    raise RuntimeError(f\"No counting CSVs found in {in_dir}\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "def fit_linear_on_log(lam, N):\n",
    "    x = np.log(lam).reshape(-1, 1)\n",
    "    y = np.log(N)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x, y)\n",
    "    y_pred = lr.predict(x)\n",
    "    resid = y - y_pred\n",
    "    n = len(x)\n",
    "    p = 2\n",
    "    mse = np.sum(resid**2) / max(n - p, 1)\n",
    "    # standard error for slope\n",
    "    se_slope = np.sqrt(mse / np.sum((x.flatten() - x.mean())**2)) if np.sum((x.flatten() - x.mean())**2) > 0 else np.nan\n",
    "    # R^2\n",
    "    ss_tot = np.sum((y - y.mean())**2)\n",
    "    r2 = 1.0 - np.sum(resid**2) / ss_tot if ss_tot > 0 else np.nan\n",
    "    return {\n",
    "        'model': lr,\n",
    "        'slope': float(lr.coef_[0]),\n",
    "        'intercept': float(lr.intercept_),\n",
    "        'y_pred': y_pred,\n",
    "        'resid': resid,\n",
    "        'mse': mse,\n",
    "        'se_slope': se_slope,\n",
    "        'r2': r2,\n",
    "        'x': x.flatten(),\n",
    "        'y': y\n",
    "    }\n",
    "\n",
    "def compute_leverage_and_cooks(x, resid, mse):\n",
    "    # x: 1D log-lam values\n",
    "    n = len(x)\n",
    "    xbar = np.mean(x)\n",
    "    Sxx = np.sum((x - xbar)**2)\n",
    "    if Sxx <= 0:\n",
    "        h = np.repeat(1.0/n, n)\n",
    "    else:\n",
    "        h = 1.0/n + ((x - xbar)**2) / Sxx\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        std_resid = resid / np.sqrt(mse * (1 - h))\n",
    "    p = 2\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cooks = (resid**2) / (p * mse) * (h / (1 - h)**2)\n",
    "    return h, std_resid, cooks\n",
    "\n",
    "def fit_theilsen(lam, N):\n",
    "    x = np.log(lam).reshape(-1, 1)\n",
    "    y = np.log(N)\n",
    "    if len(x) < 3:\n",
    "        return None\n",
    "    ts = TheilSenRegressor(random_state=0)\n",
    "    ts.fit(x, y)\n",
    "    y_pred = ts.predict(x)\n",
    "    resid = y - y_pred\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    return {'model': ts, 'slope': float(ts.coef_[0]), 'intercept': float(ts.intercept_),\n",
    "            'y_pred': y_pred, 'resid': resid, 'mse': mse}\n",
    "\n",
    "def fit_ransac(lam, N):\n",
    "    x = np.log(lam).reshape(-1, 1)\n",
    "    y = np.log(N)\n",
    "    if len(x) < 3:\n",
    "        return None\n",
    "    base = LinearRegression()\n",
    "    ransac = RANSACRegressor(estimator=base, random_state=0)\n",
    "    try:\n",
    "        ransac.fit(x, y)\n",
    "    except Exception:\n",
    "        return None\n",
    "    # Try to extract slope/intercept robustly\n",
    "    slope = np.nan\n",
    "    intercept = np.nan\n",
    "    try:\n",
    "        # estimator_ exists and should be a fitted LinearRegression\n",
    "        est = getattr(ransac, 'estimator_', None)\n",
    "        if est is not None and hasattr(est, 'coef_'):\n",
    "            slope = float(est.coef_[0])\n",
    "            intercept = float(est.intercept_)\n",
    "        else:\n",
    "            # fallback: predict two points to infer slope/intercept\n",
    "            xp = np.array([[x.min()], [x.max()]])\n",
    "            yp = ransac.predict(xp)\n",
    "            slope = float((yp[1] - yp[0]) / (xp[1,0] - xp[0,0]))\n",
    "            intercept = float(yp[0] - slope * xp[0,0])\n",
    "    except Exception:\n",
    "        slope = np.nan\n",
    "        intercept = np.nan\n",
    "    y_pred = ransac.predict(x)\n",
    "    resid = y - y_pred\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    return {'model': ransac, 'slope': slope, 'intercept': intercept,\n",
    "            'y_pred': y_pred, 'resid': resid, 'mse': mse}\n",
    "\n",
    "# Process each diagnostic file\n",
    "for fname in diag_files:\n",
    "    diag_tag = fname.replace('_counting.csv', '').replace('diag_', '')\n",
    "    csv_path = os.path.join(in_dir, fname)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    lam_vals = df['lambda'].values\n",
    "    N_vals = df['N_lambda'].values\n",
    "    # select <= lambda_max_diag\n",
    "    mask = lam_vals <= lambda_max_diag\n",
    "    lam_fit = lam_vals[mask]\n",
    "    N_fit = N_vals[mask]\n",
    "    n_points = len(lam_fit)\n",
    "    if n_points < min_points_for_fit:\n",
    "        summary_rows.append({\n",
    "            'diag': diag_tag,\n",
    "            'n_points_fit': n_points,\n",
    "            'fit_ok': False\n",
    "        })\n",
    "        print(f\"{diag_tag}: insufficient points ({n_points})\")\n",
    "        continue\n",
    "\n",
    "    # Original OLS on log-log\n",
    "    res_orig = fit_linear_on_log(lam_fit, N_fit)\n",
    "    h, std_resid, cooks = compute_leverage_and_cooks(res_orig['x'], res_orig['resid'], res_orig['mse'])\n",
    "    # influential threshold: Cook > 4/n (common rule of thumb)\n",
    "    cook_thresh = 4.0 / max(1, n_points)\n",
    "    infl_idx = np.where(cooks > cook_thresh)[0]\n",
    "    n_infl = int(len(infl_idx))\n",
    "\n",
    "    # Refit excluding influential points (if any)\n",
    "    if n_infl > 0:\n",
    "        lam_noinfl = np.delete(lam_fit, infl_idx)\n",
    "        N_noinfl = np.delete(N_fit, infl_idx)\n",
    "        res_noinfl = fit_linear_on_log(lam_noinfl, N_noinfl) if len(lam_noinfl) >= min_points_for_fit else None\n",
    "    else:\n",
    "        res_noinfl = None\n",
    "\n",
    "    # Robust fits\n",
    "    res_ts = fit_theilsen(lam_fit, N_fit)\n",
    "    res_ransac = fit_ransac(lam_fit, N_fit)\n",
    "\n",
    "    # Save a comparison plot (log-log)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    ax = plt.gca()\n",
    "    ax.loglog(lam_fit, N_fit, 'o', ms=4, alpha=0.6, label='N(lambda) data')\n",
    "    xline = np.linspace(lam_fit.min(), lam_fit.max(), 200)\n",
    "\n",
    "    # OLS\n",
    "    y_ols = np.exp(res_orig['intercept']) * xline**(res_orig['slope'])\n",
    "    ax.loglog(xline, y_ols, '-', color='C1', lw=1.5, label=f'OLS slope={res_orig[\"slope\"]:.3f}')\n",
    "    # OLS without influents\n",
    "    if res_noinfl is not None:\n",
    "        y_noinfl = np.exp(res_noinfl['intercept']) * xline**(res_noinfl['slope'])\n",
    "        ax.loglog(xline, y_noinfl, '--', color='C2', lw=1.5, label=f'OLS w/o influ slope={res_noinfl[\"slope\"]:.3f}')\n",
    "    # Theil-Sen\n",
    "    if res_ts is not None:\n",
    "        y_ts = np.exp(res_ts['intercept']) * xline**(res_ts['slope'])\n",
    "        ax.loglog(xline, y_ts, '-.', color='C3', lw=1.5, label=f'Theil-Sen slope={res_ts[\"slope\"]:.3f}')\n",
    "    # RANSAC\n",
    "    if res_ransac is not None and not np.isnan(res_ransac['slope']):\n",
    "        y_ransac = np.exp(res_ransac['intercept']) * xline**(res_ransac['slope'])\n",
    "        ax.loglog(xline, y_ransac, ':', color='C4', lw=1.5, label=f'RANSAC slope={res_ransac[\"slope\"]:.3f}')\n",
    "\n",
    "    ax.set_xlabel('lambda (eigenvalue)')\n",
    "    ax.set_ylabel('N(lambda)')\n",
    "    ax.set_title(f'{diag_tag} influence/refit (n_points={n_points}, n_infl={n_infl})')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3, which='both')\n",
    "\n",
    "    # annotate influential points on plot\n",
    "    if n_infl > 0:\n",
    "        for ii in infl_idx:\n",
    "            lam_pt = lam_fit[ii]\n",
    "            N_pt = N_fit[ii]\n",
    "            ax.loglog([lam_pt], [N_pt], 's', color='red', ms=6, label='_nolegend_')\n",
    "            ax.annotate(str(ii+1), xy=(lam_pt, N_pt), xytext=(5, -5), textcoords='offset points', color='red', fontsize=7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f'{diag_tag}_influence_refit.png'), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Save table of influential points (if any)\n",
    "    infl_table = []\n",
    "    for ii in infl_idx:\n",
    "        infl_table.append({'diag': diag_tag, 'index_in_fit': int(ii), 'lambda': float(lam_fit[ii]), 'N_lambda': int(N_fit[ii]), 'cook': float(cooks[ii]), 'leverage': float(h[ii]), 'std_resid': float(std_resid[ii])})\n",
    "    if infl_table:\n",
    "        pd.DataFrame(infl_table).to_csv(os.path.join(out_dir, f'{diag_tag}_influential_points.csv'), index=False)\n",
    "\n",
    "    # Collect summary row\n",
    "    summary_rows.append({\n",
    "        'diag': diag_tag,\n",
    "        'n_points_fit': int(n_points),\n",
    "        'fit_ok': True,\n",
    "        'slope_orig': float(res_orig['slope']),\n",
    "        'stderr_orig': float(res_orig['se_slope']),\n",
    "        'r2_orig': float(res_orig['r2']),\n",
    "        'n_influential': int(n_infl),\n",
    "        'slope_noinfl': float(res_noinfl['slope']) if res_noinfl is not None else np.nan,\n",
    "        'stderr_noinfl': float(res_noinfl['se_slope']) if res_noinfl is not None else np.nan,\n",
    "        'r2_noinfl': float(res_noinfl['r2']) if res_noinfl is not None else np.nan,\n",
    "        'slope_theilsen': float(res_ts['slope']) if res_ts is not None else np.nan,\n",
    "        'slope_ransac': float(res_ransac['slope']) if res_ransac is not None else np.nan,\n",
    "        'cook_thresh': float(cook_thresh),\n",
    "        'max_cook': float(np.max(cooks))\n",
    "    })\n",
    "\n",
    "    print(f\"{diag_tag}: n_points={n_points}, n_infl={n_infl}, slope_orig={res_orig['slope']:.4f}, slope_noinfl={summary_rows[-1]['slope_noinfl'] if not np.isnan(summary_rows[-1]['slope_noinfl']) else 'NA'}\")\n",
    "\n",
    "# Save summary CSV\n",
    "pd.DataFrame(summary_rows).to_csv(os.path.join(out_dir, 'influence_refit_summary.csv'), index=False)\n",
    "print(\"Saved influence/refit outputs to\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719b8a2",
   "metadata": {},
   "source": [
    "Résumé court et immédiat\n",
    "\n",
    "Retirer le(s) point(s) influent(s) change drastiquement la pente : les pentes OLS originales sont ≈ 0.14–0.154 (d_s ≈ 0.28–0.31) alors que les pentes recalculées sans le point influent passent à ≈ 0.54–0.56 (d_s ≈ 1.08–1.12) — factor ~3–4 d’écart sur slope.\n",
    "\n",
    "Les estimateurs robustes (Theil‑Sen, RANSAC) fournissent eux aussi des pentes élevées, souvent proches de la version « OLS sans influent » (ex. slope_theilsen ≈ 0.72–0.85; slope_ransac ≈ slope_noinfl pour la plupart des diagnostics).\n",
    "\n",
    "Donc l’estimation spectrale sur λ ≤ 0.2 est fortement sensible à un unique point influent par fit : soit ce point est un artefact/petit λ isolé qui écrase la pente, soit l’OLS est inadapté et la pente « réelle » est plus élevée.\n",
    "\n",
    "Ce que les fichiers produits contiennent (où regarder en priorité)\n",
    "\n",
    "results/spectral_influence_refit/influence_refit_summary.csv — résumé comparatif (OLS orig, OLS sans influent, Theil‑Sen, RANSAC) — tu l’as joint.\n",
    "\n",
    "results/spectral_influence_refit/diag_##_influential_points.csv — pour chaque diag, indices et valeurs (lambda, N_lambda, cook, leverage, std_resid) des points influents — ouvre-les pour voir la valeur propre incriminée.\n",
    "\n",
    "results/spectral_influence_refit/diag_##_influence_refit.png — plot comparatif (données, OLS, OLS w/o influent, Theil‑Sen, RANSAC) — inspecte ces images.\n",
    "\n",
    "Interprétation pratique\n",
    "\n",
    "L’OLS original (avec tous les points) donne une pente faible et stable mais qui repose sur un jeu de points dont un seul est très influent (Cook >> threshold).\n",
    "\n",
    "Les approches robustes ou la suppression de l’influent produisent pentes nettement plus hautes : ça signifie que la décision méthodologique (inclure/exclure ou utiliser un estimateur robuste) change radicalement l’interprétation quantitative de d_s.\n",
    "\n",
    "Statistiquement, OLS+point influent → d_s ≪ m_hat ; OLS‑sans‑influents / robust → d_s augmenté (encore loin de Levina mais plus proche)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68f82b",
   "metadata": {},
   "source": [
    "Cell Python — Synthèse comparative des d_s (orig, sans influent, Theil‑Sen, RANSAC) + tests appariés et figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db66a9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aggregates: results/spectral_influence_summary\\d_s_methods_aggregate.csv\n",
      "Saved paired tests: results/spectral_influence_summary\\d_s_paired_tests.csv\n",
      "Saved per-diag table: results/spectral_influence_summary\\d_s_per_diag.csv\n",
      "Saved figure: results/spectral_influence_summary\\d_s_methods_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# Cell: summarize d_s = 2*slope for methods (OLS orig, OLS w/o influent, Theil-Sen, RANSAC),\n",
    "# compute medians/IQR, paired Wilcoxon tests between methods, save CSV and plot.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "in_dir = 'results/spectral_influence_refit'\n",
    "paired_in_path = 'results/paired_levina_spectral_raw.csv'  # available but not used for diag-level summary\n",
    "out_dir = 'results/spectral_influence_summary'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Load the influence/refit summary produced previously\n",
    "summary_fp = os.path.join(in_dir, 'influence_refit_summary.csv')\n",
    "if not os.path.exists(summary_fp):\n",
    "    raise RuntimeError(f\"File not found: {summary_fp}\")\n",
    "\n",
    "inf_df = pd.read_csv(summary_fp)\n",
    "\n",
    "# Keep only rows with fit_ok True (safety)\n",
    "inf_df = inf_df[inf_df['fit_ok'] == True].copy()\n",
    "if inf_df.empty:\n",
    "    raise RuntimeError(\"No valid fits in influence_refit_summary.csv\")\n",
    "\n",
    "# Compute d_s = 2 * slope for each method\n",
    "def two(x): \n",
    "    return 2.0 * x\n",
    "\n",
    "inf_df['d_s_orig'] = two(inf_df['slope_orig'])\n",
    "inf_df['d_s_noinfl'] = two(inf_df['slope_noinfl'])\n",
    "inf_df['d_s_theilsen'] = two(inf_df['slope_theilsen'])\n",
    "inf_df['d_s_ransac'] = two(inf_df['slope_ransac'])\n",
    "\n",
    "# Produce aggregated summary (median, IQR)\n",
    "methods = ['d_s_orig','d_s_noinfl','d_s_theilsen','d_s_ransac']\n",
    "agg_rows = []\n",
    "for m in methods:\n",
    "    vals = inf_df[m].dropna().values\n",
    "    if vals.size == 0:\n",
    "        med = iqr_low = iqr_high = np.nan\n",
    "    else:\n",
    "        med = float(np.median(vals))\n",
    "        q1 = float(np.percentile(vals,25))\n",
    "        q3 = float(np.percentile(vals,75))\n",
    "        iqr_low, iqr_high = q1, q3\n",
    "    agg_rows.append({'method': m, 'n': int(np.sum(~inf_df[m].isna())), 'median': med, 'iqr_lower': iqr_low, 'iqr_upper': iqr_high})\n",
    "\n",
    "agg_df = pd.DataFrame(agg_rows)\n",
    "agg_df.to_csv(os.path.join(out_dir, 'd_s_methods_aggregate.csv'), index=False)\n",
    "\n",
    "# Paired comparisons (Wilcoxon) between methods across diagnostics (rows are paired by diag)\n",
    "paired_tests = []\n",
    "pairs = [('d_s_orig','d_s_noinfl'), ('d_s_orig','d_s_theilsen'), ('d_s_orig','d_s_ransac'),\n",
    "         ('d_s_noinfl','d_s_theilsen'), ('d_s_noinfl','d_s_ransac'), ('d_s_theilsen','d_s_ransac')]\n",
    "for a,b in pairs:\n",
    "    A = inf_df[a].values\n",
    "    B = inf_df[b].values\n",
    "    mask = (~np.isnan(A)) & (~np.isnan(B))\n",
    "    if mask.sum() >= 2:\n",
    "        try:\n",
    "            w = stats.wilcoxon(A[mask], B[mask], alternative='two-sided', zero_method='wilcox')\n",
    "            stat, pval = float(w.statistic), float(w.pvalue)\n",
    "        except Exception:\n",
    "            stat, pval = np.nan, np.nan\n",
    "        # also report median differences\n",
    "        med_diff = float(np.median(A[mask] - B[mask]))\n",
    "        paired_tests.append({'method_A': a, 'method_B': b, 'n_pairs': int(mask.sum()), 'wilcoxon_stat': stat, 'wilcoxon_p': pval, 'median_diff': med_diff})\n",
    "    else:\n",
    "        paired_tests.append({'method_A': a, 'method_B': b, 'n_pairs': int(mask.sum()), 'wilcoxon_stat': np.nan, 'wilcoxon_p': np.nan, 'median_diff': np.nan})\n",
    "\n",
    "pd.DataFrame(paired_tests).to_csv(os.path.join(out_dir, 'd_s_paired_tests.csv'), index=False)\n",
    "\n",
    "# Save the per-diag d_s table\n",
    "per_diag = inf_df[['diag','n_points_fit','n_influential','max_cook','slope_orig','slope_noinfl','slope_theilsen','slope_ransac',\n",
    "                   'd_s_orig','d_s_noinfl','d_s_theilsen','d_s_ransac']].copy()\n",
    "per_diag.to_csv(os.path.join(out_dir, 'd_s_per_diag.csv'), index=False)\n",
    "\n",
    "# Plot: grouped strip + box for d_s distributions (small K diagnostics)\n",
    "plt.figure(figsize=(7,4))\n",
    "data = [inf_df[m].dropna().values for m in methods]\n",
    "# boxplot\n",
    "bp = plt.boxplot(data, positions=np.arange(len(methods)), widths=0.6, patch_artist=True, showfliers=False)\n",
    "colors = ['#c6dbef','#9ecae1','#6baed6','#3182bd']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "# overlay points (jitter)\n",
    "for i, arr in enumerate(data):\n",
    "    x = np.random.normal(i, 0.08, size=arr.size)\n",
    "    plt.scatter(x, arr, color='k', s=12, alpha=0.8)\n",
    "plt.xticks(np.arange(len(methods)), ['orig','no_infl','Theil-Sen','RANSAC'])\n",
    "plt.ylabel('d_s = 2 * slope')\n",
    "plt.title('Comparison of d_s across methods (diagnostics)')\n",
    "plt.grid(alpha=0.25, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, 'd_s_methods_comparison.png'), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Small printed summary for quick inspection\n",
    "print(\"Saved aggregates:\", os.path.join(out_dir, 'd_s_methods_aggregate.csv'))\n",
    "print(\"Saved paired tests:\", os.path.join(out_dir, 'd_s_paired_tests.csv'))\n",
    "print(\"Saved per-diag table:\", os.path.join(out_dir, 'd_s_per_diag.csv'))\n",
    "print(\"Saved figure:\", os.path.join(out_dir, 'd_s_methods_comparison.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f736c",
   "metadata": {},
   "source": [
    "Résumé court des résultats que tu as fournis\n",
    "\n",
    "Les OLS originaux (avec tous points) donnent d_s médian ≈ 0.30 (slope ≈ 0.15) pour λ_max = 0.2.\n",
    "\n",
    "L’exclusion du point influent ou l’utilisation d’estimateurs robustes accroît fortement d_s :\n",
    "\n",
    "d_s without influential ≈ 1.12 (median)\n",
    "\n",
    "d_s Theil‑Sen ≈ 1.56 (median)\n",
    "\n",
    "d_s RANSAC ≈ 1.12 (median)\n",
    "\n",
    "Les tests appariés entre méthodes (K=10 diagnostics) montrent des différences statistiquement significatives (Wilcoxon p ≈ 0.002) entre orig vs no_infl / Theil‑Sen / RANSAC.\n",
    "\n",
    "En clair : la pente spectrale estimée sur λ ≤ 0.2 est très sensible à un unique point influent par fit — la décision méthodologique (inclure/exclure ou utiliser robust) change la conclusion numérique sur d_s.\n",
    "\n",
    "Interprétation pratique immédiate\n",
    "L’OLS « orig » produit des pentes faibles mais essentiellement dictées par 1 point influent par diagnostic.\n",
    "\n",
    "Les estimateurs robustes (Theil‑Sen, RANSAC) et la version OLS sans influent donnent des pentes ~3–5× plus élevées.\n",
    "\n",
    "Il n’y a pas d’unique « vérité » ici : il faut choisir et défendre une politique (robuste vs garder tous points) ou présenter les deux résultats avec justification."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5vs3Hjg7uRmHLLvE8miZh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
