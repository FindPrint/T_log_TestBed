{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8640b6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding nodes: 3256, embedding_dim=10\n",
      "Saved diagnostic 1: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 2: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 3: eigs=200, fit_points=16, fit_ok=True\n",
      "Saved diagnostic 4: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 5: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 6: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 7: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 8: eigs=200, fit_points=18, fit_ok=True\n",
      "Saved diagnostic 9: eigs=200, fit_points=16, fit_ok=True\n",
      "Saved diagnostic 10: eigs=200, fit_points=16, fit_ok=True\n",
      "Saved diagnostic 11: eigs=200, fit_points=18, fit_ok=True\n",
      "Saved diagnostic 12: eigs=200, fit_points=16, fit_ok=True\n",
      "Saved diagnostic 13: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 14: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 15: eigs=200, fit_points=16, fit_ok=True\n",
      "Saved diagnostic 16: eigs=200, fit_points=16, fit_ok=True\n",
      "Saved diagnostic 17: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 18: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostic 19: eigs=200, fit_points=18, fit_ok=True\n",
      "Saved diagnostic 20: eigs=200, fit_points=17, fit_ok=True\n",
      "Saved diagnostics to results/spectral_diagnostics\n"
     ]
    }
   ],
   "source": [
    "# Cell: Spectral diagnostic on selected bootstrap subsamples (log-log plots + fit details)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters (modifiable)\n",
    "csv_path = 'data/sunspots_raw/Sunspots.csv'\n",
    "value_col_candidates = ['Number', 'Total Sunspot', 'Total Sunspot Number', 'Monthly Mean']\n",
    "embedding_dim = 10\n",
    "tau = 1\n",
    "k_neighbors = 10\n",
    "n_eig = 200                 # eigenvalues to compute\n",
    "n_diagnostics = 20          # number of bootstrap subsamples to inspect\n",
    "subsample_frac = 0.6\n",
    "lambda_max_list = [0.1, 0.2, 0.4]  # lambda ranges to display on plots (we still fit per chosen lambda_max below)\n",
    "lambda_max_fit = 0.2        # primary lambda_max used for the numeric fit reported\n",
    "min_points_for_fit = 6\n",
    "rng_seed = 42\n",
    "\n",
    "out_dir = 'results/spectral_diagnostics'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Load series and build embedding\n",
    "df0 = pd.read_csv(csv_path)\n",
    "col = next((c for c in value_col_candidates if c in df0.columns), None)\n",
    "if col is None:\n",
    "    numeric_cols = df0.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        raise RuntimeError(\"No numeric column found in CSV.\")\n",
    "    col = numeric_cols[-1]\n",
    "series = pd.to_numeric(df0[col], errors='coerce').dropna().values\n",
    "\n",
    "def takens_embed(x, dim, tau):\n",
    "    m = len(x) - (dim - 1) * tau\n",
    "    if m <= 0:\n",
    "        return None\n",
    "    embed = np.empty((m, dim))\n",
    "    for i in range(dim):\n",
    "        embed[:, i] = x[i * tau : i * tau + m]\n",
    "    return embed\n",
    "\n",
    "X_full = takens_embed(series, embedding_dim, tau)\n",
    "if X_full is None:\n",
    "    raise RuntimeError(\"Embedding too short for embedding_dim/tau\")\n",
    "\n",
    "n_nodes = X_full.shape[0]\n",
    "print(f\"Embedding nodes: {n_nodes}, embedding_dim={embedding_dim}\")\n",
    "\n",
    "def build_laplacian_eigs(X_points, k_neighbors, n_eig):\n",
    "    n_nodes_local = X_points.shape[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k_neighbors + 1, n_nodes_local), algorithm='auto').fit(X_points)\n",
    "    distances, indices = nbrs.kneighbors(X_points)\n",
    "    adj = sparse.lil_matrix((n_nodes_local, n_nodes_local), dtype=np.float32)\n",
    "    for i in range(n_nodes_local):\n",
    "        for j in indices[i, 1:]:\n",
    "            adj[i, j] = 1.0\n",
    "            adj[j, i] = 1.0\n",
    "    adj = adj.tocsr()\n",
    "    deg = np.array(adj.sum(axis=1)).flatten()\n",
    "    deg[deg == 0] = 1.0\n",
    "    D_inv_sqrt = sparse.diags(1.0 / np.sqrt(deg))\n",
    "    I = sparse.identity(n_nodes_local, format='csr')\n",
    "    L_norm = I - D_inv_sqrt @ adj @ D_inv_sqrt\n",
    "    n_eig_local = min(n_eig, n_nodes_local - 1)\n",
    "    try:\n",
    "        eigvals, _ = eigsh(L_norm, k=n_eig_local, which='SM', tol=1e-6, maxiter=5000)\n",
    "    except Exception as e:\n",
    "        # fallback to dense for small matrices\n",
    "        try:\n",
    "            from scipy.linalg import eigh\n",
    "            Ld = L_norm.toarray()\n",
    "            eigvals_all = eigh(Ld, eigvals_only=True)\n",
    "            eigvals = np.sort(eigvals_all)[:n_eig_local]\n",
    "        except Exception as e2:\n",
    "            print(\"Eigen decomposition failed:\", e, e2)\n",
    "            return None\n",
    "    return np.sort(eigvals)\n",
    "\n",
    "def spectral_counting(eigvals):\n",
    "    eps = 1e-12\n",
    "    lams = eigvals[eigvals > eps]\n",
    "    lam_vals = np.unique(lams)\n",
    "    N_vals = np.array([np.searchsorted(lams, lam, side='right') for lam in lam_vals])\n",
    "    return lam_vals, N_vals\n",
    "\n",
    "def fit_loglog(lam_fit, N_fit):\n",
    "    log_lam = np.log(lam_fit)\n",
    "    log_N = np.log(N_fit)\n",
    "    slope, intercept, r_value, p_value, stderr = stats.linregress(log_lam, log_N)\n",
    "    return slope, intercept, r_value, p_value, stderr\n",
    "\n",
    "# Select bootstrap indices deterministically\n",
    "rng = np.random.default_rng(rng_seed)\n",
    "indices_list = [rng.choice(np.arange(n_nodes), size=max(100, int(np.floor(subsample_frac * n_nodes))), replace=False)\n",
    "                for _ in range(n_diagnostics)]\n",
    "\n",
    "summary_rows = []\n",
    "for i, idx in enumerate(indices_list, start=1):\n",
    "    X_sub = X_full[idx, :]\n",
    "    eigvals = build_laplacian_eigs(X_sub, k_neighbors, n_eig)\n",
    "    if eigvals is None:\n",
    "        print(f\"Sample {i}: eig computation failed; skipping.\")\n",
    "        continue\n",
    "    lam_vals, N_vals = spectral_counting(eigvals)\n",
    "    # Save raw eigenvalues for this diagnostic\n",
    "    pd.DataFrame({'eig_index': np.arange(1, len(eigvals)+1), 'eigval': eigvals}).to_csv(f\"{out_dir}/diag_{i:03d}_eigvals.csv\", index=False)\n",
    "    pd.DataFrame({'lambda': lam_vals, 'N_lambda': N_vals}).to_csv(f\"{out_dir}/diag_{i:03d}_counting.csv\", index=False)\n",
    "    # Fit on user lambda_max_fit and report number of points\n",
    "    mask = lam_vals <= lambda_max_fit\n",
    "    lam_fit = lam_vals[mask]\n",
    "    N_fit = N_vals[mask]\n",
    "    fit_ok = len(lam_fit) >= min_points_for_fit\n",
    "    slope = intercept = r_value = p_value = stderr = np.nan\n",
    "    if fit_ok:\n",
    "        slope, intercept, r_value, p_value, stderr = fit_loglog(lam_fit, N_fit)\n",
    "        d_s_est = 2.0 * slope\n",
    "    else:\n",
    "        d_s_est = np.nan\n",
    "    summary_rows.append({\n",
    "        'diag': i,\n",
    "        'n_nodes_sub': X_sub.shape[0],\n",
    "        'n_eig_computed': len(eigvals),\n",
    "        'n_lambda_total': len(lam_vals),\n",
    "        'n_points_fit': int(len(lam_fit)),\n",
    "        'lambda_max_fit': float(lambda_max_fit),\n",
    "        'fit_ok': bool(fit_ok),\n",
    "        'slope': float(slope) if not np.isnan(slope) else np.nan,\n",
    "        'stderr_slope': float(stderr) if not np.isnan(stderr) else np.nan,\n",
    "        'r_value': float(r_value) if not np.isnan(r_value) else np.nan,\n",
    "        'd_s_est': float(d_s_est) if not np.isnan(d_s_est) else np.nan\n",
    "    })\n",
    "    # Plot log-log with fits for multiple lambda_max_list overlays\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.loglog(lam_vals, N_vals, marker='o', markersize=4, linestyle='none', alpha=0.6, label='N(lambda)')\n",
    "    colors = ['red', 'orange', 'green']\n",
    "    for j, lm in enumerate(lambda_max_list):\n",
    "        maskj = lam_vals <= lm\n",
    "        if maskj.sum() >= 2:\n",
    "            lam_line = lam_vals[maskj]\n",
    "            N_line = N_vals[maskj]\n",
    "            # linear fit on that range for visualization\n",
    "            try:\n",
    "                s, itc, rv, pv, se = fit_loglog(lam_line, N_line)\n",
    "                label = f'fit <={lm} slope={s:.3f}'\n",
    "                lam_plot = np.linspace(lam_line.min(), lam_line.max(), 100)\n",
    "                N_plot = np.exp(itc) * lam_plot**(s)\n",
    "                plt.loglog(lam_plot, N_plot, color=colors[j], lw=1.5, label=label)\n",
    "            except Exception:\n",
    "                pass\n",
    "    plt.xlabel('lambda (eigenvalue)')\n",
    "    plt.ylabel('N(lambda)')\n",
    "    plt.title(f'Spectral diagnostic sample {i} (n_sub={X_sub.shape[0]})')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(alpha=0.25, which='both')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_dir}/diag_{i:03d}_loglog.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved diagnostic {i}: eigs={len(eigvals)}, fit_points={len(lam_fit)}, fit_ok={fit_ok}\")\n",
    "\n",
    "# Save summary CSV\n",
    "pd.DataFrame(summary_rows).to_csv(f\"{out_dir}/spectral_diagnostics_summary.csv\", index=False)\n",
    "print(\"Saved diagnostics to\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3ef9c",
   "metadata": {},
   "source": [
    "### Résumé des diagnostics spectral (ce que montrent tes fichiers)\n",
    "- Les diagnostics sur 20 sous‑échantillons montrent des fits stables sur λ ≤ 0.2 : nombre de points de fit ≈ 16–18 et slopes ≈ 0.14–0.15 → d_s spectrale ≈ 0.28–0.31 (cohérent avec ton résultat initial)【results/spectral_diagnostics_summary.csv】.  \n",
    "- Levina–Bickel MLE donne une dimension locale ≈ 7.94 (95% ≈ [7.84, 8.03]) — très différent de la d_s spectrale mais cohérent avec une ID locale élevée pour l’embedding Takens (dim embedding = 10).  \n",
    "- Diagnostic clair : le fit spectral small‑λ est appliqué sur une plage courte (≈ 16–18 points), avec pente stable ≈0.15. Cela indique que la pente est estimée de façon consistante pour λ ≤ 0.2, mais reste faible comparée aux estimateurs locaux (LB). La discordance signifie que ces méthodes mesurent des propriétés différentes (spectral scaling vs ID locale) ou que la plage de λ choisie impose une estimation de faible pente.\n",
    "\n",
    "---\n",
    "\n",
    "### Verdict opérationnel rapide\n",
    "- Signe de T_log (négatif) est robuste : même en explorant hyperparamètres et nulls, T_log reste < 0 pour configurations testées.  \n",
    "- La magnitude numérique de T_log dépend fortement de l'estimateur de dimension et des hyperparamètres (emb,k,lambda_max). Avant de rapporter une valeur absolue pour d_s/T_log il faut décider quelle définition on revendique et documenter sensibilité.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bac44",
   "metadata": {},
   "source": [
    "\n",
    "Cellule Python — Sensibilité de la pente spectrale en fonction de lambda_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac9ed0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding nodes=3256, emb_dim=10\n",
      "Processed diag 1: total lam unique=200\n",
      "Processed diag 2: total lam unique=200\n",
      "Processed diag 3: total lam unique=200\n",
      "Processed diag 4: total lam unique=200\n",
      "Processed diag 5: total lam unique=200\n",
      "Processed diag 6: total lam unique=200\n",
      "Processed diag 7: total lam unique=200\n",
      "Processed diag 8: total lam unique=200\n",
      "Processed diag 9: total lam unique=200\n",
      "Processed diag 10: total lam unique=200\n",
      "Processed diag 11: total lam unique=200\n",
      "Processed diag 12: total lam unique=200\n",
      "Processed diag 13: total lam unique=200\n",
      "Processed diag 14: total lam unique=200\n",
      "Processed diag 15: total lam unique=200\n",
      "Processed diag 16: total lam unique=200\n",
      "Processed diag 17: total lam unique=200\n",
      "Processed diag 18: total lam unique=200\n",
      "Processed diag 19: total lam unique=200\n",
      "Processed diag 20: total lam unique=200\n",
      "Saved sweep summary: results/spectral_lambda_sensitivity/lambda_max_sweep_summary.csv\n",
      "Saved aggregate: results/spectral_lambda_sensitivity/lambda_max_sweep_aggregate.csv\n",
      "Saved plot: results/spectral_lambda_sensitivity/lambda_max_sensitivity_plot.png\n"
     ]
    }
   ],
   "source": [
    "# Cell: Sweep lambda_max and report slope stability across selected diagnostic subsamples\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paramètres (modifiable)\n",
    "csv_path = 'data/sunspots_raw/Sunspots.csv'\n",
    "value_col_candidates = ['Number', 'Total Sunspot', 'Total Sunspot Number', 'Monthly Mean']\n",
    "embedding_dim = 10\n",
    "tau = 1\n",
    "k_neighbors = 10\n",
    "n_eig = 200\n",
    "subsample_frac = 0.6\n",
    "n_diagnostics = 20       # utiliser les mêmes indices diagnostiqués précédemment\n",
    "rng_seed = 42\n",
    "\n",
    "# grille de lambda_max à tester\n",
    "lambda_max_grid = [0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "min_points_for_fit = 4   # tolérance réduite pour explorer robustesse (reporter n_points)\n",
    "out_dir = 'results/spectral_lambda_sensitivity'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Chargement et embedding\n",
    "df0 = pd.read_csv(csv_path)\n",
    "col = next((c for c in value_col_candidates if c in df0.columns), None)\n",
    "if col is None:\n",
    "    numeric_cols = df0.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        raise RuntimeError(\"No numeric column found in CSV.\")\n",
    "    col = numeric_cols[-1]\n",
    "series = pd.to_numeric(df0[col], errors='coerce').dropna().values\n",
    "\n",
    "def takens_embed(x, dim, tau):\n",
    "    m = len(x) - (dim - 1) * tau\n",
    "    if m <= 0:\n",
    "        return None\n",
    "    embed = np.empty((m, dim))\n",
    "    for i in range(dim):\n",
    "        embed[:, i] = x[i * tau : i * tau + m]\n",
    "    return embed\n",
    "\n",
    "X_full = takens_embed(series, embedding_dim, tau)\n",
    "if X_full is None:\n",
    "    raise RuntimeError(\"Embedding too short for given embedding_dim/tau.\")\n",
    "n_nodes = X_full.shape[0]\n",
    "print(f\"Embedding nodes={n_nodes}, emb_dim={embedding_dim}\")\n",
    "\n",
    "def build_laplacian_eigs(X_points, k_neighbors, n_eig):\n",
    "    n_nodes_local = X_points.shape[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k_neighbors + 1, n_nodes_local), algorithm='auto').fit(X_points)\n",
    "    distances, indices = nbrs.kneighbors(X_points)\n",
    "    adj = sparse.lil_matrix((n_nodes_local, n_nodes_local), dtype=np.float32)\n",
    "    for i in range(n_nodes_local):\n",
    "        for j in indices[i, 1:]:\n",
    "            adj[i, j] = 1.0\n",
    "            adj[j, i] = 1.0\n",
    "    adj = adj.tocsr()\n",
    "    deg = np.array(adj.sum(axis=1)).flatten()\n",
    "    deg[deg == 0] = 1.0\n",
    "    D_inv_sqrt = sparse.diags(1.0 / np.sqrt(deg))\n",
    "    I = sparse.identity(n_nodes_local, format='csr')\n",
    "    L_norm = I - D_inv_sqrt @ adj @ D_inv_sqrt\n",
    "    n_eig_local = min(n_eig, n_nodes_local - 1)\n",
    "    try:\n",
    "        eigvals, _ = eigsh(L_norm, k=n_eig_local, which='SM', tol=1e-6, maxiter=5000)\n",
    "    except Exception:\n",
    "        try:\n",
    "            from scipy.linalg import eigh\n",
    "            Ld = L_norm.toarray()\n",
    "            eigvals_all = eigh(Ld, eigvals_only=True)\n",
    "            eigvals = np.sort(eigvals_all)[:n_eig_local]\n",
    "        except Exception as e:\n",
    "            print(\"Eigen decomposition failed:\", e)\n",
    "            return None\n",
    "    return np.sort(eigvals)\n",
    "\n",
    "def spectral_counting(eigvals):\n",
    "    eps = 1e-12\n",
    "    lams = eigvals[eigvals > eps]\n",
    "    lam_vals = np.unique(lams)\n",
    "    N_vals = np.array([np.searchsorted(lams, lam, side='right') for lam in lam_vals])\n",
    "    return lam_vals, N_vals\n",
    "\n",
    "def fit_loglog(lam_fit, N_fit):\n",
    "    log_lam = np.log(lam_fit)\n",
    "    log_N = np.log(N_fit)\n",
    "    slope, intercept, r_value, p_value, stderr = stats.linregress(log_lam, log_N)\n",
    "    return slope, intercept, r_value, p_value, stderr\n",
    "\n",
    "# deterministe: recréer mêmes indices de diagnostic (même seed que précédemment)\n",
    "rng = np.random.default_rng(rng_seed)\n",
    "indices_list = [rng.choice(np.arange(n_nodes), size=max(100, int(np.floor(subsample_frac * n_nodes))), replace=False)\n",
    "                for _ in range(n_diagnostics)]\n",
    "\n",
    "rows = []\n",
    "for i, idx in enumerate(indices_list, start=1):\n",
    "    X_sub = X_full[idx, :]\n",
    "    eigvals = build_laplacian_eigs(X_sub, k_neighbors, n_eig)\n",
    "    if eigvals is None:\n",
    "        print(f\"sample {i}: eig failed\")\n",
    "        continue\n",
    "    lam_vals, N_vals = spectral_counting(eigvals)\n",
    "    for lm in lambda_max_grid:\n",
    "        mask = lam_vals <= lm\n",
    "        lam_fit = lam_vals[mask]\n",
    "        N_fit = N_vals[mask]\n",
    "        n_points = int(len(lam_fit))\n",
    "        fit_ok = n_points >= min_points_for_fit\n",
    "        slope = intercept = r_value = p_value = stderr = np.nan\n",
    "        if fit_ok:\n",
    "            slope, intercept, r_value, p_value, stderr = fit_loglog(lam_fit, N_fit)\n",
    "        rows.append({\n",
    "            'diag': i,\n",
    "            'n_nodes_sub': X_sub.shape[0],\n",
    "            'lambda_max': lm,\n",
    "            'n_points_fit': n_points,\n",
    "            'fit_ok': bool(fit_ok),\n",
    "            'slope': float(slope) if not np.isnan(slope) else np.nan,\n",
    "            'stderr_slope': float(stderr) if not np.isnan(stderr) else np.nan,\n",
    "            'r_value': float(r_value) if not np.isnan(r_value) else np.nan,\n",
    "            'd_s_est': float(2.0*slope) if not np.isnan(slope) else np.nan\n",
    "        })\n",
    "    print(f\"Processed diag {i}: total lam unique={len(lam_vals)}\")\n",
    "\n",
    "df_out = pd.DataFrame(rows)\n",
    "df_out.to_csv(f\"{out_dir}/lambda_max_sweep_summary.csv\", index=False)\n",
    "\n",
    "# Aggregate: median and IQR of slope/d_s by lambda_max\n",
    "agg = df_out.groupby('lambda_max').agg(\n",
    "    n_samples=('diag', 'count'),\n",
    "    med_slope=('slope', 'median'),\n",
    "    med_d_s=('d_s_est', 'median'),\n",
    "    slope_iqr_lower=('slope', lambda x: np.quantile(x.dropna(), 0.25) if x.dropna().size>0 else np.nan),\n",
    "    slope_iqr_upper=('slope', lambda x: np.quantile(x.dropna(), 0.75) if x.dropna().size>0 else np.nan),\n",
    "    med_n_points=('n_points_fit','median')\n",
    ").reset_index()\n",
    "agg.to_csv(f\"{out_dir}/lambda_max_sweep_aggregate.csv\", index=False)\n",
    "\n",
    "# Plot median d_s vs lambda_max with IQR ribbon\n",
    "plt.figure(figsize=(6,3.5))\n",
    "xs = agg['lambda_max'].values\n",
    "ys = agg['med_d_s'].values\n",
    "ylo = agg['slope_iqr_lower'].values * 2\n",
    "yhi = agg['slope_iqr_upper'].values * 2\n",
    "plt.plot(xs, ys, '-o', color='darkblue', label='median d_s (2*slope)')\n",
    "plt.fill_between(xs, ylo, yhi, color='lightblue', alpha=0.4, label='IQR (2*slope)')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('lambda_max (log scale)')\n",
    "plt.ylabel('d_s estimate (median and IQR)')\n",
    "plt.title('Sensitivity of spectral slope to lambda_max')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/lambda_max_sensitivity_plot.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved sweep summary:\", f\"{out_dir}/lambda_max_sweep_summary.csv\")\n",
    "print(\"Saved aggregate:\", f\"{out_dir}/lambda_max_sweep_aggregate.csv\")\n",
    "print(\"Saved plot:\", f\"{out_dir}/lambda_max_sensitivity_plot.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374eb6ba",
   "metadata": {},
   "source": [
    "### Bref constat (5 lignes)\n",
    "- La pente spectrale dépend fortement de la plage de fit : lambda_max = 0.05 → médiane d_s ≈ 0.16; 0.1 → ≈ 0.21; 0.2 → ≈ 0.30; 0.4 → ≈ 0.51; 0.8 → ≈ 0.87 (médianes over diagnostics).  \n",
    "- Pour la plage small‑λ (≤ 0.2) la pente est stable sur les sous‑échantillons (n_points_fit ≈ 16–18, slope ≈ 0.14–0.16 → d_s ≈ 0.28–0.32).  \n",
    "- Levina–Bickel (MLE) donne une intrinsic dim locale ≈ 7.94 (CI 7.84–8.03) sur les mêmes embeddings.  \n",
    "- Interprétation clé : les deux estimateurs mesurent des propriétés différentes — le comptage spectral sur petites valeurs propres renvoie une très faible pente locale, tandis que les estimateurs locaux (LB) révèlent une haute dimension locale.  \n",
    "- Conséquence pratique : le signe de T_log (négatif) reste robuste, mais la valeur numérique de d_s/T_log dépend fortement de la définition (spectral vs local) et du choix de lambda_max.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff33a85",
   "metadata": {},
   "source": [
    "Cell Python — Comparaison pairée Levina–Bickel vs pente spectrale (lambda_max = 0.1, 0.2, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66eec6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Levina samples: n_boot=150, valid=150\n",
      "Embedding: emb_dim=10, n_nodes=3256\n",
      "Paired sample b=1: levina=7.857, eigvals=200, lam_unique=200\n",
      "Paired sample b=2: levina=7.934, eigvals=200, lam_unique=200\n",
      "Paired sample b=3: levina=7.910, eigvals=200, lam_unique=200\n",
      "Paired sample b=4: levina=7.854, eigvals=200, lam_unique=200\n",
      "Paired sample b=5: levina=7.886, eigvals=200, lam_unique=200\n",
      "Paired sample b=6: levina=7.898, eigvals=200, lam_unique=200\n",
      "Paired sample b=7: levina=7.937, eigvals=200, lam_unique=200\n",
      "Paired sample b=8: levina=7.912, eigvals=200, lam_unique=200\n",
      "Paired sample b=9: levina=8.002, eigvals=200, lam_unique=200\n",
      "Paired sample b=10: levina=7.906, eigvals=200, lam_unique=200\n",
      "Paired sample b=11: levina=7.906, eigvals=200, lam_unique=200\n",
      "Paired sample b=12: levina=7.877, eigvals=200, lam_unique=200\n",
      "Paired sample b=13: levina=7.958, eigvals=200, lam_unique=200\n",
      "Paired sample b=14: levina=7.941, eigvals=200, lam_unique=200\n",
      "Paired sample b=15: levina=7.957, eigvals=200, lam_unique=200\n",
      "Paired sample b=16: levina=7.980, eigvals=200, lam_unique=200\n",
      "Paired sample b=17: levina=7.986, eigvals=200, lam_unique=200\n",
      "Paired sample b=18: levina=7.995, eigvals=200, lam_unique=200\n",
      "Paired sample b=19: levina=7.923, eigvals=200, lam_unique=200\n",
      "Paired sample b=20: levina=7.956, eigvals=200, lam_unique=200\n",
      "Paired sample b=21: levina=7.930, eigvals=200, lam_unique=200\n",
      "Paired sample b=22: levina=7.955, eigvals=200, lam_unique=200\n",
      "Paired sample b=23: levina=7.870, eigvals=200, lam_unique=200\n",
      "Paired sample b=24: levina=7.977, eigvals=200, lam_unique=200\n",
      "Paired sample b=25: levina=8.013, eigvals=200, lam_unique=200\n",
      "Paired sample b=26: levina=8.008, eigvals=200, lam_unique=200\n",
      "Paired sample b=27: levina=7.912, eigvals=200, lam_unique=200\n",
      "Paired sample b=28: levina=7.978, eigvals=200, lam_unique=200\n",
      "Paired sample b=29: levina=7.862, eigvals=200, lam_unique=200\n",
      "Paired sample b=30: levina=8.064, eigvals=200, lam_unique=200\n",
      "Paired sample b=31: levina=7.871, eigvals=200, lam_unique=200\n",
      "Paired sample b=32: levina=7.827, eigvals=200, lam_unique=200\n",
      "Paired sample b=33: levina=7.928, eigvals=200, lam_unique=200\n",
      "Paired sample b=34: levina=7.902, eigvals=200, lam_unique=200\n",
      "Paired sample b=35: levina=7.862, eigvals=200, lam_unique=200\n",
      "Paired sample b=36: levina=7.956, eigvals=200, lam_unique=200\n",
      "Paired sample b=37: levina=7.953, eigvals=200, lam_unique=200\n",
      "Paired sample b=38: levina=7.977, eigvals=200, lam_unique=200\n",
      "Paired sample b=39: levina=7.968, eigvals=200, lam_unique=200\n",
      "Paired sample b=40: levina=7.874, eigvals=200, lam_unique=200\n",
      "Paired sample b=41: levina=7.988, eigvals=200, lam_unique=200\n",
      "Paired sample b=42: levina=7.938, eigvals=200, lam_unique=200\n",
      "Paired sample b=43: levina=7.993, eigvals=200, lam_unique=200\n",
      "Paired sample b=44: levina=7.880, eigvals=200, lam_unique=200\n",
      "Paired sample b=45: levina=7.957, eigvals=200, lam_unique=200\n",
      "Paired sample b=46: levina=7.921, eigvals=200, lam_unique=200\n",
      "Paired sample b=47: levina=7.939, eigvals=200, lam_unique=200\n",
      "Paired sample b=48: levina=7.954, eigvals=200, lam_unique=200\n",
      "Paired sample b=49: levina=7.998, eigvals=200, lam_unique=200\n",
      "Paired sample b=50: levina=7.918, eigvals=200, lam_unique=200\n",
      "Paired sample b=51: levina=7.878, eigvals=200, lam_unique=200\n",
      "Paired sample b=52: levina=7.891, eigvals=200, lam_unique=200\n",
      "Paired sample b=53: levina=7.949, eigvals=200, lam_unique=200\n",
      "Paired sample b=54: levina=7.961, eigvals=200, lam_unique=200\n",
      "Paired sample b=55: levina=7.955, eigvals=200, lam_unique=200\n",
      "Paired sample b=56: levina=7.923, eigvals=200, lam_unique=200\n",
      "Paired sample b=57: levina=7.936, eigvals=200, lam_unique=200\n",
      "Paired sample b=58: levina=7.842, eigvals=200, lam_unique=200\n",
      "Paired sample b=59: levina=7.935, eigvals=200, lam_unique=200\n",
      "Paired sample b=60: levina=7.948, eigvals=200, lam_unique=200\n",
      "Paired sample b=61: levina=7.939, eigvals=200, lam_unique=200\n",
      "Paired sample b=62: levina=7.789, eigvals=200, lam_unique=200\n",
      "Paired sample b=63: levina=8.022, eigvals=200, lam_unique=200\n",
      "Paired sample b=64: levina=8.039, eigvals=200, lam_unique=200\n",
      "Paired sample b=65: levina=7.940, eigvals=200, lam_unique=200\n",
      "Paired sample b=66: levina=7.876, eigvals=200, lam_unique=200\n",
      "Paired sample b=67: levina=7.915, eigvals=200, lam_unique=200\n",
      "Paired sample b=68: levina=7.981, eigvals=200, lam_unique=200\n",
      "Paired sample b=69: levina=7.900, eigvals=200, lam_unique=200\n",
      "Paired sample b=70: levina=7.855, eigvals=200, lam_unique=200\n",
      "Paired sample b=71: levina=8.025, eigvals=200, lam_unique=200\n",
      "Paired sample b=72: levina=7.987, eigvals=200, lam_unique=200\n",
      "Paired sample b=73: levina=7.908, eigvals=200, lam_unique=200\n",
      "Paired sample b=74: levina=7.887, eigvals=200, lam_unique=200\n",
      "Paired sample b=75: levina=7.961, eigvals=200, lam_unique=200\n",
      "Paired sample b=76: levina=7.996, eigvals=200, lam_unique=200\n",
      "Paired sample b=77: levina=7.923, eigvals=200, lam_unique=200\n",
      "Paired sample b=78: levina=7.888, eigvals=200, lam_unique=200\n",
      "Paired sample b=79: levina=7.870, eigvals=200, lam_unique=200\n",
      "Paired sample b=80: levina=8.010, eigvals=200, lam_unique=200\n",
      "Paired sample b=81: levina=7.958, eigvals=200, lam_unique=200\n",
      "Paired sample b=82: levina=7.966, eigvals=200, lam_unique=200\n",
      "Paired sample b=83: levina=7.936, eigvals=200, lam_unique=200\n",
      "Paired sample b=84: levina=7.875, eigvals=200, lam_unique=200\n",
      "Paired sample b=85: levina=7.921, eigvals=200, lam_unique=200\n",
      "Paired sample b=86: levina=7.988, eigvals=200, lam_unique=200\n",
      "Paired sample b=87: levina=7.936, eigvals=200, lam_unique=200\n",
      "Paired sample b=88: levina=7.875, eigvals=200, lam_unique=200\n",
      "Paired sample b=89: levina=7.941, eigvals=200, lam_unique=200\n",
      "Paired sample b=90: levina=7.893, eigvals=200, lam_unique=200\n",
      "Paired sample b=91: levina=7.807, eigvals=200, lam_unique=200\n",
      "Paired sample b=92: levina=8.015, eigvals=200, lam_unique=200\n",
      "Paired sample b=93: levina=7.951, eigvals=200, lam_unique=200\n",
      "Paired sample b=94: levina=7.922, eigvals=200, lam_unique=200\n",
      "Paired sample b=95: levina=7.944, eigvals=200, lam_unique=200\n",
      "Paired sample b=96: levina=7.988, eigvals=200, lam_unique=200\n",
      "Paired sample b=97: levina=7.939, eigvals=200, lam_unique=200\n",
      "Paired sample b=98: levina=7.946, eigvals=200, lam_unique=200\n",
      "Paired sample b=99: levina=7.907, eigvals=200, lam_unique=200\n",
      "Paired sample b=100: levina=7.982, eigvals=200, lam_unique=200\n",
      "Paired sample b=101: levina=7.898, eigvals=200, lam_unique=200\n",
      "Paired sample b=102: levina=7.924, eigvals=200, lam_unique=200\n",
      "Paired sample b=103: levina=7.952, eigvals=200, lam_unique=200\n",
      "Paired sample b=104: levina=7.985, eigvals=200, lam_unique=200\n",
      "Paired sample b=105: levina=7.931, eigvals=200, lam_unique=200\n",
      "Paired sample b=106: levina=7.968, eigvals=200, lam_unique=200\n",
      "Paired sample b=107: levina=7.906, eigvals=200, lam_unique=200\n",
      "Paired sample b=108: levina=7.943, eigvals=200, lam_unique=200\n",
      "Paired sample b=109: levina=8.004, eigvals=200, lam_unique=200\n",
      "Paired sample b=110: levina=7.981, eigvals=200, lam_unique=200\n",
      "Paired sample b=111: levina=7.995, eigvals=200, lam_unique=200\n",
      "Paired sample b=112: levina=7.899, eigvals=200, lam_unique=200\n",
      "Paired sample b=113: levina=7.956, eigvals=200, lam_unique=200\n",
      "Paired sample b=114: levina=7.940, eigvals=200, lam_unique=200\n",
      "Paired sample b=115: levina=8.016, eigvals=200, lam_unique=200\n",
      "Paired sample b=116: levina=7.972, eigvals=200, lam_unique=200\n",
      "Paired sample b=117: levina=7.884, eigvals=200, lam_unique=200\n",
      "Paired sample b=118: levina=8.025, eigvals=200, lam_unique=200\n",
      "Paired sample b=119: levina=7.909, eigvals=200, lam_unique=200\n",
      "Paired sample b=120: levina=7.897, eigvals=200, lam_unique=200\n",
      "Paired sample b=121: levina=7.900, eigvals=200, lam_unique=200\n",
      "Paired sample b=122: levina=7.969, eigvals=200, lam_unique=200\n",
      "Paired sample b=123: levina=7.981, eigvals=200, lam_unique=200\n",
      "Paired sample b=124: levina=7.992, eigvals=200, lam_unique=200\n",
      "Paired sample b=125: levina=7.997, eigvals=200, lam_unique=200\n",
      "Paired sample b=126: levina=7.929, eigvals=200, lam_unique=200\n",
      "Paired sample b=127: levina=7.909, eigvals=200, lam_unique=200\n",
      "Paired sample b=128: levina=7.951, eigvals=200, lam_unique=200\n",
      "Paired sample b=129: levina=8.006, eigvals=200, lam_unique=200\n",
      "Paired sample b=130: levina=7.902, eigvals=200, lam_unique=200\n",
      "Paired sample b=131: levina=7.821, eigvals=200, lam_unique=200\n",
      "Paired sample b=132: levina=7.940, eigvals=200, lam_unique=200\n",
      "Paired sample b=133: levina=7.969, eigvals=200, lam_unique=200\n",
      "Paired sample b=134: levina=7.935, eigvals=200, lam_unique=200\n",
      "Paired sample b=135: levina=7.880, eigvals=200, lam_unique=200\n",
      "Paired sample b=136: levina=7.899, eigvals=200, lam_unique=200\n",
      "Paired sample b=137: levina=7.982, eigvals=200, lam_unique=200\n",
      "Paired sample b=138: levina=8.017, eigvals=200, lam_unique=200\n",
      "Paired sample b=139: levina=7.991, eigvals=200, lam_unique=200\n",
      "Paired sample b=140: levina=8.033, eigvals=200, lam_unique=200\n",
      "Paired sample b=141: levina=7.959, eigvals=200, lam_unique=200\n",
      "Paired sample b=142: levina=8.085, eigvals=200, lam_unique=200\n",
      "Paired sample b=143: levina=7.933, eigvals=200, lam_unique=200\n",
      "Paired sample b=144: levina=7.926, eigvals=200, lam_unique=200\n",
      "Paired sample b=145: levina=7.894, eigvals=200, lam_unique=200\n",
      "Paired sample b=146: levina=7.991, eigvals=200, lam_unique=200\n",
      "Paired sample b=147: levina=8.050, eigvals=200, lam_unique=200\n",
      "Paired sample b=148: levina=8.018, eigvals=200, lam_unique=200\n",
      "Paired sample b=149: levina=7.931, eigvals=200, lam_unique=200\n",
      "Paired sample b=150: levina=8.020, eigvals=200, lam_unique=200\n",
      "Saved paired raw: results/paired_levina_spectral/paired_levina_spectral_raw.csv\n",
      "Saved paired summary: results/paired_levina_spectral/paired_levina_spectral_summary.csv\n",
      "Saved paired plots in results/paired_levina_spectral\n"
     ]
    }
   ],
   "source": [
    "# Cell: Paired comparison between Levina-Bickel MLE and spectral slope (same bootstrap subsamples)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# Paramètres (modifiables)\n",
    "csv_path = 'data/sunspots_raw/Sunspots.csv'\n",
    "levina_samples_path = 'results/levina_bickel_boot_samples.csv'   # produit précédemment\n",
    "out_dir = 'results/paired_levina_spectral'\n",
    "embedding_dim = 10\n",
    "tau = 1\n",
    "k_neighbors = 10\n",
    "n_eig = 200\n",
    "subsample_frac = 0.6\n",
    "rng_seed = 42\n",
    "lambda_max_list = [0.1, 0.2, 0.4]   # plages à tester (paired)\n",
    "min_points_for_fit = 4\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Load Levina samples (to pair by bootstrap index)\n",
    "if not os.path.exists(levina_samples_path):\n",
    "    raise RuntimeError(f\"Levina samples not found at {levina_samples_path}\")\n",
    "lev_df = pd.read_csv(levina_samples_path)\n",
    "# Expect columns: b, n_nodes_sub, levina_mle\n",
    "if 'b' not in lev_df.columns or 'levina_mle' not in lev_df.columns:\n",
    "    raise RuntimeError(\"Levina samples file missing required columns 'b' or 'levina_mle'\")\n",
    "\n",
    "n_boot = int(lev_df['b'].max())\n",
    "print(f\"Loaded Levina samples: n_boot={n_boot}, valid={len(lev_df)}\")\n",
    "\n",
    "# Load series and build embedding\n",
    "df0 = pd.read_csv(csv_path)\n",
    "value_col_candidates = ['Number', 'Total Sunspot', 'Total Sunspot Number', 'Monthly Mean']\n",
    "col = next((c for c in value_col_candidates if c in df0.columns), None)\n",
    "if col is None:\n",
    "    numeric_cols = df0.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        raise RuntimeError(\"No numeric column found in CSV.\")\n",
    "    col = numeric_cols[-1]\n",
    "series = pd.to_numeric(df0[col], errors='coerce').dropna().values\n",
    "\n",
    "def takens_embed(x, dim, tau):\n",
    "    m = len(x) - (dim - 1) * tau\n",
    "    if m <= 0:\n",
    "        return None\n",
    "    embed = np.empty((m, dim))\n",
    "    for i in range(dim):\n",
    "        embed[:, i] = x[i * tau : i * tau + m]\n",
    "    return embed\n",
    "\n",
    "X_full = takens_embed(series, embedding_dim, tau)\n",
    "if X_full is None:\n",
    "    raise RuntimeError(\"Embedding too short for given embedding_dim/tau.\")\n",
    "n_nodes = X_full.shape[0]\n",
    "print(f\"Embedding: emb_dim={embedding_dim}, n_nodes={n_nodes}\")\n",
    "\n",
    "def build_laplacian_eigs(X_points, k_neighbors, n_eig):\n",
    "    n_nodes_local = X_points.shape[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k_neighbors + 1, n_nodes_local), algorithm='auto').fit(X_points)\n",
    "    distances, indices = nbrs.kneighbors(X_points)\n",
    "    adj = sparse.lil_matrix((n_nodes_local, n_nodes_local), dtype=np.float32)\n",
    "    for i in range(n_nodes_local):\n",
    "        for j in indices[i, 1:]:\n",
    "            adj[i, j] = 1.0\n",
    "            adj[j, i] = 1.0\n",
    "    adj = adj.tocsr()\n",
    "    deg = np.array(adj.sum(axis=1)).flatten()\n",
    "    deg[deg == 0] = 1.0\n",
    "    D_inv_sqrt = sparse.diags(1.0 / np.sqrt(deg))\n",
    "    I = sparse.identity(n_nodes_local, format='csr')\n",
    "    L_norm = I - D_inv_sqrt @ adj @ D_inv_sqrt\n",
    "    n_eig_local = min(n_eig, n_nodes_local - 1)\n",
    "    try:\n",
    "        eigvals, _ = eigsh(L_norm, k=n_eig_local, which='SM', tol=1e-6, maxiter=5000)\n",
    "    except Exception:\n",
    "        try:\n",
    "            from scipy.linalg import eigh\n",
    "            Ld = L_norm.toarray()\n",
    "            eigvals_all = eigh(Ld, eigvals_only=True)\n",
    "            eigvals = np.sort(eigvals_all)[:n_eig_local]\n",
    "        except Exception as e:\n",
    "            print(\"Eigen decomposition failed:\", e)\n",
    "            return None\n",
    "    return np.sort(eigvals)\n",
    "\n",
    "def spectral_counting(eigvals):\n",
    "    eps = 1e-12\n",
    "    lams = eigvals[eigvals > eps]\n",
    "    lam_vals = np.unique(lams)\n",
    "    N_vals = np.array([np.searchsorted(lams, lam, side='right') for lam in lam_vals])\n",
    "    return lam_vals, N_vals\n",
    "\n",
    "def fit_loglog(lam_fit, N_fit):\n",
    "    log_lam = np.log(lam_fit)\n",
    "    log_N = np.log(N_fit)\n",
    "    slope, intercept, r_value, p_value, stderr = stats.linregress(log_lam, log_N)\n",
    "    return slope, intercept, r_value, p_value, stderr\n",
    "\n",
    "# Recreate deterministic bootstrap indices (must match Levina run)\n",
    "rng = default_rng(rng_seed)\n",
    "indices_list = [rng.choice(np.arange(n_nodes), size=max(100, int(np.floor(subsample_frac * n_nodes))), replace=False)\n",
    "                for _ in range(n_boot)]\n",
    "\n",
    "rows = []\n",
    "for row in lev_df.itertuples(index=False):\n",
    "    b_idx = int(row.b)\n",
    "    levina_val = float(row.levina_mle) if np.isfinite(row.levina_mle) else np.nan\n",
    "    # get indices for this bootstrap (1-based b in lev_df)\n",
    "    if b_idx < 1 or b_idx > len(indices_list):\n",
    "        print(f\"Skipping b={b_idx}: index out of range\")\n",
    "        continue\n",
    "    idx = indices_list[b_idx - 1]\n",
    "    X_sub = X_full[idx, :]\n",
    "    eigvals = build_laplacian_eigs(X_sub, k_neighbors, n_eig)\n",
    "    if eigvals is None:\n",
    "        for lm in lambda_max_list:\n",
    "            rows.append({'b': b_idx, 'levina_mle': levina_val, 'lambda_max': lm,\n",
    "                         'n_points_fit': 0, 'fit_ok': False, 'slope': np.nan, 'stderr_slope': np.nan, 'd_s': np.nan})\n",
    "        continue\n",
    "    lam_vals, N_vals = spectral_counting(eigvals)\n",
    "    for lm in lambda_max_list:\n",
    "        mask = lam_vals <= lm\n",
    "        lam_fit = lam_vals[mask]\n",
    "        N_fit = N_vals[mask]\n",
    "        n_points = int(len(lam_fit))\n",
    "        fit_ok = n_points >= min_points_for_fit\n",
    "        if fit_ok:\n",
    "            slope, intercept, r_value, p_value, stderr = fit_loglog(lam_fit, N_fit)\n",
    "            d_s = 2.0 * slope\n",
    "        else:\n",
    "            slope = intercept = r_value = p_value = stderr = np.nan\n",
    "            d_s = np.nan\n",
    "        rows.append({\n",
    "            'b': b_idx,\n",
    "            'levina_mle': levina_val,\n",
    "            'lambda_max': lm,\n",
    "            'n_points_fit': n_points,\n",
    "            'fit_ok': bool(fit_ok),\n",
    "            'slope': float(slope) if not np.isnan(slope) else np.nan,\n",
    "            'stderr_slope': float(stderr) if not np.isnan(stderr) else np.nan,\n",
    "            'd_s': float(d_s) if not np.isnan(d_s) else np.nan\n",
    "        })\n",
    "    print(f\"Paired sample b={b_idx}: levina={levina_val:.3f}, eigvals={len(eigvals)}, lam_unique={len(lam_vals)}\")\n",
    "\n",
    "paired_df = pd.DataFrame(rows)\n",
    "paired_df.to_csv(f\"{out_dir}/paired_levina_spectral_raw.csv\", index=False)\n",
    "\n",
    "# Aggregate paired comparisons and compute differences levina - spectral\n",
    "summary_rows = []\n",
    "for lm in lambda_max_list:\n",
    "    subset = paired_df[paired_df['lambda_max'] == lm]\n",
    "    # drop NaNs\n",
    "    valid = subset.dropna(subset=['levina_mle','d_s'])\n",
    "    n_valid = len(valid)\n",
    "    if n_valid == 0:\n",
    "        med_lev = med_spec = diff_med = np.nan\n",
    "    else:\n",
    "        med_lev = float(np.median(valid['levina_mle']))\n",
    "        med_spec = float(np.median(valid['d_s']))\n",
    "        diff_med = med_lev - med_spec\n",
    "    summary_rows.append({\n",
    "        'lambda_max': lm,\n",
    "        'n_pairs': int(len(subset)),\n",
    "        'n_valid_pairs': int(n_valid),\n",
    "        'median_levina': med_lev,\n",
    "        'median_d_s_spectral': med_spec,\n",
    "        'median_diff_levina_minus_spectral': diff_med\n",
    "    })\n",
    "\n",
    "pd.DataFrame(summary_rows).to_csv(f\"{out_dir}/paired_levina_spectral_summary.csv\", index=False)\n",
    "print(\"Saved paired raw:\", f\"{out_dir}/paired_levina_spectral_raw.csv\")\n",
    "print(\"Saved paired summary:\", f\"{out_dir}/paired_levina_spectral_summary.csv\")\n",
    "\n",
    "# Plots: scatter paired (levina vs d_s) for each lambda_max\n",
    "for lm in lambda_max_list:\n",
    "    subset = paired_df[paired_df['lambda_max'] == lm].dropna(subset=['levina_mle','d_s'])\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.scatter(subset['levina_mle'], subset['d_s'], alpha=0.7, s=20)\n",
    "    # identity line\n",
    "    mn = min(subset['levina_mle'].min(), subset['d_s'].min()) if not subset.empty else 0\n",
    "    mx = max(subset['levina_mle'].max(), subset['d_s'].max()) if not subset.empty else 1\n",
    "    plt.plot([mn,mx],[mn,mx], color='gray', linestyle='--', linewidth=1)\n",
    "    plt.xlabel('Levina-Bickel MLE (m_hat)')\n",
    "    plt.ylabel('Spectral d_s (2*slope)')\n",
    "    plt.title(f'Paired: Levina vs spectral (lambda_max={lm})')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_dir}/paired_scatter_lambda_{lm}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Paired difference boxplots (levina - spectral) across lambda_max\n",
    "plt.figure(figsize=(6,3.5))\n",
    "data = []\n",
    "labels = []\n",
    "for lm in lambda_max_list:\n",
    "    subset = paired_df[paired_df['lambda_max'] == lm].dropna(subset=['levina_mle','d_s'])\n",
    "    diff = subset['levina_mle'] - subset['d_s']\n",
    "    data.append(diff.values)\n",
    "    labels.append(str(lm))\n",
    "plt.boxplot(data, labels=labels, showfliers=False)\n",
    "plt.xlabel('lambda_max')\n",
    "plt.ylabel('Levina - spectral d_s')\n",
    "plt.title('Paired differences across lambda_max')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/paired_diff_boxplot.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved paired plots in\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e905723",
   "metadata": {},
   "source": [
    "Constat principal\n",
    "\n",
    "Les estimations sont fortement discordantes : Levina‑Bickel MLE médiane ≈ 7.94, tandis que les pentes spectrales donnent d_s médian ≈ 0.21 (λ=0.1), 0.30 (λ=0.2), 0.52 (λ=0.4). La médiane des différences (Levina − spectral) est ≈ 7.73, 7.64, 7.42 respectivement.\n",
    "\n",
    "Les scatter plots montrent un regroupement des points vers la droite-bas : Levina élevé, spectral très bas — l’accord est absent à l’échelle absolue.\n",
    "\n",
    "Interprétation technique rapide\n",
    "Les deux estimateurs mesurent des propriétés différentes : Levina‑Bickel estime une dimension intrinsèque locale fondée sur distances de voisinage ; la pente spectrale (comptage des petites valeurs propres) capte l’échelle spectrale choisie par λ_max.\n",
    "\n",
    "La valeur numérique de d_s issue de la pente spectrale varie fortement avec λ_max (signe d’une pente qui s’accroît quand on élargit la plage), donc la sensibilité en λ_max explique la majeure partie de l’écart.\n",
    "\n",
    "Sur petites valeurs propres (λ ≤ 0.2) la pente spectrale reste très faible → d_s ≪ m_hat, ce qui est cohérent avec les visualisations log‑log si la plage small‑λ est presque plate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e7666",
   "metadata": {},
   "source": [
    "Cellule Python — Tests appariés (Wilcoxon, t), Spearman, tailles d’effet et figures annotées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e84d482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tests summary: results/paired_levina_spectral\\paired_levina_spectral_tests_summary.csv\n",
      "Saved annotated scatter plots and annotated boxplot in results/paired_levina_spectral\n"
     ]
    }
   ],
   "source": [
    "# Cell: Paired tests (Wilcoxon, paired t), Spearman correlation, Cohen's d for paired differences,\n",
    "# and annotated scatter + boxplot figures.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "in_dir = 'results/paired_levina_spectral'\n",
    "out_dir = in_dir\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "paired_raw = os.path.join(in_dir, 'paired_levina_spectral_raw.csv')\n",
    "if not os.path.exists(paired_raw):\n",
    "    raise RuntimeError(f\"Paired file not found: {paired_raw}\")\n",
    "\n",
    "df = pd.read_csv(paired_raw)\n",
    "\n",
    "lambda_max_list = sorted(df['lambda_max'].unique())\n",
    "\n",
    "def cohen_d_paired(x, y):\n",
    "    # Cohen's d for paired samples: mean(diff)/sd(diff)\n",
    "    d = x - y\n",
    "    d = d[~np.isnan(d)]\n",
    "    if d.size < 2:\n",
    "        return np.nan\n",
    "    return float(np.mean(d) / (np.std(d, ddof=1)))\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for lm in lambda_max_list:\n",
    "    subset = df[df['lambda_max'] == lm].dropna(subset=['levina_mle', 'd_s'])\n",
    "    n_pairs = len(subset)\n",
    "    if n_pairs == 0:\n",
    "        summary_rows.append({\n",
    "            'lambda_max': lm, 'n_pairs': 0,\n",
    "            'median_levina': np.nan, 'median_spectral': np.nan,\n",
    "            'wilcoxon_stat': np.nan, 'wilcoxon_p': np.nan,\n",
    "            'paired_t_stat': np.nan, 'paired_t_p': np.nan,\n",
    "            'spearman_rho': np.nan, 'spearman_p': np.nan,\n",
    "            'cohens_d_paired': np.nan\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    lev = subset['levina_mle'].values\n",
    "    spec = subset['d_s'].values\n",
    "    diff = lev - spec\n",
    "\n",
    "    # Descriptives\n",
    "    med_lev = float(np.median(lev))\n",
    "    med_spec = float(np.median(spec))\n",
    "    mean_diff = float(np.mean(diff))\n",
    "    std_diff = float(np.std(diff, ddof=1))\n",
    "\n",
    "    # Wilcoxon signed-rank test (two-sided)\n",
    "    try:\n",
    "        wil_res = stats.wilcoxon(lev, spec, alternative='two-sided', zero_method='wilcox')\n",
    "        wil_stat, wil_p = float(wil_res.statistic), float(wil_res.pvalue)\n",
    "    except Exception:\n",
    "        wil_stat, wil_p = np.nan, np.nan\n",
    "\n",
    "    # Paired t-test\n",
    "    try:\n",
    "        t_res = stats.ttest_rel(lev, spec, nan_policy='omit')\n",
    "        t_stat, t_p = float(t_res.statistic), float(t_res.pvalue)\n",
    "    except Exception:\n",
    "        t_stat, t_p = np.nan, np.nan\n",
    "\n",
    "    # Spearman correlation\n",
    "    try:\n",
    "        rho, rho_p = stats.spearmanr(lev, spec, nan_policy='omit')\n",
    "        rho, rho_p = float(rho), float(rho_p)\n",
    "    except Exception:\n",
    "        rho, rho_p = np.nan, np.nan\n",
    "\n",
    "    # Cohen's d (paired)\n",
    "    d_cohen = cohen_d_paired(lev, spec)\n",
    "\n",
    "    summary_rows.append({\n",
    "        'lambda_max': lm,\n",
    "        'n_pairs': int(n_pairs),\n",
    "        'median_levina': med_lev,\n",
    "        'median_spectral': med_spec,\n",
    "        'mean_diff': mean_diff,\n",
    "        'std_diff': std_diff,\n",
    "        'wilcoxon_stat': wil_stat,\n",
    "        'wilcoxon_p': wil_p,\n",
    "        'paired_t_stat': t_stat,\n",
    "        'paired_t_p': t_p,\n",
    "        'spearman_rho': rho,\n",
    "        'spearman_p': rho_p,\n",
    "        'cohens_d_paired': d_cohen\n",
    "    })\n",
    "\n",
    "    # Annotated scatter\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.scatter(lev, spec, alpha=0.7, s=18)\n",
    "    mn = min(np.min(lev), np.min(spec))\n",
    "    mx = max(np.max(lev), np.max(spec))\n",
    "    plt.plot([mn, mx], [mn, mx], color='gray', linestyle='--', linewidth=1)\n",
    "    plt.xlabel('Levina-Bickel MLE (m_hat)')\n",
    "    plt.ylabel('Spectral d_s (2*slope)')\n",
    "    plt.title(f'Levina vs spectral (lambda_max={lm})')\n",
    "    annotation = (\n",
    "        f\"n={n_pairs}\\nmedian_lev={med_lev:.3f}\\nmedian_spec={med_spec:.3f}\\n\"\n",
    "        f\"mean_diff={mean_diff:.3f} ± {std_diff:.3f}\\n\"\n",
    "        f\"Wilcoxon p={wil_p:.2e}\\npaired t p={t_p:.2e}\\nSpearman rho={rho:.2f} (p={rho_p:.2e})\\n\"\n",
    "        f\"Cohen d_paired={d_cohen:.2f}\"\n",
    "    )\n",
    "    plt.gca().text(0.02, 0.98, annotation, transform=plt.gca().transAxes,\n",
    "                   fontsize=8, va='top', ha='left',\n",
    "                   bbox=dict(facecolor='white', alpha=0.85, edgecolor='none'))\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f'paired_scatter_lambda_{lm}_annotated_tests.png'), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Save summary CSV\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(os.path.join(out_dir, 'paired_levina_spectral_tests_summary.csv'), index=False)\n",
    "\n",
    "# Boxplot of paired differences with Wilcoxon p-values annotated\n",
    "plt.figure(figsize=(6,3.5))\n",
    "data = []\n",
    "labels = []\n",
    "p_texts = []\n",
    "for lm in lambda_max_list:\n",
    "    subset = df[df['lambda_max'] == lm].dropna(subset=['levina_mle', 'd_s'])\n",
    "    diff = (subset['levina_mle'] - subset['d_s']).values\n",
    "    data.append(diff)\n",
    "    labels.append(str(lm))\n",
    "    row = summary_df[summary_df['lambda_max'] == lm]\n",
    "    pval = float(row['wilcoxon_p'].values[0]) if not row.empty else np.nan\n",
    "    p_texts.append(f\"p={pval:.1e}\" if np.isfinite(pval) else \"p=NA\")\n",
    "\n",
    "plt.boxplot(data, labels=labels, showfliers=False)\n",
    "ymax = plt.ylim()[1]\n",
    "for xi, txt in enumerate(p_texts, start=1):\n",
    "    plt.text(xi, ymax*0.98, txt, ha='center', va='top', fontsize=8)\n",
    "plt.xlabel('lambda_max')\n",
    "plt.ylabel('Levina - spectral d_s')\n",
    "plt.title('Paired differences (Levina - spectral) with Wilcoxon p-values')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, 'paired_diff_boxplot_annotated_tests.png'), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved tests summary:\", os.path.join(out_dir, 'paired_levina_spectral_tests_summary.csv'))\n",
    "print(\"Saved annotated scatter plots and annotated boxplot in\", out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5vs3Hjg7uRmHLLvE8miZh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
