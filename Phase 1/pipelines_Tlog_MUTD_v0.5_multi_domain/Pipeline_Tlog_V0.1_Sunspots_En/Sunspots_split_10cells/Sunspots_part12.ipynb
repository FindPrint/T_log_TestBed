{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7798aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading robervalt/sunspots to data\\sunspots_kaggle\n",
      "Dataset URL: https://www.kaggle.com/datasets/robervalt/sunspots\n",
      "[INFO] Downloaded: data\\sunspots_kaggle\\sunspots.zip\n",
      "[INFO] Extracted into: data\\sunspots_kaggle\\extracted\n",
      "[INFO] Using CSV: data\\sunspots_kaggle\\extracted\\Sunspots.csv\n",
      "[INFO] Loaded series column 'Monthly Mean Total Sunspot Number' with n=3265\n",
      "[INFO] Embedding shape: (3256, 10)\n",
      "[INFO] k-NN adjacency shape: (3256, 3256), nnz=48972\n",
      "[INFO] Constructed normalized Laplacian\n",
      "[INFO] Computed 200 eigenvalues (smallest)\n",
      "[INFO] Wrote eigenvalues to results\\sunspots_external\\laplacian_eigenvalues.csv\n",
      "[INFO] Point estimate d_s = 1.999479 using 19 points (lambda<= 0.2)\n",
      "[INFO] Bootstrap complete, retained samples: 200 out of 200\n",
      "[INFO] Summary and plots written to results\\sunspots_external\n",
      "\n",
      "RESULTS SUMMARY:\n",
      "- n_system: 3265\n",
      "- d_s_point: 1.9994790122804786\n",
      "- d_s_boot_median: 2.050927876374394\n",
      "- d_s_boot_q025: 1.0731509417247878\n",
      "- d_s_boot_q975: 4.422765618088611\n",
      "- Tlog_median: -15.769971869633462\n",
      "- Tlog_q025: -23.681179755321043\n",
      "- Tlog_q975: 3.4206029750729985\n",
      "- n_boot_retained: 200\n",
      "\n",
      "Null-model (temporal shuffle) quick check: d_s_null = [15.012696738525248]\n"
     ]
    }
   ],
   "source": [
    "# Cellule autonome : Télécharger Sunspots (Kaggle) -> embedding Takens -> k-NN -> Laplacian -> eigenvalues -> d_s (Theil-Sen) -> bootstrap -> T_log\n",
    "# CONFIGURATION (éditer si besoin)\n",
    "use_kaggle = True                        # True pour télécharger depuis Kaggle (requiert ~/.kaggle/kaggle.json ou ./kaggle.json)\n",
    "kaggle_dataset = \"robervalt/sunspots\"    # dataset Kaggle à télécharger\n",
    "download_path = Path(\"data/sunspots_kaggle\")\n",
    "extract_dir = download_path / \"extracted\"\n",
    "results_dir = Path(\"results/sunspots_external\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Embedding / graph params\n",
    "embedding_dim = 10\n",
    "tau = 1\n",
    "k_neighbors = 10\n",
    "n_eig = 200                               # nombre de valeurs propres à extraire\n",
    "lambda_max = 0.2\n",
    "n_boot = 200\n",
    "subsample_frac = 0.8\n",
    "random_seed = 42\n",
    "\n",
    "# ---------- imports ----------\n",
    "from pathlib import Path\n",
    "import os, json, zipfile, math\n",
    "import numpy as np, pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "np.random.seed(random_seed); random.seed(random_seed)\n",
    "\n",
    "def log_event(level, msg):\n",
    "    print(f\"[{level.upper()}] {msg}\")\n",
    "\n",
    "# ---------- Kaggle download helper ----------\n",
    "def find_kaggle_config():\n",
    "    locations = [\n",
    "        os.path.join(os.path.expanduser('~'), '.kaggle', 'kaggle.json'),\n",
    "        os.path.join(os.getcwd(), 'kaggle.json')\n",
    "    ]\n",
    "    for loc in locations:\n",
    "        if os.path.exists(loc):\n",
    "            try:\n",
    "                with open(loc, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def kaggle_download(dataset, dst):\n",
    "    try:\n",
    "        import kaggle\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"kaggle package required: pip install kaggle\") from e\n",
    "    cfg = find_kaggle_config()\n",
    "    if not cfg:\n",
    "        raise FileNotFoundError(\"kaggle.json not trouvé; place-le dans ~/.kaggle/ ou le répertoire courant.\")\n",
    "    os.environ['KAGGLE_USERNAME'] = cfg.get('username')\n",
    "    os.environ['KAGGLE_KEY'] = cfg.get('key')\n",
    "    kaggle.api.authenticate()\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "    kaggle.api.dataset_download_files(dataset, path=str(dst), unzip=False)\n",
    "    zips = [p for p in dst.iterdir() if p.suffix=='.zip']\n",
    "    if not zips:\n",
    "        raise FileNotFoundError(\"Aucun ZIP téléchargé dans \" + str(dst))\n",
    "    return zips[0]\n",
    "\n",
    "# ---------- Step 1: download and extract ----------\n",
    "if use_kaggle:\n",
    "    log_event(\"info\", f\"Downloading {kaggle_dataset} to {download_path}\")\n",
    "    zip_path = kaggle_download(kaggle_dataset, download_path)\n",
    "    log_event(\"info\", f\"Downloaded: {zip_path}\")\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(extract_dir)\n",
    "    log_event(\"info\", f\"Extracted into: {extract_dir}\")\n",
    "    # find likely Sunspots CSV\n",
    "    candidates = list(extract_dir.rglob(\"*.csv\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"Aucun CSV trouvé dans l'archive extraite.\")\n",
    "    # prefer file with 'sunspot' or 'Sunspots' in name\n",
    "    chosen = None\n",
    "    for p in candidates:\n",
    "        if 'sun' in p.name.lower():\n",
    "            chosen = p; break\n",
    "    if chosen is None:\n",
    "        chosen = candidates[0]\n",
    "else:\n",
    "    # look for local copy\n",
    "    local_candidates = list(Path(\"data\").rglob(\"*.csv\")) + list(Path(\"external\").rglob(\"*.csv\"))\n",
    "    if not local_candidates:\n",
    "        raise FileNotFoundError(\"Aucun CSV local trouvé. Active use_kaggle or place CSV in data/ or external/.\")\n",
    "    chosen = local_candidates[0]\n",
    "\n",
    "log_event(\"info\", f\"Using CSV: {chosen}\")\n",
    "\n",
    "# ---------- Step 2: load sunspots series ----------\n",
    "df = pd.read_csv(chosen)\n",
    "# Attempt to detect monthly mean total sunspot number column\n",
    "cols = df.columns.tolist()\n",
    "col_candidates = [c for c in cols if 'sun' in c.lower()]\n",
    "if col_candidates:\n",
    "    series_col = col_candidates[0]\n",
    "else:\n",
    "    # fallback: numeric first column\n",
    "    for c in cols:\n",
    "        try:\n",
    "            arr = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if arr.notna().sum() > 0:\n",
    "                series_col = c\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "series = pd.to_numeric(df[series_col], errors=\"coerce\").dropna().values\n",
    "n_system = len(series)\n",
    "log_event(\"info\", f\"Loaded series column '{series_col}' with n={n_system}\")\n",
    "\n",
    "# ---------- Step 3: Takens embedding ----------\n",
    "def takens_embed(x, emb_dim, tau):\n",
    "    N = len(x)\n",
    "    m = emb_dim\n",
    "    T = tau\n",
    "    L = N - (m-1)*T\n",
    "    if L <= 0:\n",
    "        raise ValueError(\"Time series too short for embedding with given dim/tau\")\n",
    "    Y = np.empty((L, m))\n",
    "    for i in range(m):\n",
    "        Y[:, i] = x[i*T : i*T + L]\n",
    "    return Y\n",
    "\n",
    "X = takens_embed(series, embedding_dim, tau)\n",
    "log_event(\"info\", f\"Embedding shape: {X.shape}\")\n",
    "\n",
    "# ---------- Step 4: construct k-NN graph (symmetric unweighted) ----------\n",
    "# compute kneighbors_graph (sparse)\n",
    "A = kneighbors_graph(X, n_neighbors=k_neighbors, mode='connectivity', include_self=False, n_jobs=1)\n",
    "# symmetrize\n",
    "A = 0.5 * (A + A.T)\n",
    "A = (A > 0).astype(int)\n",
    "A = sp.csr_matrix(A)\n",
    "log_event(\"info\", f\"k-NN adjacency shape: {A.shape}, nnz={A.nnz}\")\n",
    "\n",
    "# ---------- Step 5: normalized Laplacian and eigenvalues ----------\n",
    "# degree\n",
    "deg = np.array(A.sum(axis=1)).flatten()\n",
    "# avoid zero-degree nodes by tiny smoothing\n",
    "deg[deg==0] = 1.0\n",
    "D_inv_sqrt = sp.diags(1.0/np.sqrt(deg))\n",
    "L = sp.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt  # normalized Laplacian\n",
    "log_event(\"info\", \"Constructed normalized Laplacian\")\n",
    "\n",
    "# compute smallest n_eig eigenvalues (use scipy.sparse.linalg eigsh)\n",
    "from scipy.sparse.linalg import eigsh\n",
    "k_eig = min(n_eig, L.shape[0]-2) if L.shape[0] > 3 else min(1, L.shape[0]-1)\n",
    "if k_eig < 1:\n",
    "    raise ValueError(\"Graph too small for eigen decomposition with requested n_eig\")\n",
    "vals, vecs = eigsh(L.asfptype(), k=k_eig, which='SM', tol=1e-6, maxiter=5000)\n",
    "vals = np.sort(np.real(vals))\n",
    "log_event(\"info\", f\"Computed {len(vals)} eigenvalues (smallest)\")\n",
    "\n",
    "# save eigenvalues\n",
    "eig_fp = results_dir / \"laplacian_eigenvalues.csv\"\n",
    "pd.DataFrame({\"lambda\": vals}).to_csv(eig_fp, index=False)\n",
    "log_event(\"info\", f\"Wrote eigenvalues to {eig_fp}\")\n",
    "\n",
    "# ---------- Step 6: estimate d_s using Theil-Sen on N(lambda) <= lambda_max ----------\n",
    "lam = vals[vals>0]\n",
    "Nlam = np.arange(1, len(lam)+1)\n",
    "\n",
    "mask = lam <= lambda_max\n",
    "if mask.sum() < 3:\n",
    "    log_event(\"warn\", f\"Only {mask.sum()} eigenvalues with lambda <= {lambda_max}; Theil-Sen fit requires >=3 points\")\n",
    "else:\n",
    "    x = np.log(lam[mask]).reshape(-1,1)\n",
    "    y = np.log(Nlam[mask])\n",
    "    reg = TheilSenRegressor(random_state=random_seed).fit(x,y)\n",
    "    slope = float(reg.coef_[0]); intercept = float(reg.intercept_)\n",
    "    d_s_point = 2.0 * slope\n",
    "    log_event(\"info\", f\"Point estimate d_s = {d_s_point:.6f} using {mask.sum()} points (lambda<= {lambda_max})\")\n",
    "\n",
    "# ---------- Step 7: bootstrap d_s by subsampling eigenvalues with replacement ----------\n",
    "def estimate_ds_from_eigs(eigs_arr, lambda_max_local=lambda_max):\n",
    "    lam_loc = eigs_arr[eigs_arr>0]\n",
    "    Nloc = np.arange(1, len(lam_loc)+1)\n",
    "    sel = lam_loc <= lambda_max_local\n",
    "    if sel.sum() < 3:\n",
    "        return None\n",
    "    xloc = np.log(lam_loc[sel]).reshape(-1,1)\n",
    "    yloc = np.log(Nloc[sel])\n",
    "    regloc = TheilSenRegressor(random_state=None).fit(xloc, yloc)\n",
    "    return 2.0 * float(regloc.coef_[0])\n",
    "\n",
    "rng = np.random.RandomState(random_seed)\n",
    "ds_samples = []\n",
    "n_e = len(lam)\n",
    "for i in range(n_boot):\n",
    "    idx = rng.choice(n_e, size=max(3, int(math.floor(n_e*subsample_frac))), replace=True)\n",
    "    samp = np.sort(lam[idx])\n",
    "    est_ds = estimate_ds_from_eigs(samp)\n",
    "    if est_ds is not None:\n",
    "        ds_samples.append(est_ds)\n",
    "ds_samples = np.array(ds_samples)\n",
    "log_event(\"info\", f\"Bootstrap complete, retained samples: {len(ds_samples)} out of {n_boot}\")\n",
    "\n",
    "# ---------- Step 8: propagate to T_log and save results ----------\n",
    "if len(ds_samples)>0:\n",
    "    ds_med = float(np.median(ds_samples)); ds_q025, ds_q975 = np.quantile(ds_samples, [0.025, 0.975])\n",
    "    Tlog_samples = (ds_samples - 4.0) * math.log(n_system)\n",
    "    Tlog_med = float(np.median(Tlog_samples)); Tlog_q025, Tlog_q975 = np.quantile(Tlog_samples, [0.025, 0.975])\n",
    "    # save summaries\n",
    "    out_summary = {\n",
    "        \"n_system\": int(n_system),\n",
    "        \"d_s_point\": float(d_s_point) if 'd_s_point' in locals() else None,\n",
    "        \"d_s_boot_median\": ds_med,\n",
    "        \"d_s_boot_q025\": float(ds_q025),\n",
    "        \"d_s_boot_q975\": float(ds_q975),\n",
    "        \"Tlog_median\": Tlog_med,\n",
    "        \"Tlog_q025\": float(Tlog_q025),\n",
    "        \"Tlog_q975\": float(Tlog_q975),\n",
    "        \"n_boot_retained\": int(len(ds_samples))\n",
    "    }\n",
    "    pd.DataFrame([out_summary]).to_csv(results_dir / \"external_Tlog_summary.csv\", index=False)\n",
    "    pd.DataFrame({\"d_s\": ds_samples}).to_csv(results_dir / \"external_ds_boot.csv\", index=False)\n",
    "    pd.DataFrame({\"T_log\": Tlog_samples}).to_csv(results_dir / \"external_Tlog_boot.csv\", index=False)\n",
    "    # histogram plot\n",
    "    plt.figure(figsize=(6,3.5))\n",
    "    plt.hist(Tlog_samples, bins=30, color='C0', alpha=0.85)\n",
    "    plt.axvline(Tlog_med, color='k', linestyle='--', label=f\"median {Tlog_med:.2f}\")\n",
    "    plt.title(\"Bootstrap T_log distribution (propagated from d_s)\")\n",
    "    plt.xlabel(\"T_log\"); plt.ylabel(\"count\"); plt.legend()\n",
    "    hist_fp = results_dir / \"external_Tlog_hist.png\"\n",
    "    plt.tight_layout(); plt.savefig(hist_fp, dpi=150); plt.close()\n",
    "    log_event(\"info\", f\"Summary and plots written to {results_dir}\")\n",
    "    print(\"\\nRESULTS SUMMARY:\")\n",
    "    for k,v in out_summary.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "else:\n",
    "    log_event(\"warn\", \"No bootstrap d_s samples retained. Check lambda_max or eigen-spectrum quality.\")\n",
    "\n",
    "# ---------- Optional: quick null-model (temporal shuffle) basic check ----------\n",
    "# Create a temporal shuffle null by shuffling the original series, rebuild embedding/Laplacian on the shuffled series,\n",
    "# compute eigenvalues and a single d_s estimate to compare directionally (not bootstrapped to limit runtime).\n",
    "try:\n",
    "    null_runs = 1\n",
    "    null_ds = []\n",
    "    for i in range(null_runs):\n",
    "        s_shuf = np.copy(series)\n",
    "        rng.shuffle(s_shuf)\n",
    "        Xn = takens_embed(s_shuf, embedding_dim, tau)\n",
    "        An = kneighbors_graph(Xn, n_neighbors=k_neighbors, mode='connectivity', include_self=False)\n",
    "        An = 0.5*(An + An.T); An = (An>0).astype(int); An = sp.csr_matrix(An)\n",
    "        degn = np.array(An.sum(axis=1)).flatten(); degn[degn==0]=1.0\n",
    "        D_inv_sqrtn = sp.diags(1.0/np.sqrt(degn))\n",
    "        Ln = sp.eye(An.shape[0]) - D_inv_sqrtn @ An @ D_inv_sqrtn\n",
    "        valsn, _ = eigsh(Ln.asfptype(), k=min(k_eig, Ln.shape[0]-1), which='SM')\n",
    "        valsn = np.sort(np.real(valsn))\n",
    "        estn = estimate_ds_from_eigs(valsn)\n",
    "        if estn is not None:\n",
    "            null_ds.append(estn)\n",
    "    if null_ds:\n",
    "        print(\"\\nNull-model (temporal shuffle) quick check: d_s_null =\", null_ds)\n",
    "    else:\n",
    "        print(\"\\nNull-model quick check produced no valid d_s (insufficient small-lambda points).\")\n",
    "except Exception as e:\n",
    "    log_event(\"warn\", f\"Null-model quick check failed: {e}\")\n",
    "\n",
    "# End of cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee2530",
   "metadata": {},
   "source": [
    "Interprétation rapide des résultats\n",
    "\n",
    "Estimation ponctuelle : d_s ≈ 2.00 (ajusté sur 19 points λ ≤ 0.2).\n",
    "\n",
    "Bootstrap : médiane d_s ≈ 2.05, mais intervalle 95% très large [≈1.07, 4.42].\n",
    "\n",
    "Propagation T_log (n=3265) : médiane T_log ≈ −15.77 (donc régime Divergence) mais CI 95% pour T_log inclut des valeurs > 0 (≈ [−23.68, +3.42]) à cause de la queue haute de la distribution de d_s.\n",
    "\n",
    "Null-model rapide (temporal shuffle) renvoie un d_s très élevé (~15) — signe qu’au moins un des éléments suivants se produit : (a) le shuffle a changé complètement la structure de l’embedding + graphe (attendu), (b) l’ajustement Theil‑Sen a extrapolé sur très peu de points ou sur valeurs atypiques, ou (c) une erreur de paramétrage pour le null-run (k_eig, taille du graphe) a produit un spectre avec peu de petites λ valides mais des formes qui mènent à pentes extrêmes.\n",
    "\n",
    "Ce que ça signifie pour ton équation T_log\n",
    "\n",
    "Confirmation locale : la médiane T_log négative soutient l’hypothèse (pour Sunspots) que le système est dans le régime Divergence.\n",
    "\n",
    "Incertitude : la large dispersion bootstrap et la CI de T_log traversant zéro indiquent qu’il faut investiguer les sources d’instabilité statistique (points influents, outliers bootstrap, choix de λ_max, sensibilité à embedding/k).\n",
    "\n",
    "Null-model utile : le shuffle radicalise la structure — utile pour test directionnel, mais il faut des nulls construits strictement comparables (mêmes étapes embedding→graph→Laplacien) et plusieurs réplications.\n",
    "\n",
    "\n",
    " Je fournis une cellule autonome unique qui :\n",
    "\n",
    "génère diagnostics clairs pour cette exécution Sunspots :\n",
    "\n",
    "plot du spectre (lambda vs index) et N(lambda) log-log avec la zone λ ≤ 0.2 marquée,\n",
    "\n",
    "scatter log N vs log λ avec la droite Theil‑Sen (point estimate) et annotations,\n",
    "\n",
    "histogramme des échantillons d_s (bootstrap) avec médiane et CI marqués et marquage des outliers (d_s > 4 ou d_s < 0.5),\n",
    "\n",
    "tableau des 10 plus grandes valeurs bootstrap (pour repérer la queue haute),\n",
    "\n",
    "sauvegarde des figures et d'un CSV diagnostic results/sunspots_external/diagnostic_bootstrap_details.csv.\n",
    "\n",
    "imprime des recommandations courtes basées sur ce diagnostic (p.ex. réduire λ_max, inspecter les échantillons bootstrap extrêmes, vérifier embedding/k).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9885ef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic files written to: results\\sunspots_external\\diagnostics\n",
      "\n",
      "Quick summary:\n",
      "- n_lambda_total: 199\n",
      "- n_points_fit_used: 19\n",
      "- d_s_point: 1.9994790122804766\n",
      "- d_s_boot_median: 2.050927876374394\n",
      "- d_s_boot_q025: 1.0731509417247878\n",
      "- d_s_boot_q975: 4.422765618088611\n",
      "- n_boot_retained: 200\n",
      "\n",
      "Recommendations (single actionable checks):\n",
      "1) Inspect Nlambda_loglog_fit.png and Nlambda_loglog_fit.png to confirm fit alignment on lambda <= lambda_max.\n",
      "2) If bootstrap shows heavy right tail (many d_s > 4), inspect results\\sunspots_external\\diagnostics\\ds_boot_top_high.csv to see if a few resamples cause the tail.\n",
      "3) Consider re-running with a smaller lambda_max (e.g. 0.1) or removing the smallest k eigenvalues if small-lambda noise dominates.\n",
      "4) If null-model produced extreme d_s, run several null replicates and compare medians (nulls should be processed identically).\n",
      "\n",
      "Figures:\n",
      "- spectrum index-lambda: results\\sunspots_external\\diagnostics\\spectrum_index_lambda.png\n",
      "- N(lambda) log-log with fit: results\\sunspots_external\\diagnostics\\Nlambda_loglog_fit.png\n",
      "- d_s bootstrap histogram: results\\sunspots_external\\diagnostics\\ds_boot_hist.png\n",
      "- top-high d_s CSV: results\\sunspots_external\\diagnostics\\ds_boot_top_high.csv\n",
      "- diagnostic summary CSV: results\\sunspots_external\\diagnostics\\diagnostic_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Cellule unique de diagnostic détaillé (Sunspots run) :\n",
    "# Produit : spectre, N(lambda) log-log + fit Theil-Sen, histogramme bootstrap d_s, table outliers, sauvegarde fichiers\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "root = Path.cwd()\n",
    "results_dir = Path(\"results/sunspots_external\")\n",
    "eig_fp = results_dir / \"laplacian_eigenvalues.csv\"\n",
    "ds_boot_fp = results_dir / \"external_ds_boot.csv\"\n",
    "out_dir = results_dir / \"diagnostics\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "if not eig_fp.exists():\n",
    "    raise FileNotFoundError(\"Eigenvalues file not found: \" + str(eig_fp))\n",
    "eig = pd.read_csv(eig_fp)[\"lambda\"].to_numpy(dtype=float)\n",
    "eig = np.sort(eig[eig>0])\n",
    "\n",
    "if not ds_boot_fp.exists():\n",
    "    raise FileNotFoundError(\"Bootstrap d_s file not found: \" + str(ds_boot_fp))\n",
    "ds_boot = pd.read_csv(ds_boot_fp)[\"d_s\"].to_numpy(dtype=float)\n",
    "\n",
    "# Parameters\n",
    "lambda_max = 0.2\n",
    "n_system = 3265  # same as run; adjust if needed\n",
    "\n",
    "# 1) Spectrum plot (lambda vs index)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(np.arange(1, len(eig)+1), eig, '-o', markersize=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('index (rank)')\n",
    "plt.ylabel('lambda (log scale)')\n",
    "plt.title('Spectrum: laplacian eigenvalues (ascending)')\n",
    "plt.grid(True, which='both', ls=':', alpha=0.5)\n",
    "spec_fp = out_dir / \"spectrum_index_lambda.png\"\n",
    "plt.tight_layout(); plt.savefig(spec_fp, dpi=150); plt.close()\n",
    "\n",
    "# 2) N(lambda) log-log and Theil-Sen fit on lambda <= lambda_max\n",
    "lam = eig\n",
    "Nlam = np.arange(1, len(lam)+1)\n",
    "mask = lam <= lambda_max\n",
    "lam_sel = lam[mask]; N_sel = Nlam[mask]\n",
    "if len(lam_sel) >= 3:\n",
    "    x = np.log(lam_sel).reshape(-1,1); y = np.log(N_sel)\n",
    "    reg = TheilSenRegressor(random_state=0).fit(x,y)\n",
    "    slope = float(reg.coef_[0]); intercept = float(reg.intercept_)\n",
    "    d_s_point = 2.0 * slope\n",
    "else:\n",
    "    slope = np.nan; intercept = np.nan; d_s_point = np.nan\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "plt.loglog(lam, Nlam, 'o', markersize=3, label='N(lambda)')\n",
    "if len(lam_sel) >= 3:\n",
    "    x_line = np.linspace(np.log(lam_sel).min(), np.log(lam_sel).max(), 200)\n",
    "    plt.loglog(np.exp(x_line), np.exp(intercept + slope * x_line), color='C1', lw=2, label=f\"Theil-Sen slope={slope:.3f}\")\n",
    "plt.axvline(lambda_max, color='gray', linestyle='--', label=f'lambda_max={lambda_max}')\n",
    "plt.xlabel('lambda (log)')\n",
    "plt.ylabel('N(lambda) (log)')\n",
    "plt.title('N(lambda) and Theil-Sen fit (lambda <= lambda_max)')\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, which='both', ls=':', alpha=0.5)\n",
    "nlam_fp = out_dir / \"Nlambda_loglog_fit.png\"\n",
    "plt.tight_layout(); plt.savefig(nlam_fp, dpi=150); plt.close()\n",
    "\n",
    "# 3) Histogram of bootstrap d_s with median and CI, mark outliers\n",
    "if len(ds_boot)>0:\n",
    "    med = np.median(ds_boot); q025, q975 = np.quantile(ds_boot, [0.025, 0.975])\n",
    "    plt.figure(figsize=(6,3.5))\n",
    "    plt.hist(ds_boot, bins=40, color='C0', alpha=0.85)\n",
    "    plt.axvline(med, color='k', linestyle='--', label=f\"median {med:.3f}\")\n",
    "    plt.axvline(q025, color='gray', linestyle=':', label='CI 2.5%')\n",
    "    plt.axvline(q975, color='gray', linestyle=':', label='CI 97.5%')\n",
    "    # mark outliers (heuristic thresholds)\n",
    "    out_high = ds_boot > 4.0\n",
    "    out_low = ds_boot < 0.5\n",
    "    if out_high.any():\n",
    "        plt.hist(ds_boot[out_high], bins=20, color='red', alpha=0.6, label=f'high outliers (>{4.0}): {out_high.sum()}')\n",
    "    if out_low.any():\n",
    "        plt.hist(ds_boot[out_low], bins=20, color='purple', alpha=0.6, label=f'low outliers (<{0.5}): {out_low.sum()}')\n",
    "    plt.xlabel('d_s (bootstrap samples)')\n",
    "    plt.ylabel('count')\n",
    "    plt.title('Bootstrap d_s distribution')\n",
    "    plt.legend(fontsize=8)\n",
    "    hist_ds_fp = out_dir / \"ds_boot_hist.png\"\n",
    "    plt.tight_layout(); plt.savefig(hist_ds_fp, dpi=150); plt.close()\n",
    "else:\n",
    "    med = np.nan; q025 = np.nan; q975 = np.nan\n",
    "    hist_ds_fp = None\n",
    "\n",
    "# 4) Table of top 10 highest d_s boot values (to inspect tail)\n",
    "if len(ds_boot)>0:\n",
    "    top_high = np.sort(ds_boot)[-20:][::-1]\n",
    "    df_high = pd.DataFrame({\"d_s_high\": top_high})\n",
    "    df_high.to_csv(out_dir / \"ds_boot_top_high.csv\", index=False)\n",
    "else:\n",
    "    df_high = pd.DataFrame()\n",
    "\n",
    "# 5) Save diagnostic CSV summary\n",
    "summary = {\n",
    "    \"n_lambda_total\": int(len(lam)),\n",
    "    \"n_points_fit_used\": int(len(lam_sel)),\n",
    "    \"d_s_point\": float(d_s_point) if not np.isnan(d_s_point) else None,\n",
    "    \"d_s_boot_median\": float(med) if not np.isnan(med) else None,\n",
    "    \"d_s_boot_q025\": float(q025) if not np.isnan(q025) else None,\n",
    "    \"d_s_boot_q975\": float(q975) if not np.isnan(q975) else None,\n",
    "    \"n_boot_retained\": int(len(ds_boot))\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(out_dir / \"diagnostic_summary.csv\", index=False)\n",
    "\n",
    "# 6) Print concise human-readable results and recommendations\n",
    "print(\"Diagnostic files written to:\", out_dir)\n",
    "print(\"\\nQuick summary:\")\n",
    "for k,v in summary.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "print(\"\\nRecommendations (single actionable checks):\")\n",
    "print(\"1) Inspect\", nlam_fp.name, \"and\", nlam_fp.name, \"to confirm fit alignment on lambda <= lambda_max.\")\n",
    "print(\"2) If bootstrap shows heavy right tail (many d_s > 4), inspect\", out_dir / \"ds_boot_top_high.csv\", \"to see if a few resamples cause the tail.\")\n",
    "print(\"3) Consider re-running with a smaller lambda_max (e.g. 0.1) or removing the smallest k eigenvalues if small-lambda noise dominates.\")\n",
    "print(\"4) If null-model produced extreme d_s, run several null replicates and compare medians (nulls should be processed identically).\")\n",
    "\n",
    "# print paths of generated figures to inspect\n",
    "print(\"\\nFigures:\")\n",
    "print(\"- spectrum index-lambda:\", spec_fp)\n",
    "print(\"- N(lambda) log-log with fit:\", nlam_fp)\n",
    "print(\"- d_s bootstrap histogram:\", hist_ds_fp)\n",
    "print(\"- top-high d_s CSV:\", out_dir / \"ds_boot_top_high.csv\")\n",
    "print(\"- diagnostic summary CSV:\", out_dir / \"diagnostic_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ad895",
   "metadata": {},
   "source": [
    "Résumé interprétatif (en 4 points)\n",
    "\n",
    "Estimation ponctuelle et bootstrap\n",
    "\n",
    "d_s point ≈ 2.00 ; bootstrap médian ≈ 2.05 — ceci suggère une dimension spectrale ≈ 2 pour Sunspots (dans ta configuration d'embedding/k).\n",
    "\n",
    "Incertitude et queue haute\n",
    "\n",
    "L’IC bootstrap est large : [≈1.07, 4.42]. La distribution a une queue droite avec ~10 échantillons > 4 (ds_boot_top_high.csv). Cette queue élargit la CI et fait que T_log CI traverse zéro malgré une médiane très négative.\n",
    "\n",
    "Origine probable de l’instabilité\n",
    "\n",
    "Peu de points utilisés pour le fit (19 points λ ≤ 0.2) ; Theil‑Sen extrapole sur peu de données.\n",
    "\n",
    "Certains resamples bootstrap sélectionnent configurations d’eigenvalues qui génèrent pentes très raides → outliers dans d_s.\n",
    "\n",
    "Le null-model shuffle (un seul essai) a donné un d_s énorme ; cela requiert réplication contrôlée (multiple nulls traités identiquement) pour interpréter.\n",
    "\n",
    "Conclusion sur la question scientifique\n",
    "\n",
    "Ton pipeline fournit une validation empirique locale (Sunspots) orientée : médiane T_log fortement négative → régime « Divergence » pour ces paramètres.\n",
    "\n",
    "Ce n’est pas une démonstration universelle ; il faut réduire les sources d’incertitude avant d’arguer généralité.\n",
    "\n",
    "Proposition unique et prioritaire (une cellule — exécute-la si tu veux avancer maintenant)\n",
    "\n",
    "Ce que fait la cellule : refit l’estimation en testant une petite grille de prétraitements qui ciblent exactement les causes d’instabilité observées :\n",
    "\n",
    "Essayez lambda_max = 0.1 (plus conservateur) et lambda_max = 0.2 (référence).\n",
    "\n",
    "Pour chaque lambda_max, refaites l’ajustement Theil‑Sen après avoir exclu k smallest eigenvalues where k in {0,1,2,3}.\n",
    "\n",
    "Pour chaque combinaison, calcule point estimate d_s, nombre de points utilisés, et la propagation T_log (avec n_system = 3265).\n",
    "\n",
    "Écris un petit CSV summary (results/sunspots_external/robust_grid_lambda_exclude_summary.csv) et prints succincts des combinaisons qui stabilisent la médiane et réduisent la queue droite.\n",
    "\n",
    "Pourquoi : cela teste directement si la queue droite est due à bruit aux plus petites valeurs propres ou au fait que λ_max est trop large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5169324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid results written to: results\\sunspots_external\\robust_grid_lambda_exclude_summary.csv\n",
      " lambda_max  exclude_k  n_points_total  n_points_used      d_s      T_log            note\n",
      "        0.1          0             199              9 1.441367 -20.701939                \n",
      "        0.1          1             198              8 1.976626 -16.371146                \n",
      "        0.1          2             197              7 2.820601  -9.542536                \n",
      "        0.1          3             196              6 4.082882   0.670600 high_ds_outlier\n",
      "        0.2          0             199             19 1.999479 -16.186245                \n",
      "        0.2          1             198             18 2.304848 -13.715498                \n",
      "        0.2          2             197             17 2.764757  -9.994368                \n",
      "        0.2          3             196             16 3.236706  -6.175827                \n"
     ]
    }
   ],
   "source": [
    "# Cellule unique : grid test pour lambda_max ∈ {0.1,0.2} et exclusion des k smallest eigenvals k=0..3\n",
    "# Produit : CSV summary and console table showing stability of d_s and T_log\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, math\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "root = Path.cwd()\n",
    "results_dir = Path(\"results/sunspots_external\")\n",
    "eig_fp = results_dir / \"laplacian_eigenvalues.csv\"\n",
    "out_fp = results_dir / \"robust_grid_lambda_exclude_summary.csv\"\n",
    "\n",
    "if not eig_fp.exists():\n",
    "    raise FileNotFoundError(\"Eigenvalues file not found: \" + str(eig_fp))\n",
    "\n",
    "# load eigenvalues (sorted positive)\n",
    "eig = pd.read_csv(eig_fp)[\"lambda\"].to_numpy(dtype=float)\n",
    "eig = np.sort(eig[eig>0])\n",
    "\n",
    "n_system = 3265   # same as run; adjust if needed\n",
    "\n",
    "lambda_tests = [0.1, 0.2]\n",
    "exclude_k = [0,1,2,3]\n",
    "\n",
    "def estimate_ds(lam_arr, lambda_max):\n",
    "    lam = lam_arr\n",
    "    Nlam = np.arange(1, len(lam)+1)\n",
    "    mask = lam <= lambda_max\n",
    "    if mask.sum() < 3:\n",
    "        return None\n",
    "    x = np.log(lam[mask]).reshape(-1,1)\n",
    "    y = np.log(Nlam[mask])\n",
    "    reg = TheilSenRegressor(random_state=0).fit(x,y)\n",
    "    slope = float(reg.coef_[0])\n",
    "    d_s = 2.0 * slope\n",
    "    return {\"d_s\": d_s, \"n_points\": int(mask.sum()), \"slope\": slope}\n",
    "\n",
    "rows = []\n",
    "for lam_max in lambda_tests:\n",
    "    for k in exclude_k:\n",
    "        if k >= len(eig)-1:\n",
    "            # can't exclude more than available\n",
    "            rows.append({\n",
    "                \"lambda_max\": lam_max,\n",
    "                \"exclude_k\": k,\n",
    "                \"n_points_total\": len(eig),\n",
    "                \"n_points_used\": None,\n",
    "                \"d_s\": None,\n",
    "                \"T_log\": None,\n",
    "                \"note\": \"exclude exceeds eigencount\"\n",
    "            })\n",
    "            continue\n",
    "        lam_trim = eig[k:]  # drop k smallest eigenvalues\n",
    "        est = estimate_ds(lam_trim, lam_max)\n",
    "        if est is None:\n",
    "            rows.append({\n",
    "                \"lambda_max\": lam_max,\n",
    "                \"exclude_k\": k,\n",
    "                \"n_points_total\": len(lam_trim),\n",
    "                \"n_points_used\": 0,\n",
    "                \"d_s\": None,\n",
    "                \"T_log\": None,\n",
    "                \"note\": \"too few points <= lambda_max\"\n",
    "            })\n",
    "        else:\n",
    "            d_s = float(est[\"d_s\"])\n",
    "            n_used = int(est[\"n_points\"])\n",
    "            T_log = (d_s - 4.0) * math.log(n_system)\n",
    "            # add brief diagnostic flag if d_s > 4 or d_s < 0.5\n",
    "            flag = \"\"\n",
    "            if d_s > 4.0:\n",
    "                flag = \"high_ds_outlier\"\n",
    "            elif d_s < 0.5:\n",
    "                flag = \"low_ds_outlier\"\n",
    "            rows.append({\n",
    "                \"lambda_max\": lam_max,\n",
    "                \"exclude_k\": k,\n",
    "                \"n_points_total\": int(len(lam_trim)),\n",
    "                \"n_points_used\": n_used,\n",
    "                \"d_s\": d_s,\n",
    "                \"T_log\": T_log,\n",
    "                \"note\": flag\n",
    "            })\n",
    "\n",
    "df_res = pd.DataFrame(rows)\n",
    "df_res.to_csv(out_fp, index=False)\n",
    "\n",
    "# print concise table sorted by stability heuristics (small |T_log| uncertainty proxy = larger n_points_used and d_s moderate)\n",
    "print(\"Grid results written to:\", out_fp)\n",
    "print(df_res.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6a2d4",
   "metadata": {},
   "source": [
    "Résumé bref\n",
    "\n",
    "Résultats de la grille : les configurations avec plus de points utilisés (n_points_used élevé) donnent des estimations plus stables; les configurations qui excluent trop de petits λ (exclude_k ≥ 3) produisent des d_s très élevés ou instables (queue droite).\n",
    "\n",
    "Choix recommandé (meilleur compromis stabilité / signal) : lambda_max = 0.2, exclude_k = 0.\n",
    "\n",
    "Raison : utilise 19 points pour le fit; d_s ≈ 1.999 (≈2.0) et T_log ≈ −16.19 — clairement négatif, cohérent avec la conclusion « Divergence », tout en évitant la forte sensibilité due à l’exclusion de petits λ ou à une λ_max trop petite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "975a0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final configuration applied and artifacts written:\n",
      "- final summary: results\\sunspots_external\\final_choice_summary.csv\n",
      "- annotated fit plot: results\\sunspots_external\\diagnostics\\Nlambda_loglog_fit_final.png\n",
      "- appended note to: results\\ds_remove_small_lambda\\run_ready_for_commit.txt\n",
      "\n",
      "Final values:\n",
      "- timestamp: 2025-11-11T03:49:59.602064Z\n",
      "- lambda_max: 0.2\n",
      "- exclude_k: 0\n",
      "- n_lambda_total: 199\n",
      "- n_points_used: 19\n",
      "- slope: 0.9997395061402383\n",
      "- d_s: 1.9994790122804766\n",
      "- T_log: -16.186245402896272\n"
     ]
    }
   ],
   "source": [
    "# Cellule unique : appliquer la configuration choisie (lambda_max=0.2, exclude_k=0),\n",
    "# recalculer fit Theil-Sen, sauvegarder summary final, annoter et réécrire le plot final,\n",
    "# et enregistrer une note dans run_ready_for_commit.txt\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from datetime import datetime\n",
    "\n",
    "# Config retenue\n",
    "results_dir = Path(\"results/sunspots_external\")\n",
    "eig_fp = results_dir / \"laplacian_eigenvalues.csv\"\n",
    "out_summary_fp = results_dir / \"final_choice_summary.csv\"\n",
    "diag_dir = results_dir / \"diagnostics\"\n",
    "diag_dir.mkdir(parents=True, exist_ok=True)\n",
    "lambda_max = 0.2\n",
    "exclude_k = 0\n",
    "n_system = 3265\n",
    "\n",
    "if not eig_fp.exists():\n",
    "    raise FileNotFoundError(\"Eigenvalues file not found: \" + str(eig_fp))\n",
    "\n",
    "# load eigenvalues, trim exclude_k smallest positives\n",
    "eig = pd.read_csv(eig_fp)[\"lambda\"].to_numpy(dtype=float)\n",
    "eig = np.sort(eig[eig>0])\n",
    "if exclude_k >= len(eig):\n",
    "    raise ValueError(\"exclude_k too large for eigenvalue count\")\n",
    "\n",
    "lam = eig[exclude_k:]\n",
    "Nlam = np.arange(1, len(lam)+1)\n",
    "mask = lam <= lambda_max\n",
    "n_used = int(mask.sum())\n",
    "\n",
    "if n_used < 3:\n",
    "    raise RuntimeError(f\"Too few points <= {lambda_max} after excluding {exclude_k} smallest lambdas: {n_used} points\")\n",
    "\n",
    "# fit Theil-Sen\n",
    "x = np.log(lam[mask]).reshape(-1,1)\n",
    "y = np.log(Nlam[mask])\n",
    "reg = TheilSenRegressor(random_state=0).fit(x,y)\n",
    "slope = float(reg.coef_[0]); intercept = float(reg.intercept_)\n",
    "d_s = 2.0 * slope\n",
    "T_log = (d_s - 4.0) * math.log(n_system)\n",
    "\n",
    "# write final summary CSV\n",
    "summary = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n",
    "    \"lambda_max\": float(lambda_max),\n",
    "    \"exclude_k\": int(exclude_k),\n",
    "    \"n_lambda_total\": int(len(lam)),\n",
    "    \"n_points_used\": int(n_used),\n",
    "    \"slope\": float(slope),\n",
    "    \"d_s\": float(d_s),\n",
    "    \"T_log\": float(T_log)\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(out_summary_fp, index=False)\n",
    "\n",
    "# regenerate annotated N(lambda) log-log plot (final)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.loglog(lam, Nlam, 'o', markersize=3, label='N(lambda)')\n",
    "x_line = np.linspace(np.log(lam[mask]).min(), np.log(lam[mask]).max(), 200)\n",
    "plt.loglog(np.exp(x_line), np.exp(intercept + slope * x_line), color='C1', lw=2,\n",
    "           label=f\"Theil-Sen slope={slope:.3f}  d_s={d_s:.3f}\")\n",
    "plt.axvline(lambda_max, color='gray', linestyle='--', label=f'lambda_max={lambda_max}')\n",
    "plt.xlabel('lambda (log)')\n",
    "plt.ylabel('N(lambda) (log)')\n",
    "plt.title('Final N(lambda) fit (chosen config)')\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, which='both', ls=':', alpha=0.4)\n",
    "final_plot_fp = diag_dir / \"Nlambda_loglog_fit_final.png\"\n",
    "plt.tight_layout(); plt.savefig(final_plot_fp, dpi=150); plt.close()\n",
    "\n",
    "# append note to run_ready_for_commit.txt\n",
    "run_ready = Path(\"results/ds_remove_small_lambda/run_ready_for_commit.txt\")\n",
    "note = (f\"[{datetime.utcnow().isoformat()}Z] Final choice: lambda_max={lambda_max}, exclude_k={exclude_k}, \"\n",
    "        f\"d_s={d_s:.6f}, T_log={T_log:.6f}, n_points_used={n_used}\\n\")\n",
    "run_ready.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(run_ready, \"a\", encoding=\"utf8\") as f:\n",
    "    f.write(note)\n",
    "\n",
    "# Print concise output\n",
    "print(\"Final configuration applied and artifacts written:\")\n",
    "print(\"- final summary:\", out_summary_fp)\n",
    "print(\"- annotated fit plot:\", final_plot_fp)\n",
    "print(\"- appended note to:\", run_ready)\n",
    "print(\"\\nFinal values:\")\n",
    "for k,v in summary.items():\n",
    "    print(f\"- {k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5vs3Hjg7uRmHLLvE8miZh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
