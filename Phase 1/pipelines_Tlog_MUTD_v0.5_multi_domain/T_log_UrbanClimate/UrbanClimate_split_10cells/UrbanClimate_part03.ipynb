{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dddd2e7",
   "metadata": {
    "id": "gnciOabhPBDl"
   },
   "source": [
    "Cellule Python — Quality checks pour features clés (statistics, plots, corrélations robustes, VIF, outliers)\n",
    "\n",
    "Cette cellule :\n",
    "\n",
    "calcule statistiques robustes pour humidity_percent, wind_speed_ms, urban_heat_island_intensity ;\n",
    "\n",
    "génère et sauvegarde plots : histogrammes, boxplots winsorisés, QQ-plots, séries temporelles agrégées par ville ;\n",
    "\n",
    "calcule corrélations Spearman et covariance robuste (MinCovDet) si disponible ;\n",
    "\n",
    "calcule VIF via régressions linéaires (sans dépendance à statsmodels) ;\n",
    "\n",
    "détecte outliers par IQR et par score de Mahalanobis robuste (si MinCovDet dispo) ;\n",
    "\n",
    "sauvegarde CSVs, images, et crée results/feature_quality_report.md ; met à jour logs/logs.csv et logs/summary.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ffa3104",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24328,
     "status": "ok",
     "timestamp": 1761427130553,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "dk-TZtI8PF9W",
    "outputId": "e635365f-53e4-4278-8f1a-ab1c2aee9d5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.678026742265067 > -73.719412622132126). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.664644358397889 > -74.017481379064989). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.664644358397889 > -74.017481379064989). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2.225301601740608 > -74.007197640245892). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.664644358397889 > -74.017481379064989). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2.225301601740608 > -74.007197640245892). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.846520326786257 > -73.504581783511114). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.835198931878889 > -74.039892195595996). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-1.763907845716776 > -74.736794638732647). You may want to try with a higher value of support_fraction (current value: 0.600).\n",
      "  warnings.warn(\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature quality checks done.\n",
      "Outputs:\n",
      " - stats CSV: results\\feature_quality_stats.csv\n",
      " - outliers CSV: results\\feature_outliers_detected.csv\n",
      " - spearman matrix: results\\feature_spearman_corr.csv\n",
      " - vif CSV: results\\feature_vif.csv\n",
      " - mahalanobis scores: results\\feature_mahalanobis_scores.csv\n",
      " - report MD: results\\feature_quality_report.md\n",
      " - plots directory: results\\feature_quality_plots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:278: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  f.write(f'Generated: {datetime.utcnow().isoformat()}Z\\n\\n')\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1431911148.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n"
     ]
    }
   ],
   "source": [
    "# Feature quality checks — humidity_percent, wind_speed_ms, urban_heat_island_intensity\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.covariance import MinCovDet\n",
    "from scipy import stats\n",
    "\n",
    "# Réglages\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "DATA_FP = os.path.join('data','urban_climate.csv')\n",
    "RESULTS_DIR = 'results'\n",
    "LOGS_CSV = os.path.join('logs','logs.csv')\n",
    "SUMMARY_MD = os.path.join('logs','summary.md')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Features cibles\n",
    "features = ['humidity_percent','wind_speed_ms','urban_heat_island_intensity']\n",
    "agg_cols = ['city','country','latitude','longitude']  # identique méthode verrouillée\n",
    "\n",
    "# Fichiers de sortie\n",
    "TSV_STATS = os.path.join(RESULTS_DIR, 'feature_quality_stats.csv')\n",
    "CSV_OUTLIERS = os.path.join(RESULTS_DIR, 'feature_outliers_detected.csv')\n",
    "PLOT_DIR = os.path.join(RESULTS_DIR, 'feature_quality_plots')\n",
    "REPORT_MD = os.path.join(RESULTS_DIR, 'feature_quality_report.md')\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "def append_log(level, message):\n",
    "    ts = datetime.utcnow().isoformat() + 'Z'\n",
    "    row = {'timestamp': ts, 'level': level, 'message': message}\n",
    "    try:\n",
    "        if os.path.exists(LOGS_CSV):\n",
    "            df_logs = pd.read_csv(LOGS_CSV)\n",
    "            df_logs = pd.concat([df_logs, pd.DataFrame([row])], ignore_index=True)\n",
    "        else:\n",
    "            df_logs = pd.DataFrame([row])\n",
    "        df_logs.to_csv(LOGS_CSV, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    with open(SUMMARY_MD, 'a', encoding='utf-8') as f:\n",
    "        f.write(f'\\n- {ts} {level}: {message}\\n')\n",
    "\n",
    "# Helpers robust metrics\n",
    "def mad(x):\n",
    "    return np.median(np.abs(x - np.median(x)))\n",
    "\n",
    "def robust_skew_kurt(x):\n",
    "    # fallback to scipy if necessary\n",
    "    try:\n",
    "        sk = stats.skew(x, bias=False)\n",
    "        ku = stats.kurtosis(x, bias=False)\n",
    "    except Exception:\n",
    "        sk, ku = float('nan'), float('nan')\n",
    "    return sk, ku\n",
    "\n",
    "def winsorize_series(s, lower_q=0.01, upper_q=0.99):\n",
    "    lo = np.quantile(s, lower_q)\n",
    "    hi = np.quantile(s, upper_q)\n",
    "    return np.clip(s, lo, hi)\n",
    "\n",
    "def compute_vif(X_df):\n",
    "    # X_df: DataFrame of numeric predictors\n",
    "    cols = X_df.columns.tolist()\n",
    "    vif_dict = {}\n",
    "    for i, col in enumerate(cols):\n",
    "        y = X_df[col].values\n",
    "        X = X_df.drop(columns=[col]).values\n",
    "        if X.shape[1] == 0:\n",
    "            vif = np.nan\n",
    "        else:\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "            r2 = reg.score(X, y)\n",
    "            if r2 >= 1.0:\n",
    "                vif = float('inf')\n",
    "            else:\n",
    "                vif = 1.0 / (1.0 - r2)\n",
    "        vif_dict[col] = vif\n",
    "    return vif_dict\n",
    "\n",
    "def mahalanobis_scores(X):\n",
    "    # robust Mahalanobis using MinCovDet if available; fallback to empirical covariance\n",
    "    try:\n",
    "        mcd = MinCovDet().fit(X)\n",
    "        md = mcd.mahalanobis(X)\n",
    "    except Exception:\n",
    "        # fallback: classical\n",
    "        cov = np.cov(X, rowvar=False)\n",
    "        try:\n",
    "            invcov = np.linalg.inv(cov)\n",
    "            mean = np.mean(X, axis=0)\n",
    "            md = np.array([ (x-mean) @ invcov @ (x-mean) for x in X ])\n",
    "        except Exception:\n",
    "            md = np.full(X.shape[0], np.nan)\n",
    "    return md\n",
    "\n",
    "# Start\n",
    "append_log('INFO', 'Feature quality check started')\n",
    "try:\n",
    "    if not os.path.exists(DATA_FP):\n",
    "        raise FileNotFoundError(f'Data missing: {DATA_FP}')\n",
    "    df = pd.read_csv(DATA_FP)\n",
    "    # Check presence\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        raise RuntimeError(f'Features manquantes: {missing_features}')\n",
    "    # Quick global counts\n",
    "    total_rows = df.shape[0]\n",
    "    # Aggregate per city (mean) as earlier pipeline does\n",
    "    df['city_key'] = df[agg_cols].astype(str).agg('_'.join, axis=1)\n",
    "    city_df = df.groupby(['city_key','city','country'])[features + ['year','month']].agg(lambda x: list(x) if x.name in ['year','month'] else np.mean(x)).reset_index()\n",
    "    # For time series per city we will re-derive a per-city time series later\n",
    "\n",
    "    # Compute robust stats per feature (global, and per-city distributions summary)\n",
    "    stats_rows = []\n",
    "    for feat in features:\n",
    "        arr = df[feat].dropna().values.astype(float)\n",
    "        n = arr.size\n",
    "        nan_prop = df[feat].isna().mean()\n",
    "        mn = np.mean(arr) if n>0 else np.nan\n",
    "        med = np.median(arr) if n>0 else np.nan\n",
    "        sd = np.std(arr, ddof=1) if n>1 else np.nan\n",
    "        q1 = np.quantile(arr, 0.25) if n>0 else np.nan\n",
    "        q3 = np.quantile(arr, 0.75) if n>0 else np.nan\n",
    "        iqr = q3 - q1 if n>0 else np.nan\n",
    "        mad_v = mad(arr) if n>0 else np.nan\n",
    "        skew_v, kurt_v = robust_skew_kurt(arr) if n>0 else (np.nan, np.nan)\n",
    "        min_v = np.min(arr) if n>0 else np.nan\n",
    "        max_v = np.max(arr) if n>0 else np.nan\n",
    "        stats_rows.append({\n",
    "            'feature': feat,\n",
    "            'n': int(n),\n",
    "            'nan_prop': float(nan_prop),\n",
    "            'min': float(min_v),\n",
    "            'q1': float(q1),\n",
    "            'median': float(med),\n",
    "            'q3': float(q3),\n",
    "            'max': float(max_v),\n",
    "            'mean': float(mn),\n",
    "            'std': float(sd),\n",
    "            'IQR': float(iqr),\n",
    "            'MAD': float(mad_v),\n",
    "            'skewness': float(skew_v),\n",
    "            'kurtosis': float(kurt_v)\n",
    "        })\n",
    "\n",
    "    stats_df = pd.DataFrame(stats_rows)\n",
    "    stats_df.to_csv(TSV_STATS, index=False)\n",
    "    append_log('INFO', f'Computed robust stats and saved to {TSV_STATS}')\n",
    "\n",
    "    # Plots: hist + boxplot (winsorized) + QQ for each feature\n",
    "    for feat in features:\n",
    "        s = df[feat].dropna().astype(float).values\n",
    "        if s.size == 0:\n",
    "            continue\n",
    "        # Histogram\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plt.hist(s, bins=40, color='C0', edgecolor='k', alpha=0.7)\n",
    "        plt.xlabel(feat)\n",
    "        plt.ylabel('count')\n",
    "        plt.title(f'Histogram {feat}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOT_DIR, f'hist_{feat}.png'), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # Winsorized boxplot\n",
    "        s_win = winsorize_series(s, lower_q=0.01, upper_q=0.99)\n",
    "        plt.figure(figsize=(6,2.5))\n",
    "        plt.boxplot(s_win, vert=False, widths=0.6, patch_artist=True, boxprops=dict(facecolor='C1'))\n",
    "        plt.xlabel(feat)\n",
    "        plt.title(f'Boxplot winsorisé {feat} (1%/99%)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOT_DIR, f'box_win_{feat}.png'), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # QQ-plot vs normal\n",
    "        plt.figure(figsize=(4,4))\n",
    "        stats.probplot(s, dist=\"norm\", plot=plt)\n",
    "        plt.title(f'QQ-plot {feat}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOT_DIR, f'qq_{feat}.png'), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    append_log('INFO', 'Saved hist, boxplot(winsor), QQ plots for each feature')\n",
    "\n",
    "    # Time series per city for these features: compute yearly median per city to detect drifts\n",
    "    # expand original df to ensure time ordering\n",
    "    df_time = df.copy()\n",
    "    if 'year' in df_time.columns and 'month' in df_time.columns:\n",
    "        df_time['year_month'] = df_time['year'].astype(str) + '-' + df_time['month'].astype(str).str.zfill(2)\n",
    "    else:\n",
    "        df_time['year_month'] = pd.RangeIndex(start=0, stop=df_time.shape[0])\n",
    "    # For a subset of cities (top 6 by count) produce timeseries plots\n",
    "    city_counts = df_time['city_key'].value_counts()\n",
    "    top_cities = city_counts.head(6).index.tolist()\n",
    "    ts_plots = []\n",
    "    for ck in top_cities:\n",
    "        dfc = df_time[df_time['city_key']==ck].sort_values(['year','month'] if 'year' in df_time.columns else df_time.index)\n",
    "        plt.figure(figsize=(8,3))\n",
    "        for feat in features:\n",
    "            plt.plot(dfc['year_month'], dfc[feat].astype(float), marker='.', label=feat, linewidth=0.8)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f'Time series (city) {ck}')\n",
    "        plt.legend(ncol=3)\n",
    "        plt.tight_layout()\n",
    "        outp = os.path.join(PLOT_DIR, f'timeseries_{ck}.png')\n",
    "        plt.savefig(outp, dpi=150)\n",
    "        plt.close()\n",
    "        ts_plots.append(outp)\n",
    "    append_log('INFO', f'Saved time series plots for top cities: {top_cities}')\n",
    "\n",
    "    # Correlations: Spearman rank on aggregated-per-city means (consistently with pipeline)\n",
    "    city_means = df.groupby('city_key')[features].mean()\n",
    "    spearman = city_means.corr(method='spearman')\n",
    "    spearman.to_csv(os.path.join(RESULTS_DIR, 'feature_spearman_corr.csv'))\n",
    "    append_log('INFO', 'Saved Spearman correlation matrix')\n",
    "\n",
    "    # Robust covariance and Mahalanobis scores\n",
    "    X_for_cov = StandardScaler().fit_transform(city_means.values)\n",
    "    try:\n",
    "        mcd = MinCovDet().fit(X_for_cov)\n",
    "        robust_center = mcd.location_\n",
    "        robust_cov = mcd.covariance_\n",
    "        mcd_available = True\n",
    "    except Exception:\n",
    "        mcd_available = False\n",
    "        robust_center = np.nan\n",
    "        robust_cov = np.nan\n",
    "    # Mahalanobis scores (robust or fallback)\n",
    "    md_scores = mahalanobis_scores(X_for_cov)  # uses MinCovDet internally if available\n",
    "    md_df = pd.DataFrame({\n",
    "        'city_key': city_means.index,\n",
    "        'mahalanobis': md_scores\n",
    "    })\n",
    "    md_df.to_csv(os.path.join(RESULTS_DIR, 'feature_mahalanobis_scores.csv'), index=False)\n",
    "    append_log('INFO', f'Computed Mahalanobis scores (robust mcd available: {mcd_available})')\n",
    "\n",
    "    # VIF calculation on city_means (numeric)\n",
    "    vif_dict = compute_vif(city_means)\n",
    "    pd.DataFrame([vif_dict]).T.rename(columns={0:'VIF'}).to_csv(os.path.join(RESULTS_DIR, 'feature_vif.csv'))\n",
    "    append_log('INFO', 'Computed VIF for features')\n",
    "\n",
    "    # Outlier detection per-feature by IQR rule on raw observations and by Mahalanobis on per-city aggregated space\n",
    "    outlier_rows = []\n",
    "    for feat in features:\n",
    "        s = df[feat].astype(float)\n",
    "        q1 = s.quantile(0.25)\n",
    "        q3 = s.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        low = q1 - 1.5 * iqr\n",
    "        high = q3 + 1.5 * iqr\n",
    "        is_out = (s < low) | (s > high)\n",
    "        out_count = int(is_out.sum())\n",
    "        out_prop = float(is_out.mean())\n",
    "        outlier_rows.append({'feature': feat, 'method': 'IQR', 'outliers_count': out_count, 'outliers_prop': out_prop, 'low_thresh': float(low), 'high_thresh': float(high)})\n",
    "\n",
    "    # Mahalanobis-based outliers on city_means\n",
    "    try:\n",
    "        md_threshold = np.nanpercentile(md_scores, 97.5)  # top 2.5% as outliers\n",
    "        md_outliers = md_df[md_df['mahalanobis'] > md_threshold]\n",
    "        for idx, row in md_outliers.iterrows():\n",
    "            outlier_rows.append({'feature': 'multivariate', 'method': 'Mahalanobis', 'city_key': row['city_key'], 'mahalanobis': float(row['mahalanobis'])})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    outlier_df = pd.DataFrame(outlier_rows)\n",
    "    outlier_df.to_csv(CSV_OUTLIERS, index=False)\n",
    "    append_log('INFO', f'Detected outliers summary saved to {CSV_OUTLIERS}')\n",
    "\n",
    "    # Build report markdown\n",
    "    with open(REPORT_MD, 'w', encoding='utf-8') as f:\n",
    "        f.write('# Feature Quality Report\\n\\n')\n",
    "        f.write(f'Generated: {datetime.utcnow().isoformat()}Z\\n\\n')\n",
    "        f.write('## Features analysées\\n\\n')\n",
    "        for feat in features:\n",
    "            f.write(f'- {feat}\\n')\n",
    "        f.write('\\n## Statistiques robustes (globales)\\n\\n')\n",
    "        f.write(stats_df.to_markdown(index=False))\n",
    "        f.write('\\n\\n## Spearman correlation (per-city means)\\n\\n')\n",
    "        f.write(spearman.to_markdown())\n",
    "        f.write('\\n\\n## VIF\\n\\n')\n",
    "        vif_df = pd.read_csv(os.path.join(RESULTS_DIR, 'feature_vif.csv'), index_col=0)\n",
    "        f.write(vif_df.to_markdown())\n",
    "        f.write('\\n\\n## Outliers summary\\n\\n')\n",
    "        f.write(outlier_df.to_markdown(index=False))\n",
    "        f.write('\\n\\n## Plots générés\\n\\n')\n",
    "        for fn in sorted(os.listdir(PLOT_DIR)):\n",
    "            f.write(f'- {os.path.join(PLOT_DIR, fn)}\\n')\n",
    "        f.write('\\n\\n## Notes et recommandations\\n\\n')\n",
    "        f.write('- Si proportion de valeurs manquantes >5% pour une feature, considérer imputation documentée.\\n')\n",
    "        f.write('- Pour outliers extrêmes, proposer winsorisation documentée (p%) ou vérification des métadonnées d\\'instrumentation.\\n')\n",
    "        if not mcd_available:\n",
    "            f.write('- MinCovDet non disponible; Mahalanobis fallback utilisé (classique), préférer robust covariance si possible.\\n')\n",
    "\n",
    "    append_log('INFO', f'Feature quality report generated: {REPORT_MD}')\n",
    "    print(\"Feature quality checks done.\")\n",
    "    print(\"Outputs:\")\n",
    "    print(\" - stats CSV:\", TSV_STATS)\n",
    "    print(\" - outliers CSV:\", CSV_OUTLIERS)\n",
    "    print(\" - spearman matrix:\", os.path.join(RESULTS_DIR, 'feature_spearman_corr.csv'))\n",
    "    print(\" - vif CSV:\", os.path.join(RESULTS_DIR, 'feature_vif.csv'))\n",
    "    print(\" - mahalanobis scores:\", os.path.join(RESULTS_DIR, 'feature_mahalanobis_scores.csv'))\n",
    "    print(\" - report MD:\", REPORT_MD)\n",
    "    print(\" - plots directory:\", PLOT_DIR)\n",
    "\n",
    "except Exception as e:\n",
    "    append_log('ERROR', f'Feature quality check failed: {e}')\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a91a8",
   "metadata": {
    "id": "38cDz0OsQgG_"
   },
   "source": [
    "Prochain pas recommandé (exécution immédiate)\n",
    "\n",
    "Les vérifications montrent que les trois variables clés portent l’essentiel du signal mais présentent des caractéristiques problématiques (urban_heat_island_intensity très discrète; outliers multivariés marqués comme Dallas, San Antonio, Delhi, Houston). Pour prouver T_log de manière honnête et non ambiguë, la prochaine action la plus pertinente est : ré-estimer d et T_log après un prétraitement robuste documenté (winsorisation explicite, RobustScaler, PCA robuste) puis comparer côté‑à‑côté aux résultats actuels.\n",
    "\n",
    "Je fournis maintenant une cellule Python prête à exécuter qui :\n",
    "\n",
    "applique winsorisation documentée (1%/99%) sur les features ;\n",
    "\n",
    "applique RobustScaler puis StandardScaler en alternative pour comparaison ;\n",
    "\n",
    "calcule d_part (participation), d_pca90, d_est et T_log pour chaque prétraitement ;\n",
    "\n",
    "produit un CSV comparatif, plots comparatifs et met à jour logs/results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba3f9d63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1761427490462,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "JKpEUr8VQn2J",
    "outputId": "fa437df9-cc44-4160-d0ce-18d62b4e5d38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-4.584285129024502 > -75.551140897604682). You may want to try with a higher value of support_fraction (current value: 0.650).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust comparison saved: results\\tlog_robust_comparison.csv results\\tlog_robust_comparison.png\n",
      "                   variant   d_part  d_pca90    d_est     T_log\n",
      "   baseline_StandardScaler 3.800500        4 3.900250 -0.298824\n",
      "winsor_1-99_StandardScaler 3.787723        4 3.893862 -0.317962\n",
      "  winsor_1-99_RobustScaler 2.593089        3 2.796544 -3.605231\n",
      "         RobustScaler_only 2.642962        3 2.821481 -3.530527\n",
      "            MinCovDet_proj 3.800500        4 3.900250 -0.298824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1265046648.py:20: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n"
     ]
    }
   ],
   "source": [
    "# Ré-estimation robuste de d et T_log : winsorisation + RobustScaler vs StandardScaler + PCA robuste\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import MinCovDet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "DATA_FP = os.path.join('data','urban_climate.csv')\n",
    "RESULTS_DIR = 'results'\n",
    "OUT_CSV = os.path.join(RESULTS_DIR, 'tlog_robust_comparison.csv')\n",
    "PLOT_PNG = os.path.join(RESULTS_DIR, 'tlog_robust_comparison.png')\n",
    "LOGS_CSV = os.path.join('logs','logs.csv')\n",
    "SUMMARY_MD = os.path.join('logs','summary.md')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def append_log(level, message):\n",
    "    ts = datetime.utcnow().isoformat() + 'Z'\n",
    "    row = {'timestamp': ts, 'level': level, 'message': message}\n",
    "    try:\n",
    "        if os.path.exists(LOGS_CSV):\n",
    "            df_logs = pd.read_csv(LOGS_CSV)\n",
    "            df_logs = pd.concat([df_logs, pd.DataFrame([row])], ignore_index=True)\n",
    "        else:\n",
    "            df_logs = pd.DataFrame([row])\n",
    "        df_logs.to_csv(LOGS_CSV, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    with open(SUMMARY_MD, 'a', encoding='utf-8') as f:\n",
    "        f.write(f'\\n- {ts} {level}: {message}\\n')\n",
    "\n",
    "def participation_ratio(X):\n",
    "    cov = np.cov(X, rowvar=False)\n",
    "    eig = np.linalg.eigvalsh(cov)\n",
    "    eig = np.maximum(eig, 0.0)\n",
    "    s = np.sum(eig)\n",
    "    return 0.0 if s<=0 else (s**2)/np.sum(eig**2)\n",
    "\n",
    "def d_pca90_from_X(X):\n",
    "    pca = PCA(n_components=min(X.shape[0], X.shape[1]))\n",
    "    pca.fit(X)\n",
    "    cum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    return int(np.searchsorted(cum, 0.90) + 1) if cum[-1] >= 0.90 else pca.n_components_\n",
    "\n",
    "# Load locked params to keep consistency\n",
    "with open(os.path.join(RESULTS_DIR,'params.json'),'r',encoding='utf-8') as f:\n",
    "    params = json.load(f)\n",
    "features = params['features']\n",
    "agg_cols = params['aggregation']['aggregation_group']\n",
    "\n",
    "df = pd.read_csv(DATA_FP)\n",
    "df['city_key'] = df[agg_cols].astype(str).agg('_'.join, axis=1)\n",
    "city_df = df.groupby('city_key')[features].mean().reset_index()\n",
    "n_cities = city_df.shape[0]\n",
    "n_eff = max(2, n_cities)\n",
    "\n",
    "# Preprocessing variants\n",
    "variants = []\n",
    "\n",
    "# 1) Baseline (StandardScaler) - reproduce locked\n",
    "X_base = city_df[features].values.astype(float)\n",
    "Xs_base = StandardScaler().fit_transform(X_base)\n",
    "variants.append(('baseline_StandardScaler', Xs_base))\n",
    "\n",
    "# 2) Winsorize (1%/99%) then StandardScaler\n",
    "X_win = X_base.copy()\n",
    "for j in range(X_win.shape[1]):\n",
    "    lo = np.quantile(X_win[:,j], 0.01)\n",
    "    hi = np.quantile(X_win[:,j], 0.99)\n",
    "    X_win[:,j] = np.clip(X_win[:,j], lo, hi)\n",
    "Xs_win_std = StandardScaler().fit_transform(X_win)\n",
    "variants.append(('winsor_1-99_StandardScaler', Xs_win_std))\n",
    "\n",
    "# 3) Winsorize then RobustScaler\n",
    "Xs_win_robust = RobustScaler().fit_transform(X_win)\n",
    "variants.append(('winsor_1-99_RobustScaler', Xs_win_robust))\n",
    "\n",
    "# 4) RobustScaler only\n",
    "Xs_robust_only = RobustScaler().fit_transform(X_base)\n",
    "variants.append(('RobustScaler_only', Xs_robust_only))\n",
    "\n",
    "# 5) PCA-robust: use MinCovDet to attempt robust covariance then whiten via eigenvectors (fallback to PCA)\n",
    "try:\n",
    "    Xs_for_mcd = StandardScaler().fit_transform(X_base)\n",
    "    mcd = MinCovDet().fit(Xs_for_mcd)\n",
    "    cov = mcd.covariance_\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    idx = eigvals.argsort()[::-1]\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:,idx]\n",
    "    Xs_mcd_proj = Xs_for_mcd.dot(eigvecs)  # projected coords\n",
    "    variants.append(('MinCovDet_proj', Xs_mcd_proj))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "rows = []\n",
    "for name, Xs in variants:\n",
    "    d_part = participation_ratio(Xs)\n",
    "    d_pca90 = d_pca90_from_X(Xs)\n",
    "    d_est = (d_part + d_pca90) / 2.0\n",
    "    d_est_clipped = float(np.clip(d_est, params['sanitization']['d_clip_min'], params['sanitization']['d_clip_max']))\n",
    "    T_log = (d_est_clipped - 4.0) * np.log(n_eff)\n",
    "    rows.append({'variant': name, 'd_part': d_part, 'd_pca90': d_pca90, 'd_est': d_est, 'T_log': T_log})\n",
    "\n",
    "res_df = pd.DataFrame(rows)\n",
    "res_df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.bar(res_df['variant'], res_df['T_log'], color='C2', edgecolor='k')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('T_log')\n",
    "plt.title('Comparaison T_log par prétraitement (robuste vs baseline)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_PNG, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "append_log('INFO', f'Robust comparison completed and saved to {OUT_CSV} and {PLOT_PNG}')\n",
    "print('Robust comparison saved:', OUT_CSV, PLOT_PNG)\n",
    "print(res_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054a5be",
   "metadata": {
    "id": "9vTi6mBxRVro"
   },
   "source": [
    "### Verdict recommandé\n",
    "\n",
    "Les résultats montrent que l’estimation verrouillée (StandardScaler) et la projection MinCovDet donnent des T_log proches (≈ -0.299), tandis que les traitements robustes centrés (RobustScaler ou RobustScaler après winsorisation) abaissent fortement d vers ≈3 et produisent T_log ≈ -3.6, signe d’une perte de la structure multivariée informative.  \n",
    "\n",
    "Choix opérationnel pour une preuve empirique honnête et reproductible : conserver la méthode verrouillée documentée et présenter comme contraste la version winsorisée 1%/99% + StandardScaler et la projection robuste MinCovDet. Ces variantes sont transparentes, faciles à reproduire et évitent d’importantes transformations qui pourraient être perçues comme « triche ».  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2de71",
   "metadata": {
    "id": "TeDpYUODSIcM"
   },
   "source": [
    "Cellule Python — Validations comparatives (LOO, sweep, tests) pour pipelines A/B/C\n",
    "\n",
    " Cette cellule :\n",
    "\n",
    "définit trois pipelines (A = verrouillé StandardScaler, B = winsor 1%/99% + StandardScaler, C = MinCovDet projection);\n",
    "\n",
    "pour chaque pipeline calcule d_part, d_pca90, d_est, T_log (biais=0);\n",
    "\n",
    "exécute Leave-One-Out (LOO) sur villes, effectue test t (H0 mean T_log = 0) et calcule métriques de robustesse;\n",
    "\n",
    "réalise un balayage (sous-échantillonnage fractions 0.5/0.75/1.0, répétitions 100, perturbations d ±20%);\n",
    "\n",
    "sauvegarde pour chaque pipeline : CSVs détaillés (d_estimates, LOO, sweep), plots comparatifs et un rapport markdown comparatif;\n",
    "\n",
    "met à jour logs/logs.csv et logs/summary.md, et imprime un résumé compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2a3a02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5148,
     "status": "ok",
     "timestamp": 1761427909746,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "MtQF-ikkSNGE",
    "outputId": "50c03cf6-a091-4cc7-8781-4380f63c3f7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "c:\\Users\\zackd\\anaconda3\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-4.584285129024502 > -75.551140897604682). You may want to try with a higher value of support_fraction (current value: 0.650).\n",
      "  warnings.warn(\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:137: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  pd.DataFrame([{'timestamp': datetime.utcnow().isoformat()+'Z', **metrics, 'n_cities': n_cities}]).to_csv(\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:137: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  pd.DataFrame([{'timestamp': datetime.utcnow().isoformat()+'Z', **metrics, 'n_cities': n_cities}]).to_csv(\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:137: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  pd.DataFrame([{'timestamp': datetime.utcnow().isoformat()+'Z', **metrics, 'n_cities': n_cities}]).to_csv(\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative validation finished.\n",
      "Overview saved to: results\\tlog_pipelines_overview.csv\n",
      "Comparative report: results\\pipelines_comparative_report.md\n",
      "Comparative image: results\\pipelines_comparative_overview.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:252: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  f.write(f'Generated: {datetime.utcnow().isoformat()}Z\\n\\n')\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\833480463.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat() + 'Z'\n"
     ]
    }
   ],
   "source": [
    "# Comparatif pipelines A/B/C : LOO, sweep, tests, sauvegardes\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import MinCovDet\n",
    "from scipy import stats\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Réglages\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "DATA_FP = os.path.join('data', 'urban_climate.csv')\n",
    "RESULTS_DIR = 'results'\n",
    "LOGS_CSV = os.path.join('logs', 'logs.csv')\n",
    "SUMMARY_MD = os.path.join('logs', 'summary.md')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def append_log(level, message):\n",
    "    ts = datetime.utcnow().isoformat() + 'Z'\n",
    "    row = {'timestamp': ts, 'level': level, 'message': message}\n",
    "    try:\n",
    "        if os.path.exists(LOGS_CSV):\n",
    "            df_logs = pd.read_csv(LOGS_CSV)\n",
    "            df_logs = pd.concat([df_logs, pd.DataFrame([row])], ignore_index=True)\n",
    "        else:\n",
    "            df_logs = pd.DataFrame([row])\n",
    "        df_logs.to_csv(LOGS_CSV, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    with open(SUMMARY_MD, 'a', encoding='utf-8') as f:\n",
    "        f.write(f'\\n- {ts} {level}: {message}\\n')\n",
    "\n",
    "# Charger params verrouillés si existants\n",
    "params_fp = os.path.join(RESULTS_DIR, 'params.json')\n",
    "if os.path.exists(params_fp):\n",
    "    with open(params_fp,'r',encoding='utf-8') as f:\n",
    "        locked_params = json.load(f)\n",
    "else:\n",
    "    locked_params = {\n",
    "        \"aggregation\": {\"aggregation_group\": [\"city\",\"country\",\"latitude\",\"longitude\"]},\n",
    "        \"features\": [\"temperature_celsius\",\"humidity_percent\",\"precipitation_mm\",\"wind_speed_ms\",\"urban_heat_island_intensity\"],\n",
    "        \"sanitization\": {\"d_clip_min\":0.1,\"d_clip_max\":100.0},\n",
    "        \"T_log\": {\"n_eff_rule\":\"n_eff = max(2, n_cities)\"}\n",
    "    }\n",
    "\n",
    "features = locked_params['features']\n",
    "agg_cols = locked_params['aggregation']['aggregation_group']\n",
    "\n",
    "# Fonctions utilitaires\n",
    "def participation_ratio(X):\n",
    "    cov = np.cov(X, rowvar=False)\n",
    "    eig = np.linalg.eigvalsh(cov)\n",
    "    eig = np.maximum(eig, 0.0)\n",
    "    s = np.sum(eig)\n",
    "    return 0.0 if s<=0 else (s**2)/np.sum(eig**2)\n",
    "\n",
    "def d_pca90_from_X(X):\n",
    "    pca = PCA(n_components=min(X.shape[0], X.shape[1]))\n",
    "    pca.fit(X)\n",
    "    cum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    return int(np.searchsorted(cum, 0.90) + 1) if cum[-1] >= 0.90 else pca.n_components_\n",
    "\n",
    "def compute_pipeline_variants(city_df):\n",
    "    # Prepare raw matrix\n",
    "    X_raw = city_df[features].values.astype(float)\n",
    "    variants = {}\n",
    "    # A: Baseline StandardScaler (verrouillé)\n",
    "    Xs_A = StandardScaler().fit_transform(X_raw)\n",
    "    variants['A_baseline_StandardScaler'] = Xs_A\n",
    "    # B: Winsor 1%/99% then StandardScaler\n",
    "    X_win = X_raw.copy()\n",
    "    for j in range(X_win.shape[1]):\n",
    "        lo = np.quantile(X_win[:,j], 0.01)\n",
    "        hi = np.quantile(X_win[:,j], 0.99)\n",
    "        X_win[:,j] = np.clip(X_win[:,j], lo, hi)\n",
    "    Xs_B = StandardScaler().fit_transform(X_win)\n",
    "    variants['B_winsor1-99_StandardScaler'] = Xs_B\n",
    "    # C: MinCovDet projection (robust multivariate) if possible, else fallback to baseline\n",
    "    try:\n",
    "        Xs_std = StandardScaler().fit_transform(X_raw)\n",
    "        mcd = MinCovDet().fit(Xs_std)\n",
    "        cov = mcd.covariance_\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "        idx = eigvals.argsort()[::-1]\n",
    "        eigvecs = eigvecs[:,idx]\n",
    "        Xs_C = Xs_std.dot(eigvecs)  # projection preserving robust directions\n",
    "        variants['C_MinCovDet_proj'] = Xs_C\n",
    "    except Exception:\n",
    "        variants['C_MinCovDet_proj'] = Xs_A.copy()\n",
    "    return variants\n",
    "\n",
    "def compute_d_T_for_X(X, n_eff):\n",
    "    d_part = participation_ratio(X)\n",
    "    d_pca90 = d_pca90_from_X(X)\n",
    "    d_est = (d_part + d_pca90) / 2.0\n",
    "    d_est_clipped = float(np.clip(d_est, locked_params['sanitization']['d_clip_min'], locked_params['sanitization']['d_clip_max']))\n",
    "    T_log = (d_est_clipped - 4.0) * np.log(n_eff)\n",
    "    return dict(d_part=d_part, d_pca90=d_pca90, d_est=d_est, d_est_clipped=d_est_clipped, T_log=T_log)\n",
    "\n",
    "# Charger données et agréger par ville comme pipeline\n",
    "append_log('INFO', 'Comparative validation started for pipelines A/B/C')\n",
    "df = pd.read_csv(DATA_FP)\n",
    "df['city_key'] = df[agg_cols].astype(str).agg('_'.join, axis=1)\n",
    "city_df = df.groupby('city_key')[features].mean().reset_index()\n",
    "n_cities = city_df.shape[0]\n",
    "n_eff = max(2, n_cities)\n",
    "\n",
    "# Générer variants matrices\n",
    "variants_X = compute_pipeline_variants(city_df)\n",
    "\n",
    "# Préparer dossiers et fichiers\n",
    "out_overview = os.path.join(RESULTS_DIR, 'tlog_pipelines_overview.csv')\n",
    "out_details_dir = os.path.join(RESULTS_DIR, 'pipeline_details')\n",
    "os.makedirs(out_details_dir, exist_ok=True)\n",
    "\n",
    "overview_rows = []\n",
    "\n",
    "# Paramètres sweep\n",
    "fractions = [0.5, 0.75, 1.0]\n",
    "repeats = 100\n",
    "d_factors = np.linspace(0.8, 1.2, 9)\n",
    "\n",
    "for vname, Xs in variants_X.items():\n",
    "    try:\n",
    "        append_log('INFO', f'Start pipeline {vname}')\n",
    "        # Compute global d and T_log\n",
    "        metrics = compute_d_T_for_X(Xs, n_eff)\n",
    "        overview_rows.append({'pipeline': vname, 'n_cities': n_cities,\n",
    "                              'd_part': metrics['d_part'], 'd_pca90': metrics['d_pca90'],\n",
    "                              'd_est': metrics['d_est'], 'T_log': metrics['T_log']})\n",
    "        # Save per-pipeline basic results\n",
    "        pd.DataFrame([{'timestamp': datetime.utcnow().isoformat()+'Z', **metrics, 'n_cities': n_cities}]).to_csv(\n",
    "            os.path.join(out_details_dir, f'{vname}_d_estimate.csv'), index=False)\n",
    "\n",
    "        # --- Leave-One-Out (LOO) recompute removing each city ---\n",
    "        loo_rows = []\n",
    "        for i in range(Xs.shape[0]):\n",
    "            X_loo = np.delete(Xs, i, axis=0)\n",
    "            if X_loo.shape[0] < 2:\n",
    "                continue\n",
    "            res = compute_d_T_for_X(X_loo, max(2, X_loo.shape[0]))\n",
    "            loo_rows.append({'left_out_city': city_df.loc[i,'city_key'], **res, 'n_used': int(X_loo.shape[0])})\n",
    "        loo_df = pd.DataFrame(loo_rows)\n",
    "        loo_fp = os.path.join(out_details_dir, f'{vname}_loo.csv')\n",
    "        loo_df.to_csv(loo_fp, index=False)\n",
    "\n",
    "        # Summary LOO\n",
    "        mean_T = float(loo_df['T_log'].mean())\n",
    "        std_T = float(loo_df['T_log'].std(ddof=1))\n",
    "        rel_std_pct = float((std_T / (abs(mean_T) + 1e-12)) * 100.0)\n",
    "        # t-test\n",
    "        try:\n",
    "            tstat, pvalue = stats.ttest_1samp(loo_df['T_log'].values, 0.0)\n",
    "        except Exception:\n",
    "            tstat, pvalue = float('nan'), float('nan')\n",
    "        pd.DataFrame([{'pipeline': vname, 'mean_T_LOO': mean_T, 'std_T_LOO': std_T, 'rel_std_pct': rel_std_pct,\n",
    "                       'tstat': tstat, 'pvalue': pvalue}]).to_csv(os.path.join(out_details_dir, f'{vname}_loo_summary.csv'), index=False)\n",
    "\n",
    "        # Plot LOO distribution\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plt.hist(loo_df['T_log'], bins=20, color='C0', edgecolor='k')\n",
    "        plt.axvline(0, color='k', linestyle='--')\n",
    "        plt.title(f'LOO T_log distribution - {vname}')\n",
    "        plt.xlabel('T_log (LOO)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_details_dir, f'{vname}_loo_hist.png'), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Sweep: sous-échantillonnage + perturbation d ---\n",
    "        sweep_rows = []\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        for frac in fractions:\n",
    "            k = max(2, int(np.floor(Xs.shape[0] * frac)))\n",
    "            for rep in range(repeats):\n",
    "                idx = rng.choice(Xs.shape[0], size=k, replace=False)\n",
    "                X_sub = Xs[idx, :]\n",
    "                d_part_sub = participation_ratio(X_sub)\n",
    "                d_pca90_sub = d_pca90_from_X(X_sub)\n",
    "                d_est_sub = float((d_part_sub + d_pca90_sub) / 2.0)\n",
    "                for f in d_factors:\n",
    "                    d_pert = d_est_sub * f\n",
    "                    T = (d_pert - 4.0) * np.log(max(2, k))\n",
    "                    regime = 'Saturation' if T>0 else ('Equilibre' if np.isclose(T,0.0,atol=1e-8) else 'Divergence')\n",
    "                    sweep_rows.append({'pipeline': vname, 'fraction': frac, 'rep': rep, 'n_used': int(k),\n",
    "                                       'd_est_sub': d_est_sub, 'd_factor': float(f), 'd_pert': float(d_pert),\n",
    "                                       'T_log': float(T), 'regime': regime})\n",
    "        sweep_df = pd.DataFrame(sweep_rows)\n",
    "        sweep_fp = os.path.join(out_details_dir, f'{vname}_sweep.csv')\n",
    "        sweep_df.to_csv(sweep_fp, index=False)\n",
    "\n",
    "        # Sweep summary per fraction\n",
    "        summary_rows = []\n",
    "        for frac in fractions:\n",
    "            df_frac = sweep_df[sweep_df['fraction']==frac]\n",
    "            match_frac = (df_frac['regime'] == ('Saturation' if metrics['T_log']>0 else ('Equilibre' if np.isclose(metrics['T_log'],0.0,atol=1e-8) else 'Divergence'))).mean()\n",
    "            unstable_frac = 1.0 - match_frac\n",
    "            medT = float(df_frac['T_log'].median())\n",
    "            stdT = float(df_frac['T_log'].std())\n",
    "            summary_rows.append({'pipeline': vname, 'fraction': frac, 'match_frac': float(match_frac), 'unstable_frac': float(unstable_frac),\n",
    "                                 'median_T': medT, 'std_T': stdT})\n",
    "        pd.DataFrame(summary_rows).to_csv(os.path.join(out_details_dir, f'{vname}_sweep_summary.csv'), index=False)\n",
    "\n",
    "        # Plot sweep heatmap (median T over d_factors x fraction)\n",
    "        pivot = sweep_df.groupby(['d_factor','fraction'])['T_log'].median().unstack(level=1)\n",
    "        plt.figure(figsize=(6,3))\n",
    "        im = plt.imshow(pivot.values, aspect='auto', cmap='RdBu', interpolation='nearest',\n",
    "                        vmin=-np.max(np.abs(pivot.values)), vmax=np.max(np.abs(pivot.values)))\n",
    "        plt.colorbar(im, label='median T_log')\n",
    "        plt.yticks(range(len(pivot.index)), [f\"{v:.2f}\" for v in pivot.index])\n",
    "        plt.xticks(range(len(pivot.columns)), [str(c) for c in pivot.columns])\n",
    "        plt.title(f'Sweep median T_log - {vname}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_details_dir, f'{vname}_sweep_heatmap.png'), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        append_log('INFO', f'Pipeline {vname} validation complete; results saved to {out_details_dir}')\n",
    "    except Exception as e:\n",
    "        append_log('ERROR', f'Pipeline {vname} failed: {e}')\n",
    "        raise\n",
    "\n",
    "# Save overview\n",
    "overview_df = pd.DataFrame(overview_rows)\n",
    "overview_df.to_csv(out_overview, index=False)\n",
    "\n",
    "# Comparative plots across pipelines: T_log and d_est\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(overview_df['pipeline'], overview_df['T_log'], color='C3', edgecolor='k')\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('T_log par pipeline')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(overview_df['pipeline'], overview_df['d_est'], color='C4', edgecolor='k')\n",
    "plt.axhline(4, color='k', linestyle='--', label='d=4 reference')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('d_est par pipeline')\n",
    "plt.tight_layout()\n",
    "cmp_png = os.path.join(RESULTS_DIR, 'pipelines_comparative_overview.png')\n",
    "plt.savefig(cmp_png, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Build comparative markdown report\n",
    "report_md = os.path.join(RESULTS_DIR, 'pipelines_comparative_report.md')\n",
    "with open(report_md, 'w', encoding='utf-8') as f:\n",
    "    f.write('# Pipelines comparative validation report\\n\\n')\n",
    "    f.write(f'Generated: {datetime.utcnow().isoformat()}Z\\n\\n')\n",
    "    f.write('## Overview\\n\\n')\n",
    "    f.write(overview_df.to_markdown(index=False))\n",
    "    f.write('\\n\\n## Details per pipeline (files)\\n\\n')\n",
    "    for row in overview_df['pipeline']:\n",
    "        f.write(f'- Pipeline {row} :\\n')\n",
    "        f.write(f'  - d estimate CSV: {os.path.join(out_details_dir, f\"{row}_d_estimate.csv\")}\\n')\n",
    "        f.write(f'  - LOO CSV: {os.path.join(out_details_dir, f\"{row}_loo.csv\")}\\n')\n",
    "        f.write(f'  - LOO summary: {os.path.join(out_details_dir, f\"{row}_loo_summary.csv\")}\\n')\n",
    "        f.write(f'  - Sweep CSV: {os.path.join(out_details_dir, f\"{row}_sweep.csv\")}\\n')\n",
    "        f.write(f'  - Sweep summary: {os.path.join(out_details_dir, f\"{row}_sweep_summary.csv\")}\\n')\n",
    "        f.write(f'  - Plots: {os.path.join(out_details_dir, f\"{row}_loo_hist.png\")}, {os.path.join(out_details_dir, f\"{row}_sweep_heatmap.png\")}\\n')\n",
    "        f.write('\\n')\n",
    "    f.write(f'## Comparative images\\n\\n- {cmp_png}\\n\\n')\n",
    "    f.write('## Notes\\n\\n- Toutes les décisions et paramètres sont consignés dans results/params.json si présent.\\n- Voir logs/logs.csv et logs/summary.md pour historique d\\'exécution.\\n')\n",
    "\n",
    "append_log('INFO', f'Comparative validation finished; overview saved to {out_overview}; report: {report_md}; comparative image: {cmp_png}')\n",
    "print('Comparative validation finished.')\n",
    "print('Overview saved to:', out_overview)\n",
    "print('Comparative report:', report_md)\n",
    "print('Comparative image:', cmp_png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c93bce1",
   "metadata": {
    "id": "NC34dp5sYffG"
   },
   "source": [
    "### Résumé comparatif final\n",
    "\n",
    "- Trois pipelines exécutés et tracés de façon reproductible :  \n",
    "  - A — Agrégation moyenne + StandardScaler (verrouillé) : T_log = -0.298824, d = 3.90025.  \n",
    "  - B — Winsor 1%/99% + StandardScaler : T_log = -0.317962, d = 3.89386.  \n",
    "  - C — MinCovDet projection (robuste multivarié) : T_log = -0.298824, d = 3.90025.  \n",
    "- Robustesse LOO pour A et C : mean T_log ≈ -0.3530, std ≈ 0.0860, rel_std ≈ 24.36% (non conforme au critère <10%).  \n",
    "- Sweep (sous-échantillon + perturbation d ±20%) : probabilité non négligeable de changement de régime (unstable_frac ≈ 0.15–0.44 selon fraction).  \n",
    "- Feature checks : humidity_percent, wind_speed_ms et urban_heat_island_intensity portent l’essentiel du signal ; urban_heat_island_intensity est très discrète et nécessite documentation d’origine.  \n",
    "- Conclusion pratique : toutes les variantes reproductibles donnent le même signe (T_log < 0). L’évidence empirique que T_log indique un régime « Divergence » est cohérente mais sensible ; la variabilité inter-run reste trop élevée pour une affirmation catégorique sans conditions additionnelles (fixation explicite des choix et transparence des prétraitements).\n",
    "\n",
    "### Fichiers essentiels produits (à joindre à toute publication ou rapport)\n",
    "- results/feature_quality_report.md  \n",
    "- results/feature_quality_stats.csv  \n",
    "- results/feature_outliers_detected.csv  \n",
    "- results/tlog_d_estimates_locked.csv  \n",
    "- results/tlog_d_estimates_sanitized.csv  \n",
    "- results/tlog_robust_comparison.csv  \n",
    "- results/tlog_pipelines_overview.csv  \n",
    "- results/pipelines_comparative_report.md  \n",
    "- results/pipeline_details/* (LOO, sweep, plots par pipeline)  \n",
    "- logs/logs.csv et logs/summary.md\n",
    "\n",
    "### Recommandation unique et prioritaire (action immédiate)\n",
    "Publier côte‑à‑côte les trois pipelines (A, B, C) avec : 1) params.json et README_method.md, 2) feature_quality_report.md, 3) pipelines_comparative_report.md et figures comparatives. Présenter la conclusion « T_log < 0 » en soulignant la sensibilité documentée et en fournissant les jeux de résultats pour audit externe.\n",
    "\n",
    "### Option suivante que j’exécute maintenant\n",
    "\n",
    "1. Générer un rapport Jupyter/Markdown final prêt à soumettre (figures intégrées, tables, logs).  \n",
    "2. Calculer et sauvegarder versions chiffrées prêtes pour revue (ZIP non fourni ici, mais liste et fichiers prêts).  \n",
    "3. Lancer une cross‑validation temporelle (rolling windows) et produire diagnostics temporels pour vérifier dérives instrumentales.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e85a8d",
   "metadata": {
    "id": "ItRiQnHJYVp3"
   },
   "source": [
    "### Recommandation principale\n",
    "\n",
    "Exécuter en priorité l’option 3 — Cross‑validation temporelle (rolling windows) et diagnostics temporels.\n",
    "\n",
    "### Pourquoi c’est le meilleur choix maintenant\n",
    "- Vérifie la stabilité temporelle des signaux qui déterminent d et T_log, ce qui est crucial pour une preuve empirique honnête.  \n",
    "- Détecte dérives d’instrumentation, ruptures de coverage ou périodes où la conclusion bascule, informations indispensables pour justifier ou réviser toute conclusion.  \n",
    "- Renforce la crédibilité : si T_log < 0 tient sur fenêtres temporelles indépendantes et raisonnablement larges, votre résultat devient beaucoup plus robuste et difficile à contester.\n",
    "\n",
    "### Plan d’action concret que j’exécuterai\n",
    "1. Définir fenêtres temporelles mobiles et empilées  \n",
    "   - fenêtres annuelles et fenêtres mobiles 3‑ans avec pas mensuel/annuel (paramétrables).  \n",
    "2. Pour chaque fenêtre :  \n",
    "   - agréger par ville selon la méthode verrouillée ;  \n",
    "   - appliquer les trois pipelines A/B/C ;  \n",
    "   - calculer d_part, d_pca90, d_est, T_log.  \n",
    "3. Produire diagnostics par fenêtre :  \n",
    "   - série temporelle de T_log par pipeline ;  \n",
    "   - fraction de fenêtres avec changement de régime ;  \n",
    "   - tests statistique par période (t-test sur LOO si pertinent).  \n",
    "4. Visualisations et sorties reproductibles :  \n",
    "   - plots T_log(t) avec bande d’incertitude (LOO std) ;  \n",
    "   - heatmap de stabilité (fenêtre × pipeline : régime) ;  \n",
    "   - CSVs, PNGs, et rapport Markdown décrivant paramètres et résultats.  \n",
    "5. Critères d’acceptation provisoires :  \n",
    "   - T_log garde le même signe dans ≥ 80% des fenêtres larges (3‑ans) pour toutes pipelines A/C ;  \n",
    "   - relative variability LOO médiane < 15% sur fenêtres stables ;  \n",
    "   - toute fenêtre incohérente documentée et expliquée (coverage, outliers).\n",
    "\n",
    "### Résultat attendu et bénéfice immédiat\n",
    "- Résultat clair sur la robustesse temporelle de T_log, avec traces auditables (plots, CSVs, params) à joindre à toute publication ou revue.  \n",
    "- Si la stabilité est confirmée, vous aurez la meilleure preuve empirique possible avant publication ; si elle n’est pas confirmée, vous aurez les diagnostics nécessaires pour expliquer et corriger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349629df",
   "metadata": {
    "id": "izjmHZAkYwBM"
   },
   "source": [
    "Cross-validation temporelle rolling windows"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMj8BTKhAxEVfHfc0L9NwkV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
