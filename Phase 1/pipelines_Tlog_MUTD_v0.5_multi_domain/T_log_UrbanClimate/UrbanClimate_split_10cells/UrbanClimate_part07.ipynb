{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2744c4ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18698,
     "status": "ok",
     "timestamp": 1761436480843,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "2e6H0BZHyyR3",
    "outputId": "74306f12-9246-406d-8d25-cbec2c3f3f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOO influence results written to: results/temporal_cv_atypical_windows/diagnostics/loo_influence\\loo_influence_top5_per_window.csv\n",
      "Per-window outputs under: results/temporal_cv_atypical_windows/diagnostics/loo_influence\n"
     ]
    }
   ],
   "source": [
    "# LOO étendue : classement d'influence des villes sur T_log pour fenêtres prioritaires\n",
    "import os, re, glob, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Config\n",
    "PRIO_SUM = 'results/temporal_cv_atypical_windows/diagnostics/tests_priority_summary.csv'\n",
    "IDX_ENR = 'results/temporal_cv_atypical_windows/temporal_cv_atypical_periods_windows_index_enriched.csv'\n",
    "OUT_DIR = 'results/temporal_cv_atypical_windows/diagnostics/loo_influence'\n",
    "TOPN_WINDOWS = None   # None = traiter toutes les lignes de PRIO_SUM; sinon int\n",
    "FEATURES_DEFAULT = ['temperature_celsius','humidity_percent','precipitation_mm','wind_speed_ms','urban_heat_island_intensity']\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def safe_tag(s): return re.sub(r'[^0-9A-Za-z_.-]', '_', str(s))\n",
    "\n",
    "def compute_d_estimate_from_X(X):\n",
    "    if X.shape[0] < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    cov = np.cov(X, rowvar=False)\n",
    "    eigvals = np.linalg.eigvalsh(cov)\n",
    "    eigvals = np.maximum(eigvals, 0.0)\n",
    "    sum_eig = np.sum(eigvals)\n",
    "    d_part = 0.0 if sum_eig <= 0 else (sum_eig**2) / np.sum(eigvals**2)\n",
    "    pca = PCA(n_components=min(X.shape[0], X.shape[1])).fit(X)\n",
    "    cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d_pca90 = int(np.searchsorted(cumvar, 0.90) + 1) if cumvar[-1] >= 0.90 else pca.n_components_\n",
    "    d_est = float((d_part + d_pca90) / 2.0)\n",
    "    return d_part, d_pca90, d_est\n",
    "\n",
    "def compute_T_log(d_est, n_eff):\n",
    "    n_eff = max(2, int(n_eff))\n",
    "    return (d_est - 4.0) * np.log(n_eff)\n",
    "\n",
    "# Load priorities\n",
    "if not os.path.exists(PRIO_SUM):\n",
    "    raise FileNotFoundError(PRIO_SUM)\n",
    "prio = pd.read_csv(PRIO_SUM)\n",
    "if TOPN_WINDOWS:\n",
    "    prio = prio.sort_values(prio.columns[0]).head(TOPN_WINDOWS)\n",
    "\n",
    "influence_rows = []\n",
    "for _, prow in prio.iterrows():\n",
    "    tag = prow['tag']\n",
    "    window_type = prow.get('window_type')\n",
    "    window_center = prow.get('window_center')\n",
    "    pipeline = prow.get('pipeline')\n",
    "    # locate raw file (try index enriched then heuristics)\n",
    "    raw_fp = None\n",
    "    if os.path.exists(IDX_ENR):\n",
    "        idx = pd.read_csv(IDX_ENR)\n",
    "        # match best-effort (handle numeric string)\n",
    "        cond = (idx['window_type']==window_type) & (idx['pipeline']==pipeline)\n",
    "        try:\n",
    "            wcf = float(window_center)\n",
    "            cond = cond & (idx['window_center'].astype(float)==wcf)\n",
    "        except Exception:\n",
    "            cond = cond & (idx['window_center'].astype(str)==str(window_center))\n",
    "        match = idx[cond]\n",
    "        if not match.empty and 'raw_csv' in match.columns:\n",
    "            raw_fp = match.iloc[0]['raw_csv']\n",
    "    if raw_fp is None:\n",
    "        candidates = glob.glob(os.path.join('results','temporal_cv_atypical_windows',f'*{tag}*raw.csv'))\n",
    "        raw_fp = candidates[0] if candidates else None\n",
    "    if not raw_fp or not os.path.exists(raw_fp):\n",
    "        print(f\"SKIP no raw for {tag}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(raw_fp)\n",
    "    if 'city_key' not in df.columns:\n",
    "        agg_cols = [c for c in ['city','country','latitude','longitude'] if c in df.columns]\n",
    "        df['city_key'] = df[agg_cols].astype(str).agg('_'.join, axis=1) if agg_cols else 'unknown'\n",
    "\n",
    "    # select features\n",
    "    num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in ['year','month','n_rows']]\n",
    "    features = [c for c in FEATURES_DEFAULT if c in num_cols] or (num_cols[:min(5,len(num_cols))] if num_cols else [])\n",
    "    if not features:\n",
    "        print(f\"SKIP no numeric features for {tag}\")\n",
    "        continue\n",
    "\n",
    "    # per-city aggregate (mean)\n",
    "    city_df = df.groupby('city_key')[features].mean().reset_index()\n",
    "    city_df_clean = city_df.dropna(subset=features).reset_index(drop=True)\n",
    "    if city_df_clean.shape[0] < 2:\n",
    "        print(f\"SKIP not enough cities for {tag}\")\n",
    "        continue\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(city_df_clean[features].astype(float).values)\n",
    "    d_part, d_pca90, d_est = compute_d_estimate_from_X(X)\n",
    "    T_full = compute_T_log(d_est, X.shape[0])\n",
    "\n",
    "    # LOO over cities\n",
    "    loo_records = []\n",
    "    for i in range(X.shape[0]):\n",
    "        city_left = city_df_clean.loc[i,'city_key']\n",
    "        X_loo = np.delete(X, i, axis=0)\n",
    "        if X_loo.shape[0] < 2:\n",
    "            continue\n",
    "        d_part_loo, d_pca90_loo, d_est_loo = compute_d_estimate_from_X(X_loo)\n",
    "        T_loo = compute_T_log(d_est_loo, X_loo.shape[0])\n",
    "        delta = T_loo - T_full\n",
    "        loo_records.append({'city': city_left, 'T_loo': float(T_loo), 'delta': float(delta),\n",
    "                            'd_part_loo': float(d_part_loo), 'd_pca90_loo': int(d_pca90_loo),'d_est_loo': float(d_est_loo)})\n",
    "    if not loo_records:\n",
    "        continue\n",
    "    loo_df = pd.DataFrame(loo_records).sort_values('delta')  # negative delta means removing city reduces T_log\n",
    "    # rank by absolute influence\n",
    "    loo_df['delta_abs'] = loo_df['delta'].abs()\n",
    "    loo_df = loo_df.sort_values('delta_abs', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # record top influencers\n",
    "    top5 = loo_df.head(5)\n",
    "    for rank, r in top5.iterrows():\n",
    "        influence_rows.append({\n",
    "            'tag': tag, 'window_type': window_type, 'window_center': window_center, 'pipeline': pipeline,\n",
    "            'raw_csv': raw_fp, 'n_cities': X.shape[0], 'feature_list': ';'.join(features),\n",
    "            'influencer_rank': rank+1, 'city': r['city'], 'delta_T_log': r['delta'], 'delta_abs': r['delta_abs'],\n",
    "            'T_full': float(T_full), 'T_without_city': r['T_loo'],\n",
    "            'd_est_full': float(d_est), 'd_est_without_city': r['d_est_loo']\n",
    "        })\n",
    "\n",
    "    # save LOO full table and plot per-window\n",
    "    window_out = os.path.join(OUT_DIR, safe_tag(tag))\n",
    "    os.makedirs(window_out, exist_ok=True)\n",
    "    loo_df.to_csv(os.path.join(window_out, f'{safe_tag(tag)}_loo_by_city.csv'), index=False)\n",
    "\n",
    "    # plot LOO distribution\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(loo_df['T_loo'], bins=max(8,min(20,loo_df.shape[0])), color='C0', alpha=0.8)\n",
    "    plt.axvline(T_full, color='red', linestyle='--', label='T_full')\n",
    "    plt.xlabel('T_log (leave-one-out)')\n",
    "    plt.ylabel('count')\n",
    "    plt.title(f'LOO T_log distribution - {tag}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(window_out, f'{safe_tag(tag)}_loo_hist.png'), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # bar plot top deltas\n",
    "    plt.figure(figsize=(6,3))\n",
    "    top_plot = loo_df.sort_values('delta_abs', ascending=False).head(10)\n",
    "    plt.barh(top_plot['city'].astype(str), top_plot['delta'], color='C1')\n",
    "    plt.xlabel('delta T_log (T_without_city - T_full)')\n",
    "    plt.title(f'Top city influence (abs delta) - {tag}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(window_out, f'{safe_tag(tag)}_top_influence.png'), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# write global ranking CSV\n",
    "out_csv = os.path.join(OUT_DIR, 'loo_influence_top5_per_window.csv')\n",
    "pd.DataFrame(influence_rows).to_csv(out_csv, index=False)\n",
    "print(\"LOO influence results written to:\", out_csv)\n",
    "print(\"Per-window outputs under:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120544b",
   "metadata": {
    "id": "DGcuH8x90go-"
   },
   "source": [
    "### Résumé rapide des influences LOO\n",
    "\n",
    "- Un petit groupe de villes réapparaît comme influenceur majeur : **Delhi, Beijing, Chicago, San Jose, Mexico City, Lagos, Dallas, New York, Phoenix, Mumbai**.  \n",
    "- Pattern d’influence : Delhi et Beijing produisent les plus grands changements absolus de T_log en LOO ; Chicago et San Jose sont souvent 2–3 ; Mexico City et Lagos apparaissent comme influenceurs positifs sur plusieurs fenêtres.  \n",
    "- Magnitudes observées : |delta_T_log| pour les top influencers varie ≈ 0.09–0.39, les plus fréquentes étant dans la plage 0.10–0.30.\n",
    "\n",
    "---\n",
    "\n",
    "### Interprétation synthétique\n",
    "\n",
    "- Quelques villes concentrent la sensibilité du diagnostic global d / T_log, ce qui remet en question la robustesse du diagnostic si ces villes ne sont pas traitées explicitement.  \n",
    "- Les features qui ressortent fréquemment comme sources d’outliers sont **precipitation_mm** et **urban_heat_island_intensity**.  \n",
    "- L’influence est directionnelle : certaines villes tirent T_log vers le haut (delta positif), d’autres vers le bas (delta négatif), indiquant des distributions locales systématiquement différentes.\n",
    "\n",
    "---\n",
    "\n",
    "### Vérifications immédiates recommandées\n",
    "\n",
    "1. Calculer pour les villes récurrentes (Delhi, Beijing, Chicago, San Jose, Mexico City, Lagos) : Mahalanobis robuste (MCD) et Z‑scores par feature.  \n",
    "2. Tracer séries temporelles des features coupables (precipitation_mm, urban_heat_island_intensity, temperature_celsius, humidity_percent, wind_speed_ms) pour détecter si l’influence vient de quelques points extrêmes ou d’un décalage soutenu.  \n",
    "3. Appliquer localement (par ville) des transformations alternatives et comparer : winsorisation (p.ex. 1–99e centile), log1p(precipitation_mm), clipping UHI ; mesurer l’impact sur T_log.  \n",
    "4. Vérifier la dépendance à la pipeline : répéter LOO sous la pipeline cible (A, B ou C) pour confirmer que les influencers persistent.\n",
    "\n",
    "---\n",
    "\n",
    "### Stratégies d’atténuation classées\n",
    "\n",
    "1. Pré‑traitement localisé par ville : winsoriser ou transformer la/les features responsables pour réduire l’effet de levier sans exclure la ville.  \n",
    "2. Agrégation robuste / quorum : estimer T_log via estimateurs robustes (trimmed mean, median-of-estimates, bootstrap consensus) et exiger accord entre estimateurs avant décision opérationnelle.  \n",
    "3. Politique flag‑and‑review : flag automatique des fenêtres où |delta_T_log| > seuil (p.ex. 0.10) avec paquet diagnostic (timeseries + per‑feature outlier summary) pour relecture humaine.  \n",
    "4. Exclusion conditionnelle : n’exclure qu’après règles diagnostiques (Mahalanobis extrême + transform‑insensible) confirmant que l’influence est un artefact, pas une représentativité légitime.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486e31a",
   "metadata": {
    "id": "4SCKWzdF3Zky"
   },
   "source": [
    "Cellule (a) — tableau résumé des 5 villes (signal brut et distribution)\n",
    "\n",
    "Résumé rapide de l'objectif de la cellule\n",
    "\n",
    "Charge le CSV brut de la fenêtre atypique sélectionnée.\n",
    "\n",
    "Filtre les observations des 5 villes proposées et affiche le signal brut (toutes observations conservées).\n",
    "\n",
    "Calcule et affiche un tableau résumé (count, mean, std, median, min, max, skew) pour chaque feature et chaque ville.\n",
    "\n",
    "Sauvegarde le tableau résumé et le long format des signaux bruts pour envoi / revue externe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2420e58b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1761438186073,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "YhowJWOI5MBB",
    "outputId": "15302d42-80b5-4fdc-bcd0-9fe08286bafd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu du signal brut pour les 5 villes (10 premières lignes) :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>temperature_celsius</th>\n",
       "      <th>humidity_percent</th>\n",
       "      <th>precipitation_mm</th>\n",
       "      <th>wind_speed_ms</th>\n",
       "      <th>urban_heat_island_intensity</th>\n",
       "      <th>data_source</th>\n",
       "      <th>ym</th>\n",
       "      <th>city_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [city, country, latitude, longitude, year, month, temperature_celsius, humidity_percent, precipitation_mm, wind_speed_ms, urban_heat_island_intensity, data_source, ym, city_key]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tableau résumé par ville (statistiques descriptives, distribution brute conservée) :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>temperature_celsius_count</th>\n",
       "      <th>temperature_celsius_mean</th>\n",
       "      <th>temperature_celsius_std</th>\n",
       "      <th>temperature_celsius_median</th>\n",
       "      <th>temperature_celsius_min</th>\n",
       "      <th>temperature_celsius_max</th>\n",
       "      <th>temperature_celsius_skew</th>\n",
       "      <th>humidity_percent_count</th>\n",
       "      <th>humidity_percent_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed_ms_min</th>\n",
       "      <th>wind_speed_ms_max</th>\n",
       "      <th>wind_speed_ms_skew</th>\n",
       "      <th>urban_heat_island_intensity_count</th>\n",
       "      <th>urban_heat_island_intensity_mean</th>\n",
       "      <th>urban_heat_island_intensity_std</th>\n",
       "      <th>urban_heat_island_intensity_median</th>\n",
       "      <th>urban_heat_island_intensity_min</th>\n",
       "      <th>urban_heat_island_intensity_max</th>\n",
       "      <th>urban_heat_island_intensity_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [city, temperature_celsius_count, temperature_celsius_mean, temperature_celsius_std, temperature_celsius_median, temperature_celsius_min, temperature_celsius_max, temperature_celsius_skew, humidity_percent_count, humidity_percent_mean, humidity_percent_std, humidity_percent_median, humidity_percent_min, humidity_percent_max, humidity_percent_skew, precipitation_mm_count, precipitation_mm_mean, precipitation_mm_std, precipitation_mm_median, precipitation_mm_min, precipitation_mm_max, precipitation_mm_skew, wind_speed_ms_count, wind_speed_ms_mean, wind_speed_ms_std, wind_speed_ms_median, wind_speed_ms_min, wind_speed_ms_max, wind_speed_ms_skew, urban_heat_island_intensity_count, urban_heat_island_intensity_mean, urban_heat_island_intensity_std, urban_heat_island_intensity_median, urban_heat_island_intensity_min, urban_heat_island_intensity_max, urban_heat_island_intensity_skew]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fichiers sauvegardés :\n",
      " - results/top5_cities_raw_distribution_summary.csv\n",
      " - results/top5_cities_raw_signals_long.csv\n"
     ]
    }
   ],
   "source": [
    "# Cellule (a) — afficher le signal brut et un tableau résumé pour 5 villes\n",
    "# Modifier `RAW_CSV` et `TOP5_CITIES` si nécessaire.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin CSV de la fenêtre atypique que vous voulez inspecter\n",
    "RAW_CSV = \"results/temporal_cv_atypical_windows/temporal_cv_atypical_period_window_3yr_1984.00_A_raw.csv\"\n",
    "\n",
    "# Liste des 5 villes proposées (format attendu dans la colonne 'city' du CSV)\n",
    "TOP5_CITIES = [\n",
    "    \"Delhi_India_28.7041_77.1025\",\n",
    "    \"Chicago_USA_41.8781_-87.6298\",\n",
    "    \"San Jose_USA_37.3382_-121.8863\",\n",
    "    \"Mumbai_India_19.076_72.8777\",\n",
    "    \"Beijing_China_39.9042_116.4074\"\n",
    "]\n",
    "\n",
    "# Colonnes de signal (conserver la distribution brute)\n",
    "FEATURE_COLS = [\n",
    "    \"temperature_celsius\",\n",
    "    \"humidity_percent\",\n",
    "    \"precipitation_mm\",\n",
    "    \"wind_speed_ms\",\n",
    "    \"urban_heat_island_intensity\"\n",
    "]\n",
    "\n",
    "# Chargement et filtrage\n",
    "df = pd.read_csv(RAW_CSV)\n",
    "df_top5 = df[df[\"city\"].isin(TOP5_CITIES)].copy()\n",
    "\n",
    "# Afficher quelques lignes brutes (signal brut conservé)\n",
    "print(\"Aperçu du signal brut pour les 5 villes (10 premières lignes) :\")\n",
    "display(df_top5.head(10))\n",
    "\n",
    "# Tableau résumé par ville (statistiques descriptives sur la distribution brute)\n",
    "summary_by_city = (\n",
    "    df_top5\n",
    "    .groupby(\"city\")[FEATURE_COLS]\n",
    "    .agg([\n",
    "        (\"count\", \"count\"),\n",
    "        (\"mean\", \"mean\"),\n",
    "        (\"std\", \"std\"),\n",
    "        (\"median\", \"median\"),\n",
    "        (\"min\", \"min\"),\n",
    "        (\"max\", \"max\"),\n",
    "        (\"skew\", pd.Series.skew)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Normaliser la mise en forme des colonnes (une ligne par statistique pour lisibilité)\n",
    "summary_by_city.columns = [\"_\".join(col).strip() for col in summary_by_city.columns.values]\n",
    "summary_by_city = summary_by_city.reset_index()\n",
    "\n",
    "print(\"\\nTableau résumé par ville (statistiques descriptives, distribution brute conservée) :\")\n",
    "display(summary_by_city)\n",
    "\n",
    "# Optionnel : sauvegarder les sorties pour review externe\n",
    "summary_by_city.to_csv(\"results/top5_cities_raw_distribution_summary.csv\", index=False)\n",
    "df_top5.to_csv(\"results/top5_cities_raw_signals_long.csv\", index=False)\n",
    "\n",
    "print(\"\\nFichiers sauvegardés :\")\n",
    "print(\" - results/top5_cities_raw_distribution_summary.csv\")\n",
    "print(\" - results/top5_cities_raw_signals_long.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65cd2a8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1761438475871,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "nj57yfNK6cWI",
    "outputId": "bda584f4-85c2-4021-b2a6-5f5a4c7c7587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 villes (résumé LOO) — sauvegardé sous : results/top5_loo_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>n_windows</th>\n",
       "      <th>prop_windows_pct</th>\n",
       "      <th>mean_delta_abs</th>\n",
       "      <th>median_delta_abs</th>\n",
       "      <th>max_delta_abs</th>\n",
       "      <th>mean_delta_signed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York_USA_40.7128_-74.006</td>\n",
       "      <td>6</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.354811</td>\n",
       "      <td>0.354811</td>\n",
       "      <td>0.367811</td>\n",
       "      <td>-0.354811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tokyo_Japan_35.6762_139.6503</td>\n",
       "      <td>3</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.310510</td>\n",
       "      <td>0.310510</td>\n",
       "      <td>0.310510</td>\n",
       "      <td>-0.310510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi_India_28.7041_77.1025</td>\n",
       "      <td>14</td>\n",
       "      <td>233.333333</td>\n",
       "      <td>0.281196</td>\n",
       "      <td>0.303564</td>\n",
       "      <td>0.390403</td>\n",
       "      <td>-0.281196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dallas_USA_32.7767_-96.797</td>\n",
       "      <td>7</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>0.238337</td>\n",
       "      <td>0.246162</td>\n",
       "      <td>0.276450</td>\n",
       "      <td>-0.238337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicago_USA_41.8781_-87.6298</td>\n",
       "      <td>11</td>\n",
       "      <td>183.333333</td>\n",
       "      <td>0.227382</td>\n",
       "      <td>0.205147</td>\n",
       "      <td>0.412940</td>\n",
       "      <td>-0.227382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           city  n_windows  prop_windows_pct  mean_delta_abs  \\\n",
       "0  New York_USA_40.7128_-74.006          6        100.000000        0.354811   \n",
       "1  Tokyo_Japan_35.6762_139.6503          3         50.000000        0.310510   \n",
       "2   Delhi_India_28.7041_77.1025         14        233.333333        0.281196   \n",
       "3    Dallas_USA_32.7767_-96.797          7        116.666667        0.238337   \n",
       "4  Chicago_USA_41.8781_-87.6298         11        183.333333        0.227382   \n",
       "\n",
       "   median_delta_abs  max_delta_abs  mean_delta_signed  \n",
       "0          0.354811       0.367811          -0.354811  \n",
       "1          0.310510       0.310510          -0.310510  \n",
       "2          0.303564       0.390403          -0.281196  \n",
       "3          0.246162       0.276450          -0.238337  \n",
       "4          0.205147       0.412940          -0.227382  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Générer le tableau résumé top‑5 à partir du CSV LOO existant (chemin corrigé)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin corrigé vers le CSV LOO trouvé dans le notebook\n",
    "LOO_TOP5_CSV = \"results/temporal_cv_atypical_windows/diagnostics/loo_influence/loo_influence_top5_per_window.csv\"\n",
    "OUT_SUMMARY = \"results/top5_loo_summary.csv\"\n",
    "\n",
    "if not os.path.exists(LOO_TOP5_CSV):\n",
    "    raise FileNotFoundError(f\"Fichier LOO introuvable: {LOO_TOP5_CSV}\")\n",
    "\n",
    "loo = pd.read_csv(LOO_TOP5_CSV)\n",
    "\n",
    "# Agréger par ville et calculer métriques d'influence\n",
    "group = loo.groupby(\"city\")\n",
    "summary = group.agg(\n",
    "    n_windows=(\"city\", \"count\"),\n",
    "    mean_delta_abs=(\"delta_abs\", \"mean\"),\n",
    "    median_delta_abs=(\"delta_abs\", \"median\"),\n",
    "    max_delta_abs=(\"delta_abs\", \"max\"),\n",
    "    mean_delta_signed=(\"delta_T_log\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Proportion de fenêtres couvertes (en %)\n",
    "n_windows_total = loo[\"window_center\"].astype(str).nunique()\n",
    "summary[\"prop_windows_pct\"] = (summary[\"n_windows\"] / max(1, n_windows_total)) * 100\n",
    "\n",
    "# Trier et prendre top 5 par impact moyen absolu\n",
    "summary = summary.sort_values(\"mean_delta_abs\", ascending=False).reset_index(drop=True)\n",
    "top5_summary = summary.head(5).copy()\n",
    "\n",
    "# Colonnes finales lisibles\n",
    "top5_summary = top5_summary[[\n",
    "    \"city\",\n",
    "    \"n_windows\",\n",
    "    \"prop_windows_pct\",\n",
    "    \"mean_delta_abs\",\n",
    "    \"median_delta_abs\",\n",
    "    \"max_delta_abs\",\n",
    "    \"mean_delta_signed\"\n",
    "]]\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_SUMMARY), exist_ok=True)\n",
    "top5_summary.to_csv(OUT_SUMMARY, index=False)\n",
    "\n",
    "print(\"Top 5 villes (résumé LOO) — sauvegardé sous :\", OUT_SUMMARY)\n",
    "display(top5_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce658f6c",
   "metadata": {
    "id": "6hVu1m1J6zx-"
   },
   "source": [
    "### Résumé concis des résultats top‑5 LOO (fichier top5_loo_summary.csv)\n",
    "\n",
    "- **Classement** : New York, Tokyo, Delhi, Dallas, Chicago sont les 5 villes avec le plus fort impact moyen absolu sur T_log.  \n",
    "- **Amplitude** : New York affiche le plus fort impact moyen |delta| ≈ **0.355** (max 0.368), Tokyo ≈ **0.311**, Delhi ≈ **0.281**, Dallas ≈ **0.238**, Chicago ≈ **0.227**.  \n",
    "- **Fréquence d’apparition** : Delhi et Chicago apparaissent dans beaucoup de fenêtres (Delhi 14 apparitions, Chicago 11), New York apparaît sur toutes les fenêtres considérées ici (n_windows = 6, prop = 100%).  \n",
    "- **Direction moyenne** : toutes ces villes ont une mean_delta_signed négative, donc leur suppression fait baisser T_log (elles tirent T_log vers le haut quand elles sont incluses).  \n",
    "- **Interprétation rapide** : ces villes exercent un levier substantiel sur la classification de régime; leur signal semble systématique (répété sur plusieurs fenêtres) plutôt que ponctuel pour au moins Delhi et Chicago.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e6402",
   "metadata": {
    "id": "rl70IKve7Lkg"
   },
   "source": [
    "Cell prête à exécution — diagnostics bruts pour le top‑5 (sauvegarde dans results/diagnostics_top5/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4755916",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15607,
     "status": "ok",
     "timestamp": 1761438678263,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "0ThBTd_H7Pat",
    "outputId": "625d6e51-308d-4cbd-df91-7f9d939a2684"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1730982664.py:179: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  f.write(f\"Diagnostics generated: {datetime.utcnow().isoformat()}Z\\n\")\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1730982664.py:179: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  f.write(f\"Diagnostics generated: {datetime.utcnow().isoformat()}Z\\n\")\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1730982664.py:179: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  f.write(f\"Diagnostics generated: {datetime.utcnow().isoformat()}Z\\n\")\n",
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1730982664.py:179: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  f.write(f\"Diagnostics generated: {datetime.utcnow().isoformat()}Z\\n\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostics générés pour top5 — dossier : results\\diagnostics_top5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackd\\AppData\\Local\\Temp\\ipykernel_5596\\1730982664.py:179: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  f.write(f\"Diagnostics generated: {datetime.utcnow().isoformat()}Z\\n\")\n"
     ]
    }
   ],
   "source": [
    "# Générer diagnostics bruts pour les 5 villes du top5_loo_summary.csv\n",
    "# Sortie: results/diagnostics_top5/{city_key}/ :\n",
    "#   - summary_{city_key}.csv (mini résumé LOO + stats feature)\n",
    "#   - timeseries_{city_key}.png (séries temporelles des features)\n",
    "#   - signals_{city_key}.csv (observations brutes conservées)\n",
    "#   - robust_mahalanobis_{city_key}.csv (Mahalanobis robuste par observation)\n",
    "#   - zscores_{city_key}.csv (Z-scores par feature)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.covariance import MinCovDet\n",
    "from datetime import datetime\n",
    "\n",
    "# Config chemins\n",
    "TOP5_CSV = \"results/top5_loo_summary.csv\"\n",
    "DATA_CSV = \"data/urban_climate.csv\"\n",
    "OUT_DIR = Path(\"results/diagnostics_top5\")\n",
    "FEATURE_COLS = [\n",
    "    \"temperature_celsius\",\n",
    "    \"humidity_percent\",\n",
    "    \"precipitation_mm\",\n",
    "    \"wind_speed_ms\",\n",
    "    \"urban_heat_island_intensity\"\n",
    "]\n",
    "\n",
    "# Vérifications rapides\n",
    "for p in (TOP5_CSV, DATA_CSV):\n",
    "    if not Path(p).exists():\n",
    "        raise FileNotFoundError(f\"Fichier introuvable: {p}\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Charger top5 et données brutes\n",
    "top5 = pd.read_csv(TOP5_CSV)\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "# Normaliser colonne city_key si besoin\n",
    "if \"city_key\" not in df.columns:\n",
    "    df[\"city_key\"] = df[[\"city\", \"country\", \"latitude\", \"longitude\"]].astype(str).agg(\"_\".join, axis=1)\n",
    "\n",
    "# Extraire liste de villes (city strings) depuis top5\n",
    "cities = top5[\"city\"].astype(str).tolist()\n",
    "\n",
    "# Fonctions utilitaires\n",
    "def robust_mahalanobis(X):\n",
    "    # X: numpy array (n x p) without NaNs\n",
    "    mcd = MinCovDet().fit(X)\n",
    "    md = mcd.mahalanobis(X)  # squared Mahalanobis distances\n",
    "    return md, mcd.location_, mcd.covariance_\n",
    "\n",
    "def safe_zscores(X):\n",
    "    mu = np.nanmedian(X, axis=0)\n",
    "    mad = np.nanmedian(np.abs(X - mu), axis=0)\n",
    "    # convert MAD to approx std: 1.4826*mad (robust)\n",
    "    robust_std = 1.4826 * mad\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        z = (X - mu) / robust_std\n",
    "    return z, mu, robust_std\n",
    "\n",
    "# Pour chaque ville : filtrer, calculer, sauvegarder et tracer\n",
    "for city in cities:\n",
    "    city_dir = OUT_DIR / city\n",
    "    os.makedirs(city_dir, exist_ok=True)\n",
    "\n",
    "    # Filtrer observations brutes pour la ville (toutes obs conservées)\n",
    "    city_df = df[df[\"city_key\"].astype(str) == city].copy()\n",
    "    # If no rows, still create placeholders\n",
    "    if city_df.shape[0] == 0:\n",
    "        # create empty placeholder files so reviewer sees missing data\n",
    "        pd.DataFrame().to_csv(city_dir / f\"signals_{city}.csv\", index=False)\n",
    "        pd.DataFrame().to_csv(city_dir / f\"summary_{city}.csv\", index=False)\n",
    "        continue\n",
    "\n",
    "    # Keep original columns of interest and temporal ordering\n",
    "    # create ym if absent\n",
    "    if \"ym\" not in city_df.columns:\n",
    "        city_df[\"ym\"] = city_df.apply(lambda r: f\"{int(r['year']):04d}-{int(r['month']):02d}\" if not np.isnan(r['year']) and not np.isnan(r['month']) else \"\", axis=1)\n",
    "\n",
    "    signals_out = city_dir / f\"signals_{city}.csv\"\n",
    "    city_df.to_csv(signals_out, index=False)\n",
    "\n",
    "    # Per-feature descriptive stats (brute distribution)\n",
    "    stats = city_df[FEATURE_COLS].agg([\"count\", \"mean\", \"std\", \"median\", \"min\", \"max\", \"skew\"]).transpose()\n",
    "    stats = stats.reset_index().rename(columns={\"index\":\"feature\"})\n",
    "    # Add simple LOO summary if available in top5_loo_summary or loo_influence file\n",
    "    loo_row = top5[top5[\"city\"]==city]\n",
    "    if not loo_row.empty:\n",
    "        loo_meta = loo_row.iloc[0].to_dict()\n",
    "    else:\n",
    "        loo_meta = {}\n",
    "\n",
    "    # Robust Mahalanobis: drop rows with NaN in FEATURE_COLS for robust fit\n",
    "    X = city_df[FEATURE_COLS].to_numpy(dtype=float)\n",
    "    valid_mask = ~np.isnan(X).any(axis=1)\n",
    "    md = np.array([])\n",
    "    mcd_loc = None\n",
    "    mcd_cov = None\n",
    "    if valid_mask.sum() >= (len(FEATURE_COLS) + 1):\n",
    "        try:\n",
    "            md, mcd_loc, mcd_cov = robust_mahalanobis(X[valid_mask])\n",
    "        except Exception:\n",
    "            md = np.array([])\n",
    "    # Prepare mahalanobis DF aligned to original rows (NaN where invalid)\n",
    "    md_full = np.full(shape=(city_df.shape[0],), fill_value=np.nan)\n",
    "    if md.size:\n",
    "        md_full[valid_mask] = md\n",
    "    mahal_df = pd.DataFrame({\n",
    "        \"ym\": city_df.get(\"ym\", pd.Series([\"\"]*len(city_df))),\n",
    "        \"mahalanobis_sq\": md_full\n",
    "    })\n",
    "    mahal_df.to_csv(city_dir / f\"robust_mahalanobis_{city}.csv\", index=False)\n",
    "\n",
    "    # Robust Z-scores (median/MAD)\n",
    "    z, med, rstd = safe_zscores(X)\n",
    "    z_full = np.full_like(X, np.nan)\n",
    "    z_full[~np.isnan(X).any(axis=1)] = z[~np.isnan(X).any(axis=1)]\n",
    "    z_df = pd.DataFrame(z_full, columns=FEATURE_COLS)\n",
    "    z_df.insert(0, \"ym\", city_df.get(\"ym\", pd.Series([\"\"]*len(city_df))))\n",
    "    z_df.to_csv(city_dir / f\"zscores_{city}.csv\", index=False)\n",
    "\n",
    "    # Compose summary CSV (combined)\n",
    "    summary_rows = []\n",
    "    # per-feature summary lines\n",
    "    for _, row in stats.iterrows():\n",
    "        summary_rows.append({\n",
    "            \"city\": city,\n",
    "            \"feature\": row[\"feature\"],\n",
    "            \"count\": row[\"count\"],\n",
    "            \"mean\": row[\"mean\"],\n",
    "            \"std\": row[\"std\"],\n",
    "            \"median\": row[\"median\"],\n",
    "            \"min\": row[\"min\"],\n",
    "            \"max\": row[\"max\"],\n",
    "            \"skew\": row[\"skew\"]\n",
    "        })\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    # Add a top-level metadata row (one-line) appended at top\n",
    "    meta = {\n",
    "        \"city\": city,\n",
    "        \"feature\": \"meta\",\n",
    "        \"count\": city_df.shape[0],\n",
    "        \"mean\": loo_meta.get(\"mean_delta_abs\", np.nan),\n",
    "        \"std\": loo_meta.get(\"median_delta_abs\", np.nan),\n",
    "        \"median\": loo_meta.get(\"max_delta_abs\", np.nan),\n",
    "        \"min\": loo_meta.get(\"n_windows\", np.nan),\n",
    "        \"max\": loo_meta.get(\"prop_windows_pct\", np.nan),\n",
    "        \"skew\": loo_meta.get(\"mean_delta_signed\", np.nan)\n",
    "    }\n",
    "    summary_out = city_dir / f\"summary_{city}.csv\"\n",
    "    pd.concat([pd.DataFrame([meta]), summary_df], ignore_index=True).to_csv(summary_out, index=False)\n",
    "\n",
    "    # Timeseries plot (raw values per feature)\n",
    "    fig, ax = plt.subplots(len(FEATURE_COLS), 1, figsize=(10, 2.2 * len(FEATURE_COLS)), sharex=True)\n",
    "    if len(FEATURE_COLS) == 1:\n",
    "        ax = [ax]\n",
    "    # temporal order by year-month if available, else index\n",
    "    if \"year\" in city_df.columns and \"month\" in city_df.columns and not city_df[\"year\"].isna().all():\n",
    "        city_df[\"__time\"] = pd.to_datetime(city_df[\"year\"].astype(int).astype(str) + \"-\" + city_df[\"month\"].astype(int).astype(str) + \"-01\", errors=\"coerce\")\n",
    "        city_df = city_df.sort_values(\"__time\")\n",
    "        x = city_df[\"__time\"]\n",
    "    else:\n",
    "        city_df = city_df.reset_index(drop=True)\n",
    "        x = city_df.index\n",
    "    for i, f in enumerate(FEATURE_COLS):\n",
    "        ax[i].plot(x, city_df[f], marker=\".\", linestyle=\"-\", alpha=0.8)\n",
    "        ax[i].set_ylabel(f)\n",
    "        ax[i].grid(True, alpha=0.3)\n",
    "    title = f\"{city} — signal brut ({city_df.shape[0]} obs)\"\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plot_path = city_dir / f\"timeseries_{city}.png\"\n",
    "    fig.savefig(plot_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save a short meta README\n",
    "    with open(city_dir / \"README.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Diagnostics generated: {datetime.utcnow().isoformat()}Z\\n\")\n",
    "        f.write(f\"City key: {city}\\n\")\n",
    "        f.write(f\"Observations saved: {signals_out.name}\\n\")\n",
    "        f.write(f\"Summary saved: {summary_out.name}\\n\")\n",
    "        f.write(f\"Mahalanobis saved: robust_mahalanobis_{city}.csv\\n\")\n",
    "        f.write(f\"Zscores saved: zscores_{city}.csv\\n\")\n",
    "        if mcd_loc is not None:\n",
    "            f.write(\"Mahalanobis model: robust location and covariance available\\n\")\n",
    "        else:\n",
    "            f.write(\"Mahalanobis model: not fitted (insufficient valid rows) or error\\n\")\n",
    "\n",
    "print(\"Diagnostics générés pour top5 — dossier :\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMj8BTKhAxEVfHfc0L9NwkV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
