{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423ac2f0",
   "metadata": {
    "id": "gjoSn5NmffFB"
   },
   "source": [
    "Block 1 ‚Äî Preparation\n",
    "Here's the preparation section: imports, seeds, folder creation, logger setup, log file bootstrapping, and secure deletion of \"/content/sample_data\" if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f557b3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation/Mise √† jour des d√©pendances via requirements.txt...\n",
      "\n",
      "‚úÖ Toutes les d√©pendances ont √©t√© install√©es ou mises √† jour avec succ√®s.\n",
      "Veuillez RED√âMARRER le noyau (kernel) du notebook si c'est la premi√®re ex√©cution.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ‚öôÔ∏è Installation des d√©pendances du projet\n",
    "# Cette cellule garantit que toutes les librairies n√©cessaires sont install√©es.\n",
    "# ============================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_requirements(file_path=\"requirements.txt\"):\n",
    "    \"\"\"Installe les paquets list√©s dans requirements.txt.\"\"\"\n",
    "    print(f\"Installation/Mise √† jour des d√©pendances via {file_path}...\")\n",
    "    try:\n",
    "        # Ex√©cute la commande pip\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", file_path])\n",
    "        print(\"\\n‚úÖ Toutes les d√©pendances ont √©t√© install√©es ou mises √† jour avec succ√®s.\")\n",
    "        print(\"Veuillez RED√âMARRER le noyau (kernel) du notebook si c'est la premi√®re ex√©cution.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n‚ùå ERREUR lors de l'installation des d√©pendances : {e}\")\n",
    "\n",
    "# Ex√©cuter l'installation\n",
    "install_requirements()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18034f10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1761280407076,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "ANliyMq9eWYS",
    "outputId": "8086a104-2c59-4b9a-cf9f-13666ba83f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 03:18:37,522 | INFO | Aucun r√©pertoire /content/sample_data √† supprimer.\n",
      "2025-11-11 03:18:37,526 | INFO | Pr√©paration termin√©e: librairies import√©es, seeds fix√©s, dossiers cr√©√©s, logger op√©rationnel.\n",
      "Dossiers: {'data': 'c:\\\\Users\\\\zackd\\\\OneDrive\\\\Desktop\\\\T_log_Tsunami_V_0_1En\\\\data', 'results': 'c:\\\\Users\\\\zackd\\\\OneDrive\\\\Desktop\\\\T_log_Tsunami_V_0_1En\\\\results', 'logs': 'c:\\\\Users\\\\zackd\\\\OneDrive\\\\Desktop\\\\T_log_Tsunami_V_0_1En\\\\logs'}\n",
      "Logger pr√™t. Fichiers de log:\n",
      "- c:\\Users\\zackd\\OneDrive\\Desktop\\T_log_Tsunami_V_0_1En\\logs\\logs.txt\n",
      "- c:\\Users\\zackd\\OneDrive\\Desktop\\T_log_Tsunami_V_0_1En\\logs\\logs.csv\n",
      "- c:\\Users\\zackd\\OneDrive\\Desktop\\T_log_Tsunami_V_0_1En\\logs\\summary.md\n"
     ]
    }
   ],
   "source": [
    "# Bloc 1 ‚Äî Pr√©paration\n",
    "# - Imports des librairies\n",
    "# - Seed pour reproductibilit√©\n",
    "# - Cr√©ation des dossiers: data/, results/, logs/\n",
    "# - Setup du logger (fichier + console)\n",
    "# - Bootstrap des fichiers de log: logs/logs.csv et logs/summary.md\n",
    "# - Suppression de /content/sample_data si pr√©sent (environnements type Colab)\n",
    "# - Messages de confirmation imprim√©s en sortie\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Reproductibilit√©\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 2) Cr√©ation des dossiers (idempotent)\n",
    "BASE_DIRS = ['data', 'results', 'logs']\n",
    "for d in BASE_DIRS:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# 3) Setup logger\n",
    "# Format standardis√©: timestamp | level | message\n",
    "log_formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "logger = logging.getLogger('T_log_V0_1')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = []  # √©vite doublons si r√©-ex√©cut√©\n",
    "\n",
    "# Handler fichier (logs/logs.txt pour lecture humaine rapide)\n",
    "file_handler = logging.FileHandler('logs/logs.txt', mode='a', encoding='utf-8')\n",
    "file_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Handler console\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# 4) Bootstrap des fichiers de log structur√©s\n",
    "# logs/logs.csv: colonnes = timestamp, level, message\n",
    "logs_csv_path = 'logs/logs.csv'\n",
    "if not os.path.exists(logs_csv_path):\n",
    "    df_init = pd.DataFrame(columns=['timestamp', 'level', 'message'])\n",
    "    df_init.to_csv(logs_csv_path, index=False)\n",
    "\n",
    "# logs/summary.md: ent√™te + contexte\n",
    "summary_md_path = 'logs/summary.md'\n",
    "if not os.path.exists(summary_md_path):\n",
    "    with open(summary_md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('# Journal de test ‚Äî Mod√®le T_log V0.1\\n\\n')\n",
    "        f.write(f'- Cr√©√© le: {datetime.now().isoformat()}\\n')\n",
    "        f.write('- Contexte: Pr√©paration de l‚Äôenvironnement de test (imports, logger, dossiers)\\n\\n')\n",
    "        f.write('## √âv√©nements cl√©s\\n')\n",
    "\n",
    "# 5) Fonction utilitaire pour loguer dans logs.csv\n",
    "def log_to_csv(level: str, message: str):\n",
    "    ts = datetime.now().isoformat()\n",
    "    row = pd.DataFrame([[ts, level, message]], columns=['timestamp', 'level', 'message'])\n",
    "    try:\n",
    "        row.to_csv(logs_csv_path, mode='a', header=False, index=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Erreur lors de l‚Äô√©criture dans logs.csv: {e}')\n",
    "\n",
    "# 6) Suppression de /content/sample_data si pr√©sent (environnements type Colab)\n",
    "sample_data_path = '/content/sample_data'\n",
    "try:\n",
    "    if os.path.exists(sample_data_path):\n",
    "        import shutil\n",
    "        shutil.rmtree(sample_data_path, ignore_errors=True)\n",
    "        logger.info('R√©pertoire /content/sample_data d√©tect√© et supprim√©.')\n",
    "        log_to_csv('INFO', 'R√©pertoire /content/sample_data supprim√©.')\n",
    "    else:\n",
    "        logger.info('Aucun r√©pertoire /content/sample_data √† supprimer.')\n",
    "        log_to_csv('INFO', 'Aucun /content/sample_data trouv√©.')\n",
    "except Exception as e:\n",
    "    logger.error(f'Erreur lors de la suppression de /content/sample_data: {e}')\n",
    "    log_to_csv('ERROR', f'Suppression /content/sample_data √©chou√©e: {e}')\n",
    "\n",
    "# 7) Messages de confirmation\n",
    "logger.info('Pr√©paration termin√©e: librairies import√©es, seeds fix√©s, dossiers cr√©√©s, logger op√©rationnel.')\n",
    "log_to_csv('INFO', 'Pr√©paration termin√©e: environnement pr√™t.')\n",
    "\n",
    "print('Dossiers:', {d: os.path.abspath(d) for d in BASE_DIRS})\n",
    "print('Logger pr√™t. Fichiers de log:')\n",
    "print('-', os.path.abspath('logs/logs.txt'))\n",
    "print('-', os.path.abspath('logs/logs.csv'))\n",
    "print('-', os.path.abspath('logs/summary.md'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e33867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative d'authentification Kaggle...\n",
      "INFO: Cl√©s lues et d√©finies via C:\\Users\\zackd\\.kaggle\\kaggle.json.\n",
      "SUCC√àS: Authentification Kaggle r√©ussie.\n",
      "\n",
      "D√©but du t√©l√©chargement du fichier ZIP pour : ahmeduzaki/global-earthquake-tsunami-risk-assessment-dataset\n",
      "Dataset URL: https://www.kaggle.com/datasets/ahmeduzaki/global-earthquake-tsunami-risk-assessment-dataset\n",
      "\n",
      "==================================================\n",
      "T√âL√âCHARGEMENT DU ZIP R√âUSSI üéâ\n",
      "Dataset : ahmeduzaki/global-earthquake-tsunami-risk-assessment-dataset\n",
      "Fichier ZIP sauvegard√© ici : /content/data\\Global Earthquake-Tsunami Risk Assessment Dataset.zip\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# --- 0. INSTALLATION DE KAGGLE ---\n",
    "# Cette ligne assure que la librairie Kaggle est install√©e\n",
    "!pip install kaggle --quiet\n",
    "\n",
    "# --- D√©pendance Kaggle ---\n",
    "try:\n",
    "    import kaggle.api as kaggle_api\n",
    "except ImportError:\n",
    "    print(\"√âchec de l'importation de 'kaggle'. V√©rifiez votre installation.\")\n",
    "    raise\n",
    "# ------------------------\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# Identifiant du Dataset Kaggle\n",
    "KAGGLE_DATASET_ID = \"ahmeduzaki/global-earthquake-tsunami-risk-assessment-dataset\"\n",
    "DOWNLOAD_DIR = '/content/data'\n",
    "\n",
    "# Cr√©ation du dossier de destination\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "def find_and_auth_kaggle():\n",
    "    \"\"\"Tente de trouver les cl√©s d'API et authentifie l'API Kaggle.\"\"\"\n",
    "    print(\"Tentative d'authentification Kaggle...\")\n",
    "    \n",
    "    # 1. V√©rifier les variables d'environnement (m√©thode Colab/Notebook)\n",
    "    if os.getenv('KAGGLE_USERNAME') and os.getenv('KAGGLE_KEY'):\n",
    "        print('INFO: Authentification via variables d\\'environnement (KAGGLE_USERNAME/KEY).')\n",
    "    \n",
    "    # 2. Chercher le fichier kaggle.json\n",
    "    else:\n",
    "        locations = [\n",
    "            os.path.join(os.path.expanduser('~'), '.kaggle', 'kaggle.json'), # Emplacement standard\n",
    "            os.path.join(os.getcwd(), 'kaggle.json')                       # R√©pertoire actuel\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for loc in locations:\n",
    "            if os.path.exists(loc):\n",
    "                try:\n",
    "                    with open(loc, 'r') as f:\n",
    "                        config = json.load(f)\n",
    "                        username = config.get('username')\n",
    "                        key = config.get('key')\n",
    "                        if username and key:\n",
    "                            os.environ['KAGGLE_USERNAME'] = username\n",
    "                            os.environ['KAGGLE_KEY'] = key\n",
    "                            print(f'INFO: Cl√©s lues et d√©finies via {loc}.')\n",
    "                            found = True\n",
    "                            break\n",
    "                except (json.JSONDecodeError, Exception):\n",
    "                    continue\n",
    "        \n",
    "        if not found:\n",
    "            print(\"ERREUR: Fichier kaggle.json introuvable. Veuillez le placer dans ~/.kaggle/ ou le r√©pertoire courant.\")\n",
    "            return False\n",
    "\n",
    "    # 3. Authentifier l'API\n",
    "    try:\n",
    "        kaggle_api.authenticate()\n",
    "        print('SUCC√àS: Authentification Kaggle r√©ussie.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'ERREUR: √âchec de l\\'authentification de l\\'API: {e}')\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- 2. T√âL√âCHARGEMENT DU FICHIER ZIP ---\n",
    "try:\n",
    "    if not find_and_auth_kaggle():\n",
    "        raise RuntimeError(\"Processus annul√©. √âchec de la configuration Kaggle.\")\n",
    "    \n",
    "    print(f\"\\nD√©but du t√©l√©chargement du fichier ZIP pour : {KAGGLE_DATASET_ID}\")\n",
    "    \n",
    "    # T√©l√©charger le dataset SANS D√âCOMPRESSION (unzip=False)\n",
    "    kaggle_api.dataset_download_files(\n",
    "        KAGGLE_DATASET_ID, \n",
    "        path=DOWNLOAD_DIR, \n",
    "        unzip=False, # <-- Ceci maintient le fichier au format ZIP\n",
    "        quiet=True\n",
    "    )\n",
    "    \n",
    "    # Tenter de trouver le nom du fichier ZIP t√©l√©charg√©\n",
    "    zip_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.zip')]\n",
    "    \n",
    "    if zip_files:\n",
    "        zip_filename = zip_files[0]\n",
    "        original_path = os.path.join(DOWNLOAD_DIR, zip_filename)\n",
    "        target_path = os.path.join(DOWNLOAD_DIR, 'Global Earthquake-Tsunami Risk Assessment Dataset.zip')\n",
    "        \n",
    "        # Renommer le fichier pour correspondre au nom souhait√©\n",
    "        os.rename(original_path, target_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"T√âL√âCHARGEMENT DU ZIP R√âUSSI üéâ\")\n",
    "        print(f\"Dataset : {KAGGLE_DATASET_ID}\")\n",
    "        print(f\"Fichier ZIP sauvegard√© ici : {target_path}\")\n",
    "        print(\"=\"*50)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Le t√©l√©chargement a r√©ussi mais aucun fichier .zip n'a √©t√© trouv√© dans {DOWNLOAD_DIR}.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"#\"*50)\n",
    "    print(\"√âCHEC DU T√âL√âCHARGEMENT CRITIQUE.\")\n",
    "    print(f\"Erreur: {e}\")\n",
    "    print(f\"V√©rifiez que votre cl√© d'API Kaggle est correctement configur√©e.\")\n",
    "    print(\"#\"*50)\n",
    "    # Ne pas lever l'exception pour √©viter de casser le notebook si le probl√®me est Kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392fea8",
   "metadata": {
    "id": "8ffAHI07hBfN"
   },
   "source": [
    "Block 2 ‚Äî Data Acquisition (Unzip + Initial Inspection)\n",
    "Here is the Python cell that will:\n",
    "\n",
    "Unzip the Global Earthquake-Tsunami Risk Assessment Dataset.zip file located in /content/data/.\n",
    "\n",
    "List the extracted files.\n",
    "\n",
    "Load only the found CSV files.\n",
    "\n",
    "Check for each CSV: number of rows/columns, completely empty columns, and number of NaN values.\n",
    "\n",
    "Save an overall summary in results/data_summary.csv.\n",
    "\n",
    "Log the events in logs/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bc337f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1761280798106,
     "user": {
      "displayName": "GlobalZivotPrint",
      "userId": "12055292741917834281"
     },
     "user_tz": 240
    },
    "id": "8g9O3rC_hCbr",
    "outputId": "15798e48-9d41-4ef0-e724-8dd390461879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 03:18:46,576 | INFO | Fichiers extraits: ['earthquake_data_tsunami.csv']\n",
      "CSV trouv√©s: ['earthquake_data_tsunami.csv']\n",
      "\n",
      "--- earthquake_data_tsunami.csv ---\n",
      "Shape: (782, 13)\n",
      "Colonnes vides: []\n",
      "Nombre total de NaN: 0\n",
      "   magnitude  cdi  mmi  sig  nst   dmin   gap  depth  latitude  longitude  \\\n",
      "0        7.0    8    7  768  117  0.509  17.0   14.0   -9.7963    159.596   \n",
      "1        6.9    4    4  735   99  2.229  34.0   25.0   -4.9559    100.738   \n",
      "2        7.0    3    3  755  147  3.125  18.0  579.0  -20.0508   -178.346   \n",
      "\n",
      "   Year  Month  tsunami  \n",
      "0  2022     11        1  \n",
      "1  2022     11        0  \n",
      "2  2022     11        1  \n",
      "\n",
      "R√©sum√© global sauvegard√© dans: results/data_summary.csv\n",
      "                          file  rows  cols  empty_cols  total_NaN\n",
      "0  earthquake_data_tsunami.csv   782    13           0          0\n"
     ]
    }
   ],
   "source": [
    "# Bloc 2 ‚Äî Acquisition de donn√©es\n",
    "# D√©zipper le fichier et analyser les CSV pour colonnes vides ou NaN\n",
    "\n",
    "import zipfile\n",
    "\n",
    "zip_path = '/content/data/Global Earthquake-Tsunami Risk Assessment Dataset.zip'\n",
    "extract_dir = 'data/extracted'\n",
    "\n",
    "# 1) Extraction\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "    extracted_files = zip_ref.namelist()\n",
    "\n",
    "logger.info(f\"Fichiers extraits: {extracted_files}\")\n",
    "log_to_csv('INFO', f\"Fichiers extraits: {extracted_files}\")\n",
    "\n",
    "# 2) Filtrer les CSV\n",
    "csv_files = [f for f in extracted_files if f.lower().endswith('.csv')]\n",
    "print(\"CSV trouv√©s:\", csv_files)\n",
    "\n",
    "# 3) Inspection des CSV\n",
    "summary_rows = []\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        shape = df.shape\n",
    "        empty_cols = [col for col in df.columns if df[col].isna().all()]\n",
    "        nan_counts = df.isna().sum().sum()\n",
    "\n",
    "        print(f\"\\n--- {csv_file} ---\")\n",
    "        print(\"Shape:\", shape)\n",
    "        print(\"Colonnes vides:\", empty_cols)\n",
    "        print(\"Nombre total de NaN:\", nan_counts)\n",
    "        print(df.head(3))  # aper√ßu rapide\n",
    "\n",
    "        summary_rows.append({\n",
    "            'file': csv_file,\n",
    "            'rows': shape[0],\n",
    "            'cols': shape[1],\n",
    "            'empty_cols': len(empty_cols),\n",
    "            'total_NaN': nan_counts\n",
    "        })\n",
    "\n",
    "        log_to_csv('INFO', f\"Inspection {csv_file}: {shape}, NaN={nan_counts}, empty_cols={len(empty_cols)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lecture {csv_file}: {e}\")\n",
    "        log_to_csv('ERROR', f\"Erreur lecture {csv_file}: {e}\")\n",
    "\n",
    "# 4) Sauvegarde du r√©sum√© global\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = 'results/data_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\nR√©sum√© global sauvegard√© dans:\", summary_path)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2d90c",
   "metadata": {
    "id": "HQCK2o6ZhrZ-"
   },
   "source": [
    "Perfect üëç ‚Äî your dataset is clean: 782 rows, 13 columns, no empty columns, no NaNs.\n",
    "We can now move on to the next step of the protocol.\n",
    "\n",
    "---\n",
    "\n",
    "### Block 3 ‚Äî Calculating \\(T_{\\log}\\) (preparation)\n",
    "\n",
    "To apply your model \\(T_{\\log}(n,d) = (d-4)\\cdot \\ln(n)\\), we need to define:\n",
    "\n",
    "- **\\(n\\)**: the size of the system. Here, we can take \\(n = 782\\) (total number of seismic events in the dataset).\n",
    "- **\\(d\\)**: the effective dimension. Since this dataset is not a graph with a Laplacian spectrum, we must choose an approximation. Two possible options:\n",
    "1. **Physical dimension**: take \\(d=3\\) (3D geographic space: latitude, longitude, depth).\n",
    "2. **Enriched dimension**: Include time as an additional axis ‚Üí \\(d=4\\).\n",
    "\n",
    "üëâ To stay true to V0.1 (without the PDE extension), I suggest starting with **\\(d=3\\)** (spatial dimension). We can then test the sensitivity by sweeping \\(d\\) around 3‚Äì4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb1f1a4",
   "metadata": {
    "id": "urPtYVVVigm-"
   },
   "source": [
    "### Block 3 ‚Äî Calculating T_{\\log} with d = 3\n",
    "\n",
    "Here is cell 3. It calculates T_{\\log} for your dataset (782 events), with d=3 and bias=0. It displays the numerical result and the corresponding regime, then logs the event."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqO8+QWyJDBtFCYwkGAuDq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
